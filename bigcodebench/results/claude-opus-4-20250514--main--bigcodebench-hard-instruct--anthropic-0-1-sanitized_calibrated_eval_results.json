{
  "date": "2025-09-03 19:13",
  "eval": {
    "BigCodeBench/19": [
      {
        "task_id": "BigCodeBench/19",
        "solution": "import os\nimport glob\nimport zipfile\ndef task_func(directory):\n\n    pass\nimport os\nimport glob\nimport zipfile\ndef task_func(directory):\n    \"\"\"\n    Zips all files (not including subdirectories) located in the specified directory \n    and returns the path to the created zip file.\n    \n    Args:\n        directory (str): The path to the directory containing files to zip\n        \n    Returns:\n        str: The path to the generated zip file. Returns None if the directory \n             does not contain any files.\n             \n    Raises:\n        FileNotFoundError: if the specified directory does not exist\n    \"\"\"\n    # Check if directory exists\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"Directory '{directory}' does not exist\")\n    \n    if not os.path.isdir(directory):\n        raise FileNotFoundError(f\"'{directory}' is not a directory\")\n    \n    # Get all files in the directory (not subdirectories)\n    files = []\n    for item in os.listdir(directory):\n        item_path = os.path.join(directory, item)\n        if os.path.isfile(item_path):\n            files.append(item_path)\n    \n    # If no files found, return None\n    if not files:\n        return None\n    \n    # Create zip file path\n    zip_path = os.path.join(directory, 'files.zip')\n    \n    # Create zip file\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for file_path in files:\n            # Add file to zip with just the filename (not the full path)\n            arcname = os.path.basename(file_path)\n            zipf.write(file_path, arcname)\n    \n    return zip_path",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/13": [
      {
        "task_id": "BigCodeBench/13",
        "solution": "import subprocess\nimport ftplib\nimport os\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n\n    pass\nimport subprocess\nimport ftplib\nimport os\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    \"\"\"\n    Download all files from a specific directory on an FTP server using wget in a subprocess.\n    \n    Args:\n        ftp_server (str): The FTP server address. Default is 'ftp.dlptest.com'.\n        ftp_user (str): The FTP server username. Default is 'dlpuser'.\n        ftp_password (str): The FTP server password. Default is 'rNrKYTX9g7z3RgJRmxWuGHbeu'.\n        ftp_dir (str): The directory path on the FTP server from which files need to be downloaded. Default is '/ftp/test'.\n    \n    Returns:\n        List[str]: A list of filenames that were attempted to be downloaded from the FTP server.\n    \n    Raises:\n        Exception: If there is a failure in connecting to the FTP server, logging in, or changing directory.\n    \"\"\"\n    \n    # First, connect to FTP server to get list of files\n    try:\n        ftp = ftplib.FTP()\n        ftp.connect(ftp_server)\n    except Exception as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n    \n    # Login to FTP server\n    try:\n        ftp.login(ftp_user, ftp_password)\n    except Exception as e:\n        ftp.quit()\n        raise Exception(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n    \n    # Change to specified directory\n    try:\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        ftp.quit()\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n    \n    # Get list of files in the directory\n    files = []\n    try:\n        # Get list of files (not directories)\n        ftp.retrlines('LIST', lambda x: files.append(x))\n        \n        # Parse filenames from LIST output\n        filenames = []\n        for file_info in files:\n            # FTP LIST format typically has filename as the last element\n            parts = file_info.split()\n            if parts and not file_info.startswith('d'):  # Skip directories\n                filename = parts[-1]\n                filenames.append(filename)\n        \n        ftp.quit()\n    except Exception as e:\n        ftp.quit()\n        raise Exception(f\"Failed to list files in directory {ftp_dir}: {str(e)}\")\n    \n    # Download each file using wget\n    downloaded_files = []\n    for filename in filenames:\n        # Construct the full FTP URL for the file\n        ftp_url = f\"ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/{filename}\"\n        \n        try:\n            # Use wget to download the file\n            subprocess.run(['wget', '-q', ftp_url], check=True)\n            downloaded_files.append(filename)\n        except subprocess.CalledProcessError:\n            # Still add to the list as it was \"attempted\"\n            downloaded_files.append(filename)\n        except FileNotFoundError:\n            # wget might not be installed\n            downloaded_files.append(filename)\n    \n    return downloaded_files",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 107, in test_case_1\nAssertionError: 0 != 2\n"
        }
      }
    ],
    "BigCodeBench/120": [
      {
        "task_id": "BigCodeBench/120",
        "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n\n    pass\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    \"\"\"\n    Generate a pandas Series of random dates within a specified date range.\n    \n    Parameters:\n    - start_date: datetime object for the start of the range (inclusive)\n    - end_date: datetime object for the end of the range (inclusive)\n    - seed: integer for random seed (default 42)\n    \n    Returns:\n    - pandas.Series: Series containing random dates within the specified range\n    \n    Raises:\n    - ValueError: If inputs are not datetime objects or if start_date > end_date\n    \"\"\"\n    \n    # Validate inputs\n    if not isinstance(start_date, datetime):\n        raise ValueError(\"start_date must be a datetime.datetime instance\")\n    \n    if not isinstance(end_date, datetime):\n        raise ValueError(\"end_date must be a datetime.datetime instance\")\n    \n    if start_date > end_date:\n        raise ValueError(\"start_date must be earlier than or equal to end_date\")\n    \n    # Set random seed for reproducibility\n    random_seed(seed)\n    \n    # Calculate the number of days in the range (inclusive)\n    days_diff = (end_date - start_date).days + 1\n    \n    # Generate random dates\n    random_dates = []\n    for _ in range(days_diff):\n        # Generate a random number of days from start_date\n        random_days = randint(0, days_diff - 1)\n        random_date = start_date + timedelta(days=random_days)\n        random_dates.append(random_date)\n    \n    # Create and return pandas Series\n    return pd.Series(random_dates)",
        "status": "fail",
        "details": {
          "test_reproducibility_with_seed": "Traceback (most recent call last):\n  File \"__test__.py\", line 66, in test_reproducibility_with_seed\nAssertionError: Lists differ: ['202[5056 chars]11-26', '2020-06-02', '2020-08-22', '2020-06-10', '2020-02-07'] != ['202[5056 chars]11-26', '2020-06-02', '2020-08-22', '2020-06-10']\n\nFirst list contains 1 additional elements.\nFirst extra element 365:\n'2020-02-07'\n\nDiff is 6275 characters long. Set self.maxDiff to None to see it. : DataFrame contents should match the expected output\n",
          "test_series_length": "Traceback (most recent call last):\n  File \"__test__.py\", line 72, in test_series_length\nAssertionError: 10 != 9\n"
        }
      }
    ],
    "BigCodeBench/227": [
      {
        "task_id": "BigCodeBench/227",
        "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n\n    pass\nimport numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n    \"\"\"\n    Creates an MxN matrix from a list L, normalizes it based on the sound pressure level (SPL) \n    of a specified audio file, and generates a spectrogram from the matrix.\n    \n    Parameters:\n    L: list - Input list to create matrix from\n    M: int - Number of rows in the matrix\n    N: int - Number of columns in the matrix\n    audio_file: str - Path to the audio file\n    \n    Returns:\n    numpy.ndarray: The normalized MxN matrix\n    matplotlib.figure.Figure: The figure object for the generated spectrogram\n    \n    Raises:\n    FileNotFoundError: If the specified audio file does not exist\n    \"\"\"\n    \n    # Check if audio file exists\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"Audio file '{audio_file}' not found\")\n    \n    # Create MxN matrix from list L\n    # If L doesn't have enough elements, cycle through it\n    total_elements = M * N\n    if len(L) < total_elements:\n        # Repeat the list to have enough elements\n        extended_L = L * (total_elements // len(L) + 1)\n    else:\n        extended_L = L\n    \n    # Take only the required number of elements and reshape\n    matrix = np.array(extended_L[:total_elements]).reshape(M, N)\n    \n    # Read audio file and calculate SPL\n    audio_data, sample_rate = sf.read(audio_file)\n    \n    # If stereo, convert to mono by averaging channels\n    if len(audio_data.shape) > 1:\n        audio_data = np.mean(audio_data, axis=1)\n    \n    # Calculate SPL using the formula: SPL = 20 * log10(sqrt(mean(data^2)))\n    rms = np.sqrt(np.mean(audio_data**2))\n    spl = 20 * np.log10(rms) if rms > 0 else -np.inf\n    \n    # Normalize the matrix based on SPL\n    # Convert SPL back to linear scale for normalization\n    normalization_factor = 10**(spl/20)\n    normalized_matrix = matrix * normalization_factor\n    \n    # Generate spectrogram\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Create time and frequency arrays for plotting\n    time = np.arange(N)\n    freq = np.arange(M) + 1  # Start from 1 to avoid log(0)\n    \n    # Plot the spectrogram\n    # Use the absolute value of the normalized matrix as amplitude\n    amplitude = np.abs(normalized_matrix)\n    \n    # Create mesh for pcolormesh\n    T, F = np.meshgrid(time, freq)\n    \n    # Plot with logarithmic frequency scale\n    pcm = ax.pcolormesh(T, F, amplitude, shading='auto', cmap='viridis')\n    \n    # Set logarithmic scale for frequency (y-axis)\n    ax.set_yscale('log')\n    \n    # Set linear scale for time (x-axis) - this is default\n    ax.set_xscale('linear')\n    \n    # Add labels and title\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Frequency (log scale)')\n    ax.set_title(f'Spectrogram (SPL: {spl:.2f} dB)')\n    \n    # Add colorbar\n    cbar = plt.colorbar(pcm, ax=ax)\n    cbar.set_label('Amplitude')\n    \n    # Adjust layout\n    plt.tight_layout()\n    \n    return normalized_matrix, fig",
        "status": "fail",
        "details": {
          "test_docstring_examples": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 134, in test_docstring_examples\n  File \"__test__.py\", line 35, in task_func\nFileNotFoundError: Audio file 'audio.wav' not found\n",
          "test_empty_list_input": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 114, in test_empty_list_input\n  File \"__test__.py\", line 35, in task_func\nFileNotFoundError: Audio file 'audio.wav' not found\n",
          "test_spl_calculation": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 144, in test_spl_calculation\n  File \"__test__.py\", line 35, in task_func\nFileNotFoundError: Audio file 'audio.wav' not found\n",
          "test_successful_matrix_creation": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 122, in test_successful_matrix_creation\n  File \"__test__.py\", line 35, in task_func\nFileNotFoundError: Audio file 'audio.wav' not found\n"
        }
      }
    ],
    "BigCodeBench/199": [
      {
        "task_id": "BigCodeBench/199",
        "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n\n    pass\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    \"\"\"\n    Generate a weather report for specified cities at a given UTC datetime.\n    \n    Parameters:\n    - utc_datetime: datetime object representing the UTC time\n    - cities: list of city names\n    - weather_conditions: list of possible weather conditions\n    - timezones: dictionary mapping cities to their timezone strings\n    - seed: random seed for reproducible results\n    \n    Returns:\n    - pandas.DataFrame with columns: 'City', 'Local Time', 'Weather Condition'\n    \n    Raises:\n    - ValueError: If utc_datetime is not a datetime object or if any of the other \n                  parameters are not in the expected format\n    \"\"\"\n    \n    # Validate utc_datetime\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n    \n    # Validate cities\n    if not isinstance(cities, list) or not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"cities must be a list of strings\")\n    \n    # Validate weather_conditions\n    if not isinstance(weather_conditions, list) or not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"weather_conditions must be a list of strings\")\n    \n    # Validate timezones\n    if not isinstance(timezones, dict):\n        raise ValueError(\"timezones must be a dictionary\")\n    \n    # Check that all cities have corresponding timezones\n    for city in cities:\n        if city not in timezones:\n            raise ValueError(f\"Timezone not found for city: {city}\")\n    \n    # Set random seed for reproducibility\n    set_seed(seed)\n    \n    # Initialize lists to store data\n    report_data = []\n    \n    # Make sure utc_datetime is timezone-aware (UTC)\n    if utc_datetime.tzinfo is None:\n        utc_datetime = pytz.UTC.localize(utc_datetime)\n    else:\n        utc_datetime = utc_datetime.astimezone(pytz.UTC)\n    \n    # Generate weather report for each city\n    for city in cities:\n        # Get timezone for the city\n        tz_str = timezones[city]\n        \n        try:\n            tz = pytz.timezone(tz_str)\n        except pytz.exceptions.UnknownTimeZoneError:\n            raise ValueError(f\"Invalid timezone '{tz_str}' for city '{city}'\")\n        \n        # Convert UTC time to local time\n        local_time = utc_datetime.astimezone(tz)\n        \n        # Format local time as 'YYYY-MM-DD HH:MM:SS ZZZ'\n        formatted_time = local_time.strftime('%Y-%m-%d %H:%M:%S %Z')\n        \n        # Generate random weather condition\n        weather = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        \n        # Add to report data\n        report_data.append({\n            'City': city,\n            'Local Time': formatted_time,\n            'Weather Condition': weather\n        })\n    \n    # Create and return DataFrame\n    df = pd.DataFrame(report_data)\n    return df",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/123": [
      {
        "task_id": "BigCodeBench/123",
        "solution": "import pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n\n    pass\nimport pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    \"\"\"\n    Modify a list by adding the element '12', then concatenate a number of CSV files \n    from a directory into a single DataFrame. The number of files concatenated is \n    determined by the sum of the numbers in the list.\n    \n    Parameters:\n    - my_list: A list of numbers\n    - file_dir: Directory containing CSV files (default: './data_files/')\n    - file_ext: File extension to search for (default: '.csv')\n    \n    Returns:\n    - DataFrame: A pandas DataFrame concatenating data from the selected CSV files\n    \n    Raises:\n    - TypeError: If 'my_list' is not a list\n    - FileNotFoundError: If no files are found in the specified directory\n    \"\"\"\n    \n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list\")\n    \n    # Add '12' to the list\n    my_list.append('12')\n    \n    # Convert all elements to numbers and calculate sum\n    try:\n        # Convert string elements to numbers\n        numeric_list = []\n        for item in my_list:\n            if isinstance(item, str):\n                numeric_list.append(int(item))\n            else:\n                numeric_list.append(item)\n        \n        num_files = sum(numeric_list)\n    except (ValueError, TypeError) as e:\n        raise TypeError(f\"List contains non-numeric values: {e}\")\n    \n    # Get all CSV files from the directory\n    file_pattern = os.path.join(file_dir, f'*{file_ext}')\n    csv_files = glob.glob(file_pattern)\n    \n    # Check if any files were found\n    if not csv_files:\n        raise FileNotFoundError(f\"No {file_ext} files found in {file_dir}\")\n    \n    # Sort files for consistent ordering\n    csv_files.sort()\n    \n    # Select the number of files to concatenate\n    files_to_concat = csv_files[:num_files]\n    \n    # If we need more files than available, use all available files\n    if len(files_to_concat) == 0:\n        files_to_concat = csv_files[:1]  # At least one file\n    \n    # Read and concatenate the CSV files\n    dataframes = []\n    for file in files_to_concat:\n        try:\n            df = pd.read_csv(file)\n            dataframes.append(df)\n        except Exception as e:\n            print(f\"Warning: Could not read file {file}: {e}\")\n            continue\n    \n    # Check if we successfully read any files\n    if not dataframes:\n        raise FileNotFoundError(\"Could not read any CSV files\")\n    \n    # Concatenate all dataframes\n    result_df = pd.concat(dataframes, ignore_index=True)\n    \n    return result_df",
        "status": "fail",
        "details": {
          "test_list_modification": "Traceback (most recent call last):\n  File \"__test__.py\", line 124, in test_list_modification\nAssertionError: 12 not found in [1, 2, 3, '12']\n"
        }
      }
    ],
    "BigCodeBench/161": [
      {
        "task_id": "BigCodeBench/161",
        "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(log_file):\n\n    pass\nimport re\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(log_file):\n    \"\"\"\n    Extracts logging information such as message type, timestamp, and the message itself \n    from a log file and stores the data in a CSV format.\n    \n    Args:\n        log_file (str): Path to the input log file\n        \n    Returns:\n        str: The file path to the newly created CSV file which contains the structured log data.\n        \n    Raises:\n        ValueError: If the timestamp in any log entry is invalid or if no valid log entries are found.\n    \"\"\"\n    # Regular expression pattern to match log entries\n    # Format: TYPE: [TIMESTAMP (YYYY-MM-DD HH:MM:SS)] - MESSAGE\n    pattern = r'^(\\w+):\\s*\\[(\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}:\\d{2})\\]\\s*-\\s*(.*)$'\n    \n    # Lists to store extracted data\n    message_types = []\n    timestamps = []\n    messages = []\n    \n    # Read and parse the log file\n    try:\n        with open(log_file, 'r') as f:\n            lines = f.readlines()\n    except FileNotFoundError:\n        raise ValueError(f\"Log file '{log_file}' not found\")\n    \n    # Process each line\n    for line_num, line in enumerate(lines, 1):\n        line = line.strip()\n        if not line:  # Skip empty lines\n            continue\n            \n        match = re.match(pattern, line)\n        if match:\n            msg_type = match.group(1)\n            timestamp_str = match.group(2)\n            message = match.group(3)\n            \n            # Validate timestamp\n            try:\n                timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')\n            except ValueError:\n                raise ValueError(f\"Invalid timestamp format at line {line_num}: {timestamp_str}\")\n            \n            # Store the extracted data\n            message_types.append(msg_type)\n            timestamps.append(timestamp)\n            messages.append(message)\n    \n    # Check if any valid entries were found\n    if not message_types:\n        raise ValueError(\"No valid log entries found in the file\")\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'message_type': message_types,\n        'timestamp': timestamps,\n        'message': messages\n    })\n    \n    # Generate output CSV filename\n    output_file = log_file.rsplit('.', 1)[0] + '_structured.csv'\n    \n    # Save to CSV\n    df.to_csv(output_file, index=False)\n    \n    return output_file",
        "status": "fail",
        "details": {
          "test_log_to_csv_content": "Traceback (most recent call last):\n  File \"__test__.py\", line 106, in test_log_to_csv_content\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 1185, in assert_frame_equal\n    assert_index_equal(\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 322, in assert_index_equal\n    _testing.assert_almost_equal(\n  File \"pandas/_libs/testing.pyx\", line 52, in pandas._libs.testing.assert_almost_equal\n  File \"pandas/_libs/testing.pyx\", line 172, in pandas._libs.testing.assert_almost_equal\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 599, in raise_assert_detail\n    raise AssertionError(msg)\nAssertionError: DataFrame.columns are different\n\nDataFrame.columns values are different (100.0 %)\n[left]:  Index(['Type', 'Timestamp', 'Message'], dtype='object')\n[right]: Index(['message_type', 'timestamp', 'message'], dtype='object')\nAt positional index 0, first diff: Type != message_type\n"
        }
      }
    ],
    "BigCodeBench/211": [
      {
        "task_id": "BigCodeBench/211",
        "solution": "import requests\nimport os\nimport zipfile\ndef task_func(url, destination_directory, headers=None):\n\n    pass\nimport requests\nimport os\nimport zipfile\ndef task_func(url, destination_directory, headers=None):\n    \"\"\"\n    Download and keep a zip file from a URL, extract its contents to the specified directory,\n    and return the list of extracted files.\n    \n    Args:\n        url (str): The URL of the zip file to download\n        destination_directory (str): The directory where files will be extracted\n        headers (dict, optional): Optional headers for the HTTP request\n    \n    Returns:\n        list: A list of filenames of the extracted files\n    \"\"\"\n    # Create destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n    \n    # Extract filename from URL or use a default name\n    zip_filename = os.path.basename(url)\n    if not zip_filename.endswith('.zip'):\n        zip_filename = 'downloaded_file.zip'\n    \n    # Full path for the zip file\n    zip_filepath = os.path.join(destination_directory, zip_filename)\n    \n    try:\n        # Download the zip file\n        response = requests.get(url, headers=headers, stream=True)\n        response.raise_for_status()  # Raise an exception for bad status codes\n        \n        # Save the zip file\n        with open(zip_filepath, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                if chunk:\n                    f.write(chunk)\n        \n        # Extract the zip file\n        extracted_files = []\n        with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n            # Get list of files in the zip\n            file_list = zip_ref.namelist()\n            \n            # Extract all files\n            zip_ref.extractall(destination_directory)\n            \n            # Build list of extracted files (excluding directories)\n            for file_info in zip_ref.filelist:\n                if not file_info.is_dir():\n                    extracted_files.append(file_info.filename)\n        \n        return extracted_files\n        \n    except requests.exceptions.RequestException as e:\n        raise Exception(f\"Error downloading file: {str(e)}\")\n    except zipfile.BadZipFile as e:\n        raise Exception(f\"Error: The downloaded file is not a valid zip file: {str(e)}\")\n    except Exception as e:\n        raise Exception(f\"An error occurred: {str(e)}\")",
        "status": "fail",
        "details": {
          "test_2": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 122, in test_2\n  File \"__test__.py\", line 24, in task_func\n  File \"/usr/local/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.10/os.py\", line 225, in makedirs\n    mkdir(name, mode)\nPermissionError: [Errno 13] Permission denied: '/path'\n",
          "test_3": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 147, in test_3\n  File \"__test__.py\", line 24, in task_func\n  File \"/usr/local/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.10/os.py\", line 225, in makedirs\n    mkdir(name, mode)\nPermissionError: [Errno 13] Permission denied: '/path'\n",
          "test_4": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 172, in test_4\n  File \"__test__.py\", line 24, in task_func\n  File \"/usr/local/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.10/os.py\", line 225, in makedirs\n    mkdir(name, mode)\nPermissionError: [Errno 13] Permission denied: '/path'\n",
          "test_5": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 197, in test_5\n  File \"__test__.py\", line 24, in task_func\n  File \"/usr/local/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.10/os.py\", line 225, in makedirs\n    mkdir(name, mode)\nPermissionError: [Errno 13] Permission denied: '/path'\n",
          "test_download_and_extract": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 97, in test_download_and_extract\n  File \"__test__.py\", line 24, in task_func\n  File \"/usr/local/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.10/os.py\", line 225, in makedirs\n    mkdir(name, mode)\nPermissionError: [Errno 13] Permission denied: '/path'\n"
        }
      }
    ],
    "BigCodeBench/17": [
      {
        "task_id": "BigCodeBench/17",
        "solution": "import subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n\n    pass\nimport subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n    \"\"\"\n    Check if a particular process is running based on its name. If it is not running, \n    start it using the process name as a command. If it is running, terminate the \n    process and restart it by executing the process name as a command.\n    \n    Args:\n        process_name (str): The name of the process to check and manage\n        \n    Returns:\n        str: A message indicating the action taken:\n             \"Process not found. Starting <process_name>.\"\n             \"Process found. Restarting <process_name>.\"\n    \"\"\"\n    # Check if process is running\n    process_found = False\n    for proc in psutil.process_iter(['pid', 'name']):\n        try:\n            # Check if process name matches (case-insensitive)\n            if proc.info['name'] and process_name.lower() in proc.info['name'].lower():\n                process_found = True\n                # Terminate the process\n                proc.terminate()\n                # Wait for process to terminate\n                try:\n                    proc.wait(timeout=5)\n                except psutil.TimeoutExpired:\n                    # Force kill if it doesn't terminate gracefully\n                    proc.kill()\n                    proc.wait()\n                break\n        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n            continue\n    \n    # Start the process\n    try:\n        # Start the process in the background\n        subprocess.Popen([process_name], \n                        stdout=subprocess.DEVNULL, \n                        stderr=subprocess.DEVNULL,\n                        stdin=subprocess.DEVNULL)\n        \n        # Small delay to allow process to start\n        time.sleep(0.1)\n        \n        if process_found:\n            return f\"Process found. Restarting {process_name}.\"\n        else:\n            return f\"Process not found. Starting {process_name}.\"\n            \n    except Exception as e:\n        # If we can't start the process, still return appropriate message\n        if process_found:\n            return f\"Process found. Restarting {process_name}.\"\n        else:\n            return f\"Process not found. Starting {process_name}.\"",
        "status": "fail",
        "details": {
          "test_process_found_restarts_process": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 85, in test_process_found_restarts_process\nAssertionError: 'Process not found. Starting notepad.' != 'Process found. Restarting notepad.'\n- Process not found. Starting notepad.\n?         ----       ^\n+ Process found. Restarting notepad.\n?                ^^^\n\n",
          "test_process_not_found_starts_process": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 76, in test_process_not_found_starts_process\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 941, in assert_called_once_with\n    return self.assert_called_with(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 929, in assert_called_with\n    raise AssertionError(_error_message()) from cause\nAssertionError: expected call not found.\nExpected: Popen('random_non_existent_process')\nActual: Popen(['random_non_existent_process'], stdout=-3, stderr=-3, stdin=-3)\n",
          "test_process_terminates_and_restarts_multiple_instances": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 99, in test_process_terminates_and_restarts_multiple_instances\nAssertionError: 'Process not found. Starting multi_instance.' != 'Process found. Restarting multi_instance.'\n- Process not found. Starting multi_instance.\n?         ----       ^\n+ Process found. Restarting multi_instance.\n?                ^^^\n\n"
        }
      }
    ],
    "BigCodeBench/187": [
      {
        "task_id": "BigCodeBench/187",
        "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n\n    pass\nimport numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    \"\"\"\n    Create a GeoPandas DataFrame for a list of cities with randomly generated coordinates based on specified ranges.\n    \n    Parameters:\n    - dic (dict): Dictionary with 'Lon' and 'Lat' keys, each containing a tuple of (min, max) values\n    - cities (list): List of city names\n    \n    Returns:\n    - GeoDataFrame: A GeoPandas DataFrame containing 'City' and 'Coordinates' (Point objects)\n    \n    Raises:\n    - ValueError: If 'Lon' or 'Lat' keys are missing in the dictionary, or if their values are not tuples\n    \"\"\"\n    \n    # Validate input dictionary\n    if 'Lon' not in dic:\n        raise ValueError(\"'Lon' key is missing in the dictionary\")\n    if 'Lat' not in dic:\n        raise ValueError(\"'Lat' key is missing in the dictionary\")\n    \n    # Check if values are tuples\n    if not isinstance(dic['Lon'], tuple):\n        raise ValueError(\"'Lon' value must be a tuple\")\n    if not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"'Lat' value must be a tuple\")\n    \n    # Extract longitude and latitude ranges\n    lon_min, lon_max = dic['Lon']\n    lat_min, lat_max = dic['Lat']\n    \n    # Generate random coordinates for each city\n    coordinates = []\n    for city in cities:\n        # Generate random longitude and latitude\n        lon = np.random.uniform(lon_min, lon_max)\n        lat = np.random.uniform(lat_min, lat_max)\n        \n        # Create Point object\n        point = Point(lon, lat)\n        coordinates.append(point)\n    \n    # Create GeoDataFrame\n    gdf = gpd.GeoDataFrame({\n        'City': cities,\n        'Coordinates': coordinates\n    }, geometry='Coordinates')\n    \n    return gdf",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/273": [
      {
        "task_id": "BigCodeBench/273",
        "solution": "import cgi\nimport http.server\nimport json\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\ndef task_func():\n\n    pass\nimport cgi\nimport http.server\nimport json\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\ndef task_func():\n    class PostRequestHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            # Check Content-Type header\n            content_type = self.headers.get('Content-Type')\n            if content_type != 'application/json':\n                self.send_error_response(400, \"Content-Type header is not application/json\")\n                return\n            \n            # Read the request body\n            content_length = int(self.headers.get('Content-Length', 0))\n            post_data = self.rfile.read(content_length)\n            \n            # Try to parse JSON\n            try:\n                json_data = json.loads(post_data.decode('utf-8'))\n            except (json.JSONDecodeError, UnicodeDecodeError):\n                self.send_error_response(400, \"Invalid JSON\")\n                return\n            \n            # Check if 'data' key exists\n            if 'data' not in json_data:\n                self.send_error_response(400, \"No data key in request\")\n                return\n            \n            # Send success response\n            self.send_success_response()\n        \n        def send_success_response(self):\n            response_body = json.dumps(SUCCESS_RESPONSE).encode('utf-8')\n            self.send_response(200)\n            self.send_header('Content-Type', 'application/json')\n            self.send_header('Content-Length', str(len(response_body)))\n            self.end_headers()\n            self.wfile.write(response_body)\n        \n        def send_error_response(self, code, message):\n            error_response = ERROR_RESPONSE.copy()\n            error_response['message'] = message\n            response_body = json.dumps(error_response).encode('utf-8')\n            self.send_response(code)\n            self.send_header('Content-Type', 'application/json')\n            self.send_header('Content-Length', str(len(response_body)))\n            self.end_headers()\n            self.wfile.write(response_body)\n        \n        def log_message(self, format, *args):\n            # Override to suppress default logging\n            pass\n    \n    return PostRequestHandler",
        "status": "fail",
        "details": {
          "test_valid_json_data": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 113, in test_valid_json_data\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 929, in assert_called_with\n    raise AssertionError(_error_message()) from cause\nAssertionError: expected call not found.\nExpected: mock(200)\nActual: mock(400)\n",
          "test_invalid_content_type": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 91, in test_invalid_content_type\n  File \"__test__.py\", line 32, in do_POST\n  File \"__test__.py\", line 66, in send_error_response\n  File \"/usr/local/lib/python3.10/http/server.py\", line 498, in send_response\n    self.log_request(code)\n  File \"/usr/local/lib/python3.10/http/server.py\", line 551, in log_request\n    self.requestline, str(code), str(size))\nAttributeError: 'PostRequestHandler' object has no attribute 'requestline'\n",
          "test_invalid_json": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 124, in test_invalid_json\n  File \"__test__.py\", line 32, in do_POST\n  File \"__test__.py\", line 66, in send_error_response\n  File \"/usr/local/lib/python3.10/http/server.py\", line 498, in send_response\n    self.log_request(code)\n  File \"/usr/local/lib/python3.10/http/server.py\", line 551, in log_request\n    self.requestline, str(code), str(size))\nAttributeError: 'PostRequestHandler' object has no attribute 'requestline'\n",
          "test_missing_data_key": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 135, in test_missing_data_key\n  File \"__test__.py\", line 32, in do_POST\n  File \"__test__.py\", line 66, in send_error_response\n  File \"/usr/local/lib/python3.10/http/server.py\", line 498, in send_response\n    self.log_request(code)\n  File \"/usr/local/lib/python3.10/http/server.py\", line 551, in log_request\n    self.requestline, str(code), str(size))\nAttributeError: 'PostRequestHandler' object has no attribute 'requestline'\n"
        }
      }
    ],
    "BigCodeBench/129": [
      {
        "task_id": "BigCodeBench/129",
        "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\ndef task_func(url='http://example.com'):\n\n    pass\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\ndef task_func(url='http://example.com'):\n    \"\"\"\n    Scrape the first table from a web page and extract data into a Pandas DataFrame.\n    \n    This function scrapes the first table found on the specified web page URL and extracts \n    the data into a DataFrame, where each row in the DataFrame corresponds to a table row (<tr>) \n    from the web page, and each column represents the data contained within table data elements \n    (<td>) of that row. The DataFrame's columns are named after the table's header row \n    (<th> elements), if present. If the table lacks headers, the DataFrame's columns remain unnamed.\n    \n    Parameters:\n    -----------\n    url : str\n        The URL of the webpage to scrape (default: 'http://example.com')\n    \n    Returns:\n    --------\n    pd.DataFrame\n        A DataFrame containing the scraped table data, with rows corresponding to table rows and\n        columns named after the table headers, if available.\n    \n    Raises:\n    -------\n    ConnectionError\n        If there is an issue connecting to the URL.\n    requests.HTTPError\n        If the HTTP request to the URL fails.\n    ValueError\n        If no table data is found on the page or if the page content cannot be parsed.\n    \"\"\"\n    \n    try:\n        # Make HTTP request to the URL\n        response = requests.get(url, timeout=10)\n        \n        # Raise HTTPError for bad status codes\n        response.raise_for_status()\n        \n    except requests.exceptions.ConnectionError as e:\n        raise ConnectionError(f\"Failed to connect to {url}: {str(e)}\")\n    except requests.exceptions.Timeout as e:\n        raise ConnectionError(f\"Request to {url} timed out: {str(e)}\")\n    except requests.exceptions.RequestException as e:\n        raise ConnectionError(f\"Error making request to {url}: {str(e)}\")\n    \n    try:\n        # Parse the HTML content\n        soup = BeautifulSoup(response.content, 'html.parser')\n    except Exception as e:\n        raise ValueError(f\"Failed to parse page content: {str(e)}\")\n    \n    # Find the first table on the page\n    table = soup.find('table')\n    \n    if table is None:\n        raise ValueError(\"No table found on the page\")\n    \n    # Extract headers if present\n    headers = []\n    header_row = table.find('tr')\n    if header_row:\n        # Check if the first row contains <th> elements\n        th_elements = header_row.find_all('th')\n        if th_elements:\n            headers = [th.get_text(strip=True) for th in th_elements]\n    \n    # Extract all rows\n    rows = table.find_all('tr')\n    \n    if not rows:\n        raise ValueError(\"No table rows found\")\n    \n    # Extract data from rows\n    data = []\n    for row in rows:\n        # Get all <td> elements in the row\n        cells = row.find_all('td')\n        \n        # If this row has <td> elements, add it to data\n        if cells:\n            row_data = [cell.get_text(strip=True) for cell in cells]\n            data.append(row_data)\n        # If no <td> elements but has <th> elements and we haven't processed headers yet\n        elif not headers and row.find_all('th'):\n            # This might be a header row in the middle of the table\n            th_elements = row.find_all('th')\n            if not data:  # If we haven't collected any data yet, treat as headers\n                headers = [th.get_text(strip=True) for th in th_elements]\n    \n    if not data:\n        raise ValueError(\"No table data found\")\n    \n    # Create DataFrame\n    if headers:\n        # Ensure headers match the number of columns in data\n        max_cols = max(len(row) for row in data)\n        if len(headers) < max_cols:\n            # Pad headers with empty strings if necessary\n            headers.extend([''] * (max_cols - len(headers)))\n        elif len(headers) > max_cols:\n            # Trim headers if necessary\n            headers = headers[:max_cols]\n        \n        df = pd.DataFrame(data, columns=headers)\n    else:\n        df = pd.DataFrame(data)\n    \n    return df",
        "status": "fail",
        "details": {
          "test_http_error": "Traceback (most recent call last):\n  File \"__test__.py\", line 46, in task_func\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1114, in __call__\n    return self._mock_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1118, in _mock_call\n    return self._execute_mock_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1173, in _execute_mock_call\n    raise effect\nrequests.exceptions.HTTPError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 157, in test_http_error\n  File \"__test__.py\", line 53, in task_func\nConnectionError: Error making request to http://example.com/error: \n",
          "test_return_type": "Traceback (most recent call last):\n  File \"__test__.py\", line 57, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/bs4/__init__.py\", line 319, in __init__\n    for (self.markup, self.original_encoding, self.declared_html_encoding,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/builder/_htmlparser.py\", line 325, in prepare_markup\n    dammit = UnicodeDammit(markup, try_encodings, is_html=True,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 445, in __init__\n    for encoding in self.detector.encodings:\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 301, in encodings\n    self.declared_encoding = self.find_declared_encoding(\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 378, in find_declared_encoding\n    declared_encoding_match = xml_re.search(markup, endpos=xml_endpos)\nTypeError: expected string or bytes-like object\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 128, in test_return_type\n  File \"__test__.py\", line 59, in task_func\nValueError: Failed to parse page content: expected string or bytes-like object\n",
          "test_return_type_with_complex_data": "Traceback (most recent call last):\n  File \"__test__.py\", line 57, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/bs4/__init__.py\", line 319, in __init__\n    for (self.markup, self.original_encoding, self.declared_html_encoding,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/builder/_htmlparser.py\", line 325, in prepare_markup\n    dammit = UnicodeDammit(markup, try_encodings, is_html=True,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 445, in __init__\n    for encoding in self.detector.encodings:\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 301, in encodings\n    self.declared_encoding = self.find_declared_encoding(\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 378, in find_declared_encoding\n    declared_encoding_match = xml_re.search(markup, endpos=xml_endpos)\nTypeError: expected string or bytes-like object\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 176, in test_return_type_with_complex_data\n  File \"__test__.py\", line 59, in task_func\nValueError: Failed to parse page content: expected string or bytes-like object\n"
        }
      }
    ],
    "BigCodeBench/208": [
      {
        "task_id": "BigCodeBench/208",
        "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n\n    pass\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n    \"\"\"\n    Generate and draw a random sequence of \"elements\" number of steps. \n    The steps are either -1 or 1, and the sequence is plotted as a random walk.\n    \n    Parameters:\n    -----------\n    elements : int\n        Number of steps in the random walk\n    seed : int, optional\n        Random seed for reproducibility (default=0)\n    \n    Returns:\n    --------\n    dict: A dictionary containing the descriptive statistics of the random walk.\n    matplotlib.axes.Axes: The Axes object with the plotted random walk.\n    \n    Raises:\n    -------\n    ValueError: If elements is not a positive integer.\n    \"\"\"\n    \n    # Validate input\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n    \n    # Set random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate random steps (-1 or 1)\n    steps = np.random.choice([-1, 1], size=elements)\n    \n    # Calculate cumulative sum to get the random walk positions\n    walk = np.cumsum(steps)\n    \n    # Calculate descriptive statistics\n    stats = {\n        'count': len(walk),\n        'mean': np.mean(walk),\n        'std': np.std(walk, ddof=1),  # Sample standard deviation\n        'min': np.min(walk),\n        '5%': np.percentile(walk, 5),\n        '25%': np.percentile(walk, 25),\n        '50%': np.percentile(walk, 50),  # median\n        '75%': np.percentile(walk, 75),\n        '95%': np.percentile(walk, 95),\n        'max': np.max(walk)\n    }\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(range(elements), walk, 'b-', linewidth=1)\n    ax.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n    ax.set_xlabel('Step')\n    ax.set_ylabel('Position')\n    ax.set_title(f'Random Walk with {elements} Steps')\n    ax.grid(True, alpha=0.3)\n    \n    return stats, ax",
        "status": "fail",
        "details": {
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 114, in test_case_5\nAssertionError: 'Random Walk with 100 Steps' != 'Random Walk'\n- Random Walk with 100 Steps\n+ Random Walk\n\n"
        }
      }
    ],
    "BigCodeBench/274": [
      {
        "task_id": "BigCodeBench/274",
        "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n\n    pass\nimport cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    \"\"\"\n    Creates an HTTP POST request handler that processes incoming email data and sends an email.\n    \n    Args:\n        smtp_server: SMTP server address\n        smtp_port: SMTP server port\n        smtp_username: SMTP username for authentication\n        smtp_password: SMTP password for authentication\n    \n    Returns:\n        function: A class that handles HTTP POST requests and sends emails based on\n        the provided data.\n    \"\"\"\n    \n    class EmailRequestHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            # Get content length\n            content_length = int(self.headers.get('Content-Length', 0))\n            \n            # Read the request body\n            post_data = self.rfile.read(content_length)\n            \n            try:\n                # Parse JSON data\n                email_data = json.loads(post_data.decode('utf-8'))\n                \n                # Check for required fields\n                required_fields = ['subject', 'message', 'to']\n                for field in required_fields:\n                    if field not in email_data:\n                        raise ValueError(f\"Missing required field: {field}\")\n                \n                # Extract email details\n                subject = email_data['subject']\n                message = email_data['message']\n                to_address = email_data['to']\n                \n                # Create email message\n                msg = MIMEText(message)\n                msg['Subject'] = subject\n                msg['From'] = smtp_username\n                msg['To'] = to_address\n                \n                # Send email\n                try:\n                    # Connect to SMTP server\n                    server = smtplib.SMTP(smtp_server, smtp_port)\n                    server.starttls()  # Enable TLS encryption\n                    server.login(smtp_username, smtp_password)\n                    \n                    # Send the email\n                    server.send_message(msg)\n                    server.quit()\n                    \n                    # Send success response\n                    response_data = json.dumps({\"status\": \"success\", \"message\": \"Email sent successfully\"})\n                    self.send_response(200)\n                    self.send_header('Content-Type', 'application/json')\n                    self.send_header('Content-Length', str(len(response_data)))\n                    self.end_headers()\n                    self.wfile.write(response_data.encode('utf-8'))\n                    \n                except smtplib.SMTPAuthenticationError:\n                    # Authentication failed\n                    response_data = json.dumps({\"status\": \"error\", \"message\": \"Authentication Failed\"})\n                    self.send_response(535)\n                    self.send_header('Content-Type', 'application/json')\n                    self.send_header('Content-Length', str(len(response_data)))\n                    self.end_headers()\n                    self.wfile.write(response_data.encode('utf-8'))\n                    \n                except Exception as e:\n                    # Other SMTP errors\n                    response_data = json.dumps({\"status\": \"error\", \"message\": str(e)})\n                    self.send_response(500)\n                    self.send_header('Content-Type', 'application/json')\n                    self.send_header('Content-Length', str(len(response_data)))\n                    self.end_headers()\n                    self.wfile.write(response_data.encode('utf-8'))\n                    \n            except json.JSONDecodeError:\n                # Invalid JSON\n                response_data = json.dumps({\"status\": \"error\", \"message\": \"Invalid JSON data\"})\n                self.send_response(400)\n                self.send_header('Content-Type', 'application/json')\n                self.send_header('Content-Length', str(len(response_data)))\n                self.end_headers()\n                self.wfile.write(response_data.encode('utf-8'))\n                \n            except ValueError as e:\n                # Missing required fields\n                response_data = json.dumps({\"status\": \"error\", \"message\": str(e)})\n                self.send_response(400)\n                self.send_header('Content-Type', 'application/json')\n                self.send_header('Content-Length', str(len(response_data)))\n                self.end_headers()\n                self.wfile.write(response_data.encode('utf-8'))\n        \n        def log_message(self, format, *args):\n            # Override to suppress default logging\n            pass\n    \n    return EmailRequestHandler",
        "status": "fail",
        "details": {
          "test_email_sending_exception": "Traceback (most recent call last):\n  File \"__test__.py\", line 39, in do_POST\n  File \"/usr/local/lib/python3.10/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/local/lib/python3.10/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/usr/local/lib/python3.10/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 180, in test_email_sending_exception\n  File \"__test__.py\", line 99, in do_POST\n  File \"/usr/local/lib/python3.10/http/server.py\", line 519, in send_header\n    if self.request_version != 'HTTP/0.9':\nAttributeError: 'EmailRequestHandler' object has no attribute 'request_version'. Did you mean: 'server_version'?\n",
          "test_empty_json_data": "Traceback (most recent call last):\n  File \"__test__.py\", line 39, in do_POST\n  File \"/usr/local/lib/python3.10/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/local/lib/python3.10/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/usr/local/lib/python3.10/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 166, in test_empty_json_data\n  File \"__test__.py\", line 99, in do_POST\n  File \"/usr/local/lib/python3.10/http/server.py\", line 519, in send_header\n    if self.request_version != 'HTTP/0.9':\nAttributeError: 'EmailRequestHandler' object has no attribute 'request_version'. Did you mean: 'server_version'?\n",
          "test_invalid_content_type": "Traceback (most recent call last):\n  File \"__test__.py\", line 138, in test_invalid_content_type\n  File \"__test__.py\", line 35, in do_POST\nValueError: I/O operation on closed file.\n",
          "test_invalid_json_format": "Traceback (most recent call last):\n  File \"__test__.py\", line 39, in do_POST\n  File \"/usr/local/lib/python3.10/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/local/lib/python3.10/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/usr/local/lib/python3.10/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 160, in test_invalid_json_format\n  File \"__test__.py\", line 99, in do_POST\n  File \"/usr/local/lib/python3.10/http/server.py\", line 519, in send_header\n    if self.request_version != 'HTTP/0.9':\nAttributeError: 'EmailRequestHandler' object has no attribute 'request_version'. Did you mean: 'server_version'?\n",
          "test_missing_key_in_json_data": "Traceback (most recent call last):\n  File \"__test__.py\", line 39, in do_POST\n  File \"/usr/local/lib/python3.10/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/local/lib/python3.10/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/usr/local/lib/python3.10/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 144, in test_missing_key_in_json_data\n  File \"__test__.py\", line 99, in do_POST\n  File \"/usr/local/lib/python3.10/http/server.py\", line 519, in send_header\n    if self.request_version != 'HTTP/0.9':\nAttributeError: 'EmailRequestHandler' object has no attribute 'request_version'. Did you mean: 'server_version'?\n",
          "test_valid_json_request": "Traceback (most recent call last):\n  File \"__test__.py\", line 39, in do_POST\n  File \"/usr/local/lib/python3.10/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/local/lib/python3.10/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/usr/local/lib/python3.10/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 151, in test_valid_json_request\n  File \"__test__.py\", line 99, in do_POST\n  File \"/usr/local/lib/python3.10/http/server.py\", line 519, in send_header\n    if self.request_version != 'HTTP/0.9':\nAttributeError: 'EmailRequestHandler' object has no attribute 'request_version'. Did you mean: 'server_version'?\n"
        }
      }
    ],
    "BigCodeBench/308": [
      {
        "task_id": "BigCodeBench/308",
        "solution": "import pandas as pd\nfrom statistics import mean\nimport random\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\ndef task_func(additional_fields = []):\n\n    pass\nimport pandas as pd\nfrom statistics import mean\nimport random\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\ndef task_func(additional_fields = []):\n    \"\"\"\n    Create a report on students' grades in different subjects and then calculate \n    the average grade for each student and subject.\n    \n    Parameters:\n    additional_fields (list): Additional subject fields to include in the report\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the columns being subjects, each student's grades, \n               and their average grades. The DataFrame also includes the average grade per subject.\n    \"\"\"\n    # Combine default fields with additional fields\n    all_fields = FIELDS + additional_fields\n    \n    # Initialize the data dictionary\n    data = {}\n    \n    # Generate random grades for each student in each subject\n    for student in STUDENTS:\n        student_grades = []\n        for field in all_fields:\n            # Generate random grade between 0 and 100\n            grade = random.randint(0, 100)\n            student_grades.append(grade)\n        data[student] = student_grades\n    \n    # Create DataFrame with students as rows and subjects as columns\n    df = pd.DataFrame(data, index=all_fields).T\n    \n    # Calculate average grade for each student (row-wise average)\n    df['Average Grade'] = df.mean(axis=1)\n    \n    # Calculate average grade for each subject (column-wise average)\n    subject_averages = df.drop('Average Grade', axis=1).mean(axis=0)\n    \n    # Add the average row for subjects\n    average_row = pd.Series(subject_averages, name='Average')\n    \n    # Calculate the overall average for the 'Average Grade' column\n    average_row['Average Grade'] = df['Average Grade'].mean()\n    \n    # Append the average row to the DataFrame\n    df = pd.concat([df, average_row.to_frame().T])\n    \n    return df",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/184": [
      {
        "task_id": "BigCodeBench/184",
        "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\ndef task_func(dataframe, text_column):\n\n    pass\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\ndef task_func(dataframe, text_column):\n    \"\"\"\n    Prepares and transforms text data from a specified column in a DataFrame by removing stopwords, \n    numbers, and punctuation, and subsequently applying a vectorization process to convert text \n    into a numeric format suitable for analysis.\n    \n    Parameters:\n    dataframe (pd.DataFrame): The input DataFrame containing text data\n    text_column (str): The name of the column containing text to process\n    \n    Returns:\n    DataFrame: Returns a DataFrame with each word (after preprocessing) as a column and their count as rows.\n    \"\"\"\n    \n    # Check if the column exists in the dataframe\n    if text_column not in dataframe.columns:\n        raise ValueError(f\"Column '{text_column}' not found in DataFrame\")\n    \n    # Extract the text data from the specified column\n    text_data = dataframe[text_column].astype(str).tolist()\n    \n    # Preprocess function to clean text\n    def preprocess_text(text):\n        # Convert to lowercase\n        text = text.lower()\n        \n        # Remove numbers and punctuation, keep only alphabetic characters and spaces\n        text = re.sub(r'[^a-z\\s]', '', text)\n        \n        # Split into words\n        words = text.split()\n        \n        # Remove stopwords\n        words = [word for word in words if word not in STOPWORDS]\n        \n        # Join back into a string\n        return ' '.join(words)\n    \n    # Apply preprocessing to all text data\n    processed_texts = [preprocess_text(text) for text in text_data]\n    \n    # Create CountVectorizer instance\n    vectorizer = CountVectorizer()\n    \n    # Fit and transform the processed text data\n    count_matrix = vectorizer.fit_transform(processed_texts)\n    \n    # Get feature names (words)\n    feature_names = vectorizer.get_feature_names_out()\n    \n    # Convert sparse matrix to dense array\n    count_array = count_matrix.toarray()\n    \n    # Create DataFrame with words as columns and counts as rows\n    result_df = pd.DataFrame(count_array, columns=feature_names)\n    \n    return result_df",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/310": [
      {
        "task_id": "BigCodeBench/310",
        "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\ndef task_func(filename):\n\n    pass\nimport os\nimport csv\nimport random\nfrom statistics import mean\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\ndef task_func(filename):\n    \"\"\"\n    Generates a CSV file containing simulated data for 100 people, including name, age, height, and weight.\n    It also calculates and appends the average age, height, and weight at the end of the file.\n    \n    Args:\n        filename (str): The name of the CSV file to create\n        \n    Returns:\n        str: The path of the created CSV file\n    \"\"\"\n    # Lists of sample first and last names\n    first_names = ['John', 'Jane', 'Bob', 'Alice', 'Charlie', 'Emma', 'Oliver', 'Sophia', \n                   'William', 'Isabella', 'James', 'Mia', 'Benjamin', 'Charlotte', 'Lucas',\n                   'Amelia', 'Henry', 'Evelyn', 'Alexander', 'Harper']\n    \n    last_names = ['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller', \n                  'Davis', 'Rodriguez', 'Martinez', 'Hernandez', 'Lopez', 'Gonzalez',\n                  'Wilson', 'Anderson', 'Thomas', 'Taylor', 'Moore', 'Jackson', 'Martin']\n    \n    # Generate data for 100 people\n    people_data = []\n    ages = []\n    heights = []\n    weights = []\n    \n    for i in range(PEOPLE_COUNT):\n        # Generate random name\n        name = f\"{random.choice(first_names)} {random.choice(last_names)}\"\n        \n        # Generate random age (18-80)\n        age = random.randint(18, 80)\n        ages.append(age)\n        \n        # Generate random height in cm (150-200)\n        height = random.randint(150, 200)\n        heights.append(height)\n        \n        # Generate random weight in kg (45-120)\n        weight = random.randint(45, 120)\n        weights.append(weight)\n        \n        people_data.append([name, age, height, weight])\n    \n    # Calculate averages\n    avg_age = mean(ages)\n    avg_height = mean(heights)\n    avg_weight = mean(weights)\n    \n    # Write to CSV file\n    filepath = os.path.abspath(filename)\n    \n    with open(filepath, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        \n        # Write header\n        writer.writerow(COLUMNS)\n        \n        # Write people data\n        for person in people_data:\n            writer.writerow(person)\n        \n        # Write averages\n        writer.writerow([])  # Empty row for separation\n        writer.writerow(['AVERAGES', f'{avg_age:.2f}', f'{avg_height:.2f}', f'{avg_weight:.2f}'])\n    \n    return filepath",
        "status": "fail",
        "details": {
          "test_average_row_label": "Traceback (most recent call last):\n  File \"__test__.py\", line 136, in test_average_row_label\nAssertionError: 'AVERAGES' != 'Average'\n- AVERAGES\n+ Average\n\n",
          "test_file_content_rows": "Traceback (most recent call last):\n  File \"__test__.py\", line 106, in test_file_content_rows\nAssertionError: 103 != 102\n",
          "test_averages_calculation": "Traceback (most recent call last):\n  File \"__test__.py\", line 115, in test_averages_calculation\n  File \"__test__.py\", line 115, in <listcomp>\nIndexError: list index out of range\n"
        }
      }
    ],
    "BigCodeBench/326": [
      {
        "task_id": "BigCodeBench/326",
        "solution": "import subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n\n    pass\nimport subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n    \"\"\"\n    Find and run all .bat files in a given directory, returning their file names and exit codes.\n    \n    Args:\n        directory_path (str): Path to the directory containing .bat files\n        \n    Returns:\n        list of tuples: A list where each tuple contains the file name and its exit code.\n        The exit code is None if the file could not be executed.\n    \"\"\"\n    results = []\n    \n    # Check if directory exists\n    if not os.path.exists(directory_path):\n        return results\n    \n    # Find all .bat files in the directory\n    bat_pattern = os.path.join(directory_path, \"*.bat\")\n    bat_files = glob.glob(bat_pattern)\n    \n    # Run each .bat file and collect results\n    for bat_file in bat_files:\n        file_name = os.path.basename(bat_file)\n        exit_code = None\n        \n        try:\n            # Run the .bat file\n            # shell=True is needed for .bat files on Windows\n            # capture_output=True to prevent output from cluttering console\n            result = subprocess.run(\n                bat_file,\n                shell=True,\n                capture_output=True,\n                text=True,\n                timeout=60  # 60 second timeout to prevent hanging\n            )\n            exit_code = result.returncode\n            \n        except subprocess.TimeoutExpired:\n            # Handle timeout\n            exit_code = None\n            \n        except Exception as e:\n            # Handle any other execution errors\n            exit_code = None\n        \n        results.append((file_name, exit_code))\n    \n    return results",
        "status": "fail",
        "details": {
          "test_exception_handling": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 109, in test_exception_handling\nAssertionError: Lists differ: [] != [('file1.bat', None)]\n\nSecond list contains 1 additional elements.\nFirst extra element 0:\n('file1.bat', None)\n\n- []\n+ [('file1.bat', None)]\n",
          "test_multiple_bat_files_mixed_results": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 102, in test_multiple_bat_files_mixed_results\nAssertionError: Lists differ: [] != [('file1.bat', 0), ('file2.bat', 1), ('file3.bat', None)]\n\nSecond list contains 3 additional elements.\nFirst extra element 0:\n('file1.bat', 0)\n\n- []\n+ [('file1.bat', 0), ('file2.bat', 1), ('file3.bat', None)]\n",
          "test_single_bat_file_failure": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 89, in test_single_bat_file_failure\nAssertionError: Lists differ: [] != [('file1.bat', 1)]\n\nSecond list contains 1 additional elements.\nFirst extra element 0:\n('file1.bat', 1)\n\n- []\n+ [('file1.bat', 1)]\n",
          "test_single_bat_file_success": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 80, in test_single_bat_file_success\nAssertionError: Lists differ: [] != [('file1.bat', 0)]\n\nSecond list contains 1 additional elements.\nFirst extra element 0:\n('file1.bat', 0)\n\n- []\n+ [('file1.bat', 0)]\n"
        }
      }
    ],
    "BigCodeBench/108": [
      {
        "task_id": "BigCodeBench/108",
        "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n\n    pass\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    Decomposes a time series in the 'value' column of a DataFrame into trend, seasonality, and residuals.\n    \n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        DataFrame containing the time series data with a 'value' column\n    freq : str, default='D'\n        Frequency string for the time series (e.g., 'D' for daily, 'M' for monthly)\n    decomposition_model : str, default='multiplicative'\n        Type of decomposition model ('additive' or 'multiplicative')\n    \n    Returns:\n    --------\n    tuple: A tuple containing the decomposition result (DecomposeResult object) and the matplotlib Axes object.\n    \n    Raises:\n    -------\n    ValueError: If 'df' is not a DataFrame, lacks required columns, or contains invalid data types.\n    ValueError: If 'freq' is not a valid frequency string.\n    ValueError: If 'decomposition_model' is not 'additive' or 'multiplicative'.\n    \"\"\"\n    \n    # Validate input DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' must be a pandas DataFrame\")\n    \n    # Check if 'value' column exists\n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'value' column\")\n    \n    # Check if 'value' column contains numeric data\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"'value' column must contain numeric data\")\n    \n    # Check for NaN values in 'value' column\n    if df['value'].isna().any():\n        raise ValueError(\"'value' column contains NaN values\")\n    \n    # Validate decomposition model\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"'decomposition_model' must be 'additive' or 'multiplicative'\")\n    \n    # Validate frequency string\n    valid_freq_strings = ['D', 'W', 'M', 'Q', 'A', 'H', 'T', 'S', 'L', 'U', 'N',\n                         'B', 'C', 'W-MON', 'W-TUE', 'W-WED', 'W-THU', 'W-FRI', \n                         'W-SAT', 'W-SUN', 'MS', 'BM', 'BMS', 'QS', 'BQ', 'BQS', \n                         'AS', 'BA', 'BAS']\n    \n    # Extract base frequency for validation\n    base_freq = freq\n    if freq and any(char.isdigit() for char in freq):\n        # Handle frequencies like '2D', '3M', etc.\n        import re\n        match = re.match(r'(\\d+)?([A-Z\\-]+)', freq)\n        if match:\n            base_freq = match.group(2)\n    \n    if base_freq not in valid_freq_strings:\n        raise ValueError(f\"'{freq}' is not a valid frequency string\")\n    \n    # Create a copy of the DataFrame to avoid modifying the original\n    df_copy = df.copy()\n    \n    # Ensure the DataFrame has a datetime index\n    if not isinstance(df_copy.index, pd.DatetimeIndex):\n        # Try to convert the index to datetime\n        try:\n            df_copy.index = pd.to_datetime(df_copy.index)\n        except:\n            # If index conversion fails, create a date range\n            df_copy.index = pd.date_range(start='2020-01-01', periods=len(df_copy), freq=freq)\n    \n    # Set the frequency of the index\n    df_copy.index.freq = freq\n    \n    # Perform seasonal decomposition\n    decomposition = seasonal_decompose(df_copy['value'], \n                                     model=decomposition_model,\n                                     extrapolate_trend='freq')\n    \n    # Create the plot\n    fig, axes = plt.subplots(4, 1, figsize=(12, 10))\n    \n    # Plot original series\n    df_copy['value'].plot(ax=axes[0], title='Original Time Series')\n    axes[0].set_ylabel('Value')\n    \n    # Plot trend\n    decomposition.trend.plot(ax=axes[1], title='Trend Component')\n    axes[1].set_ylabel('Trend')\n    \n    # Plot seasonal\n    decomposition.seasonal.plot(ax=axes[2], title='Seasonal Component')\n    axes[2].set_ylabel('Seasonal')\n    \n    # Plot residual\n    decomposition.resid.plot(ax=axes[3], title='Residual Component')\n    axes[3].set_ylabel('Residual')\n    \n    # Adjust layout\n    plt.tight_layout()\n    \n    return decomposition, axes",
        "status": "fail",
        "details": {
          "test_return_type": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py\", line 1917, in _validate_frequency\n    raise ValueError\nValueError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 129, in test_return_type\n  File \"__test__.py\", line 85, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/datetimelike.py\", line 100, in freq\n    self._data.freq = value  # type: ignore[misc]\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py\", line 1883, in freq\n    self._validate_frequency(self, value)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py\", line 1928, in _validate_frequency\n    raise ValueError(\nValueError: Inferred frequency N from passed values does not conform to passed frequency D\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 132, in test_return_type\nAssertionError: Unexpected ValueError raised: Inferred frequency N from passed values does not conform to passed frequency D\n",
          "test_additive_model": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py\", line 1917, in _validate_frequency\n    raise ValueError\nValueError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 168, in test_additive_model\n  File \"__test__.py\", line 85, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/datetimelike.py\", line 100, in freq\n    self._data.freq = value  # type: ignore[misc]\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py\", line 1883, in freq\n    self._validate_frequency(self, value)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py\", line 1928, in _validate_frequency\n    raise ValueError(\nValueError: Inferred frequency N from passed values does not conform to passed frequency D\n",
          "test_component_shapes": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py\", line 1917, in _validate_frequency\n    raise ValueError\nValueError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 162, in test_component_shapes\n  File \"__test__.py\", line 85, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/datetimelike.py\", line 100, in freq\n    self._data.freq = value  # type: ignore[misc]\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py\", line 1883, in freq\n    self._validate_frequency(self, value)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py\", line 1928, in _validate_frequency\n    raise ValueError(\nValueError: Inferred frequency N from passed values does not conform to passed frequency D\n",
          "test_components_existence": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py\", line 1917, in _validate_frequency\n    raise ValueError\nValueError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 156, in test_components_existence\n  File \"__test__.py\", line 85, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/datetimelike.py\", line 100, in freq\n    self._data.freq = value  # type: ignore[misc]\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py\", line 1883, in freq\n    self._validate_frequency(self, value)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py\", line 1928, in _validate_frequency\n    raise ValueError(\nValueError: Inferred frequency N from passed values does not conform to passed frequency D\n"
        }
      }
    ],
    "BigCodeBench/324": [
      {
        "task_id": "BigCodeBench/324",
        "solution": "import subprocess\nimport time\nimport threading\ndef task_func(file_list):\n\n    pass\nimport subprocess\nimport time\nimport threading\ndef task_func(file_list):\n    \"\"\"\n    Run files from list of files as subprocesses at the same time.\n    \n    Args:\n        file_list: List of file paths to execute as subprocesses\n        \n    Returns:\n        list: The exit codes of the subprocesses\n    \"\"\"\n    # List to store subprocess objects\n    processes = []\n    \n    # Start all subprocesses\n    for file_path in file_list:\n        try:\n            # Start subprocess without waiting for it to complete\n            # Using shell=True to handle different file types (scripts, executables)\n            process = subprocess.Popen(\n                file_path,\n                shell=True,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE\n            )\n            processes.append(process)\n        except Exception as e:\n            print(f\"Error starting {file_path}: {e}\")\n            # Append None to maintain order\n            processes.append(None)\n    \n    # Wait for all processes to complete and collect exit codes\n    exit_codes = []\n    for i, process in enumerate(processes):\n        if process is None:\n            # If process failed to start, use exit code -1\n            exit_codes.append(-1)\n        else:\n            # Wait for process to complete and get exit code\n            exit_code = process.wait()\n            exit_codes.append(exit_code)\n    \n    return exit_codes",
        "status": "fail",
        "details": {
          "test_multiple_processes_with_different_exit_codes": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 88, in test_multiple_processes_with_different_exit_codes\nAssertionError: Lists differ: [<MagicMock name='Popen().wait()' id='1399[121 chars]36'>] != [0, 1, None]\n\nFirst differing element 0:\n<MagicMock name='Popen().wait()' id='139918978238736'>\n0\n\n+ [0, 1, None]\n- [<MagicMock name='Popen().wait()' id='139918978238736'>,\n-  <MagicMock name='Popen().wait()' id='139918978238736'>,\n-  <MagicMock name='Popen().wait()' id='139918978238736'>]\n",
          "test_process_still_running": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 82, in test_process_still_running\nAssertionError: Lists differ: [<MagicMock name='Popen().wait()' id='139918978177760'>] != [None]\n\nFirst differing element 0:\n<MagicMock name='Popen().wait()' id='139918978177760'>\nNone\n\n- [<MagicMock name='Popen().wait()' id='139918978177760'>]\n+ [None]\n",
          "test_valid_directory_and_files": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 75, in test_valid_directory_and_files\nAssertionError: Lists differ: [<MagicMock name='Popen().wait()' id='1399[65 chars]52'>] != [0, 0]\n\nFirst differing element 0:\n<MagicMock name='Popen().wait()' id='139918976432352'>\n0\n\n+ [0, 0]\n- [<MagicMock name='Popen().wait()' id='139918976432352'>,\n-  <MagicMock name='Popen().wait()' id='139918976432352'>]\n",
          "test_valid_files": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 67, in test_valid_files\nAssertionError: Lists differ: [<MagicMock name='Popen().wait()' id='1399[65 chars]68'>] != [0, 0]\n\nFirst differing element 0:\n<MagicMock name='Popen().wait()' id='139918975475168'>\n0\n\n+ [0, 0]\n- [<MagicMock name='Popen().wait()' id='139918975475168'>,\n-  <MagicMock name='Popen().wait()' id='139918975475168'>]\n"
        }
      }
    ],
    "BigCodeBench/139": [
      {
        "task_id": "BigCodeBench/139",
        "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n\n    pass\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    \"\"\"\n    Draw histograms of numeric columns in a DataFrame and return the plots.\n    \n    Each histogram represents the distribution of values in one numeric column,\n    with the column name as the plot title, 'Value' as the x-axis label,\n    and 'Frequency' as the y-axis label.\n    \n    Parameters:\n    df (pd.DataFrame): Input DataFrame\n    \n    Returns:\n    list: A list of Matplotlib Axes objects, each representing a histogram for a numeric column.\n    \n    Raises:\n    ValueError: If the input is not a non-empty DataFrame or if there are no numeric columns in the DataFrame.\n    \"\"\"\n    # Check if input is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame\")\n    \n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame cannot be empty\")\n    \n    # Get numeric columns\n    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n    \n    # Check if there are numeric columns\n    if not numeric_columns:\n        raise ValueError(\"DataFrame must contain at least one numeric column\")\n    \n    # Create list to store axes objects\n    axes_list = []\n    \n    # Create a histogram for each numeric column\n    for col in numeric_columns:\n        # Create a new figure and axis for each histogram\n        fig, ax = plt.subplots(figsize=(8, 6))\n        \n        # Plot histogram\n        ax.hist(df[col].dropna(), bins='auto', edgecolor='black')\n        \n        # Set labels and title\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        ax.set_title(col)\n        \n        # Add grid for better readability\n        ax.grid(True, alpha=0.3)\n        \n        # Append axis to list\n        axes_list.append(ax)\n    \n    return axes_list",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/287": [
      {
        "task_id": "BigCodeBench/287",
        "solution": "from collections import Counter\nimport os\nimport json\ndef task_func(filename, directory):\n\n    pass\nfrom collections import Counter\nimport os\nimport json\ndef task_func(filename, directory):\n    \"\"\"\n    Count the number of words in .txt files within a specified directory,\n    export the counts to a JSON file, and then return the total number of words.\n    \n    Args:\n        filename (str): The name of the JSON file to save word counts\n        directory (str): The directory path containing .txt files\n    \n    Returns:\n        int: total number of words in .txt files\n    \"\"\"\n    word_counts = {}\n    total_words = 0\n    \n    # Check if directory exists\n    if not os.path.exists(directory):\n        # Save empty JSON and return 0\n        with open(filename, 'w') as f:\n            json.dump(word_counts, f)\n        return total_words\n    \n    # Iterate through all files in the directory\n    for file in os.listdir(directory):\n        # Check if file is a .txt file\n        if file.endswith('.txt'):\n            file_path = os.path.join(directory, file)\n            \n            try:\n                # Read the file and count words\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                    # Split by whitespace to get words\n                    words = content.split()\n                    word_count = len(words)\n                    \n                    # Store count for this file\n                    word_counts[file] = word_count\n                    total_words += word_count\n                    \n            except Exception as e:\n                # If there's an error reading the file, skip it\n                print(f\"Error reading {file}: {e}\")\n                continue\n    \n    # Export word counts to JSON file\n    with open(filename, 'w') as f:\n        json.dump(word_counts, f, indent=2)\n    \n    return total_words",
        "status": "fail",
        "details": {
          "test_empty_files": "Traceback (most recent call last):\n  File \"__test__.py\", line 109, in test_empty_files\nAssertionError: {'empty_file.txt': 0} != {}\n- {'empty_file.txt': 0}\n+ {}\n",
          "test_files_with_special_characters": "Traceback (most recent call last):\n  File \"__test__.py\", line 122, in test_files_with_special_characters\nAssertionError: {'special_chars.txt': 2} != {'hello-world': 1, 'hello_python': 1}\n- {'special_chars.txt': 2}\n+ {'hello-world': 1, 'hello_python': 1}\n",
          "test_multiple_files": "Traceback (most recent call last):\n  File \"__test__.py\", line 98, in test_multiple_files\nAssertionError: {'first.txt': 2, 'second.txt': 3, 'third.txt': 2} != {'hello': 2, 'world': 2, 'python': 2, 'coding': 1}\n- {'first.txt': 2, 'second.txt': 3, 'third.txt': 2}\n+ {'coding': 1, 'hello': 2, 'python': 2, 'world': 2}\n",
          "test_nested_directories": "Traceback (most recent call last):\n  File \"__test__.py\", line 136, in test_nested_directories\nAssertionError: {'nested_file.txt': 3} != {'hello': 2, 'world': 1}\n- {'nested_file.txt': 3}\n+ {'hello': 2, 'world': 1}\n",
          "test_single_file_few_words": "Traceback (most recent call last):\n  File \"__test__.py\", line 83, in test_single_file_few_words\nAssertionError: {'single_file.txt': 3} != {'hello': 2, 'world': 1}\n- {'single_file.txt': 3}\n+ {'hello': 2, 'world': 1}\n"
        }
      }
    ],
    "BigCodeBench/147": [
      {
        "task_id": "BigCodeBench/147",
        "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\ndef task_func(ip_range, port):\n\n    pass\nimport socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\ndef task_func(ip_range, port):\n    \"\"\"\n    Scans a specified IP address range and checks if a specified port is open on each IP.\n    \n    Args:\n        ip_range (str): IP address range in CIDR notation (e.g., \"192.168.1.0/24\")\n        port (int): Port number to check\n    \n    Returns:\n        dict: A dictionary mapping IP addresses to their port status (True if open).\n    \"\"\"\n    results = {}\n    threads = []\n    \n    def check_port(ip, port, results):\n        \"\"\"Check if a port is open on a given IP address.\"\"\"\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(1)  # 1 second timeout\n        try:\n            result = sock.connect_ex((str(ip), port))\n            if result == 0:\n                results[str(ip)] = True\n            else:\n                results[str(ip)] = False\n        except socket.error:\n            results[str(ip)] = False\n        finally:\n            sock.close()\n    \n    # Parse the IP range\n    try:\n        network = IPv4Network(ip_range, strict=False)\n    except ValueError as e:\n        print(f\"Invalid IP range: {e}\")\n        return results\n    \n    # Create threads for each IP in the range\n    for ip in network.hosts():\n        thread = Thread(target=check_port, args=(ip, port, results))\n        threads.append(thread)\n        thread.start()\n    \n    # Wait for all threads to complete\n    for thread in threads:\n        thread.join()\n    \n    return results",
        "status": "fail",
        "details": {
          "test_all_ips_checked": "Traceback (most recent call last):\n  File \"__test__.py\", line 86, in test_all_ips_checked\nAssertionError: Items in the second set but not the first:\n'192.168.0.0'\n'192.168.0.3' : All IPs in the range should be checked.\n",
          "test_open_port": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 73, in test_open_port\nAssertionError: False is not true : At least one port should be open for the test range.\n"
        }
      }
    ],
    "BigCodeBench/162": [
      {
        "task_id": "BigCodeBench/162",
        "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n\n    pass\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n    \"\"\"\n    Analyzes and visualizes the distribution of word lengths in a text.\n    \n    Parameters:\n    -----------\n    text : str\n        The input text to analyze\n    rwidth : float, optional\n        The relative width of the histogram bars (default: 0.8)\n    \n    Returns:\n    --------\n    matplotlib.axes.Axes: An Axes object containing the histogram of word lengths.\n    \n    Note:\n    -----\n    If there are no words in the input text, or all words are filtered out, \n    the histogram will be empty as no bins will be created.\n    \"\"\"\n    # Extract words from the text using regex\n    # This pattern matches sequences of word characters (letters, digits, underscore)\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Calculate the length of each word\n    word_lengths = [len(word) for word in words]\n    \n    # Create a figure and axis\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    if word_lengths:\n        # If we have words, create the histogram\n        # Determine the range of word lengths\n        min_length = min(word_lengths)\n        max_length = max(word_lengths)\n        \n        # Create bins for each possible word length\n        bins = np.arange(min_length, max_length + 2) - 0.5\n        \n        # Create the histogram\n        n, bins, patches = ax.hist(word_lengths, bins=bins, rwidth=rwidth, \n                                   edgecolor='black', alpha=0.7)\n        \n        # Set labels and title\n        ax.set_xlabel('Word Length', fontsize=12)\n        ax.set_ylabel('Frequency', fontsize=12)\n        ax.set_title('Distribution of Word Lengths', fontsize=14, fontweight='bold')\n        \n        # Set x-axis to show integer values only\n        ax.set_xticks(range(min_length, max_length + 1))\n        \n        # Add grid for better readability\n        ax.grid(axis='y', alpha=0.3, linestyle='--')\n        \n        # Add statistics text\n        mean_length = np.mean(word_lengths)\n        median_length = np.median(word_lengths)\n        ax.text(0.02, 0.98, f'Total words: {len(word_lengths)}\\n'\n                           f'Mean length: {mean_length:.2f}\\n'\n                           f'Median length: {median_length:.1f}',\n                transform=ax.transAxes, verticalalignment='top',\n                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n    else:\n        # If no words found, create an empty plot with a message\n        ax.text(0.5, 0.5, 'No words found in the text', \n                transform=ax.transAxes, ha='center', va='center',\n                fontsize=14, color='gray')\n        ax.set_xlabel('Word Length', fontsize=12)\n        ax.set_ylabel('Frequency', fontsize=12)\n        ax.set_title('Distribution of Word Lengths', fontsize=14, fontweight='bold')\n    \n    # Adjust layout to prevent label cutoff\n    plt.tight_layout()\n    \n    return ax",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/368": [
      {
        "task_id": "BigCodeBench/368",
        "solution": "import os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n\n    pass\nimport os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed: int = 100) -> str:\n    \"\"\"\n    Moves a random file from the source directory to the specified destination directory.\n    \n    Args:\n        src_dir: Source directory path\n        dest_dir: Destination directory path\n        seed: Random seed for reproducibility (default: 100)\n    \n    Returns:\n        str: The name of the file moved. Format: 'filename.extension' (e.g., 'file1.txt').\n    \n    Raises:\n        ValueError: If source directory doesn't exist or is empty\n        ValueError: If destination directory doesn't exist\n    \"\"\"\n    # Set random seed for reproducibility\n    random.seed(seed)\n    \n    # Check if source directory exists\n    if not os.path.exists(src_dir):\n        raise ValueError(f\"Source directory '{src_dir}' does not exist\")\n    \n    # Check if destination directory exists\n    if not os.path.exists(dest_dir):\n        raise ValueError(f\"Destination directory '{dest_dir}' does not exist\")\n    \n    # Get list of files (not directories) in source directory\n    files = [f for f in os.listdir(src_dir) \n             if os.path.isfile(os.path.join(src_dir, f))]\n    \n    # Check if there are any files to move\n    if not files:\n        raise ValueError(f\"No files found in source directory '{src_dir}'\")\n    \n    # Select a random file\n    selected_file = random.choice(files)\n    \n    # Construct full paths\n    src_path = os.path.join(src_dir, selected_file)\n    dest_path = os.path.join(dest_dir, selected_file)\n    \n    # Move the file\n    shutil.move(src_path, dest_path)\n    \n    return selected_file",
        "status": "fail",
        "details": {
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 107, in test_case_3\n  File \"__test__.py\", line 43, in task_func\nValueError: No files found in source directory '/tmp/tmplpse3dp_/test/src_test_dir_3'\n"
        }
      }
    ],
    "BigCodeBench/241": [
      {
        "task_id": "BigCodeBench/241",
        "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n\n    pass\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n    \"\"\"\n    Create a numeric array from the \"original\" list, normalize the array, \n    and draw the original and normalized arrays.\n    \n    Parameters:\n    original (list): List of numeric values to be normalized and plotted\n    \n    Returns:\n    np.array: A numpy array for the original data.\n    np.array: Normalized array.\n    matplotlib.axes.Axes: Axes object with the plotted data.\n    \"\"\"\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n    \n    # Reshape for sklearn preprocessing (needs 2D array)\n    original_reshaped = original_array.reshape(-1, 1)\n    \n    # Create a MinMaxScaler instance\n    scaler = preprocessing.MinMaxScaler()\n    \n    # Fit and transform the data\n    normalized_reshaped = scaler.fit_transform(original_reshaped)\n    \n    # Flatten back to 1D array\n    normalized_array = normalized_reshaped.flatten()\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot both arrays\n    x = np.arange(len(original_array))\n    ax.plot(x, original_array, 'b-', label='Original', linewidth=2, marker='o')\n    ax.plot(x, normalized_array, 'r-', label='Normalized', linewidth=2, marker='s')\n    \n    # Set title and labels\n    ax.set_title('Original vs. Normalized Data', fontsize=14, fontweight='bold')\n    ax.set_xlabel('Index', fontsize=12)\n    ax.set_ylabel('Value', fontsize=12)\n    \n    # Add legend\n    ax.legend()\n    \n    # Add grid for better readability\n    ax.grid(True, alpha=0.3)\n    \n    # Return the required outputs\n    return original_array, normalized_array, ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 65, in test_case_1\n  File \"__test__.py\", line 33, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 157, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 916, in fit_transform\n    return self.fit(X, **fit_params).transform(X)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 435, in fit\n    return self.partial_fit(X, y)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 473, in partial_fit\n    X = self._validate_data(\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 605, in _validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'a'\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 78, in test_case_2\n  File \"__test__.py\", line 33, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 157, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 916, in fit_transform\n    return self.fit(X, **fit_params).transform(X)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 435, in fit\n    return self.partial_fit(X, y)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 473, in partial_fit\n    X = self._validate_data(\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 605, in _validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'a'\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 94, in test_case_3\n  File \"__test__.py\", line 33, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 157, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 916, in fit_transform\n    return self.fit(X, **fit_params).transform(X)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 435, in fit\n    return self.partial_fit(X, y)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 473, in partial_fit\n    X = self._validate_data(\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 605, in _validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'a'\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 107, in test_case_4\n  File \"__test__.py\", line 33, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 157, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 916, in fit_transform\n    return self.fit(X, **fit_params).transform(X)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 435, in fit\n    return self.partial_fit(X, y)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 473, in partial_fit\n    X = self._validate_data(\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 605, in _validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'a'\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 124, in test_case_5\n  File \"__test__.py\", line 33, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 157, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 916, in fit_transform\n    return self.fit(X, **fit_params).transform(X)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 435, in fit\n    return self.partial_fit(X, y)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 473, in partial_fit\n    X = self._validate_data(\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 605, in _validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 967, in check_array\n    raise ValueError(\nValueError: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by MinMaxScaler.\n"
        }
      }
    ],
    "BigCodeBench/124": [
      {
        "task_id": "BigCodeBench/124",
        "solution": "from random import randint,seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n\n    pass\nfrom random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n    \"\"\"\n    Enhances 'my_list' by appending the number 12, then generates a list of random integers \n    based on the sum of elements in 'my_list', limited by 'size'. It measures the time taken \n    for this process and plots a histogram of the generated random numbers.\n    \n    Parameters:\n    - my_list: list of numeric values\n    - size: maximum size of the random numbers list (default: 100)\n    - seed: random seed for reproducibility (default: 100)\n    \n    Returns:\n    - tuple: (time_taken, axes_object)\n    \n    Raises:\n    - TypeError: If 'my_list' is not a list\n    - ValueError: If 'my_list' contains non-numeric elements\n    \"\"\"\n    \n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list\")\n    \n    # Check if all elements in my_list are numeric\n    for element in my_list:\n        if not isinstance(element, (int, float)):\n            raise ValueError(\"All elements in my_list must be numeric (int or float)\")\n    \n    # Set random seed for reproducibility\n    random_seed(seed)\n    \n    # Start timing\n    start_time = time.time()\n    \n    # Append 12 to my_list\n    my_list.append(12)\n    \n    # Calculate the sum of elements in my_list\n    list_sum = sum(my_list)\n    \n    # Determine the size of random numbers list (limited by 'size' parameter)\n    random_list_size = min(int(list_sum), size)\n    \n    # Generate list of random integers between 1 and 100 (inclusive)\n    random_numbers = [randint(1, 100) for _ in range(random_list_size)]\n    \n    # End timing\n    end_time = time.time()\n    time_taken = end_time - start_time\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    \n    # Plot histogram with bins from 1 to 101 (to include 100)\n    ax.hist(random_numbers, bins=range(1, 102), edgecolor='black', alpha=0.7)\n    \n    # Set labels\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Random Numbers')\n    \n    # Set x-axis limits to show full range\n    ax.set_xlim(0, 101)\n    \n    # Return tuple of time taken and axes object\n    return (time_taken, ax)",
        "status": "fail",
        "details": {
          "test_output_types": "Traceback (most recent call last):\n  File \"__test__.py\", line 86, in test_output_types\nAssertionError: Lists differ: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0[453 chars] 0.0] != [2.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0[53 chars] 2.0]\n\nFirst differing element 0:\n0.0\n2.0\n\nFirst list contains 80 additional elements.\nFirst extra element 20:\n0.0\n\nDiff is 928 characters long. Set self.maxDiff to None to see it. : DataFrame contents should match the expected output\n",
          "test_plot_title": "Traceback (most recent call last):\n  File \"__test__.py\", line 96, in test_plot_title\nAssertionError: 'Distribution of Random Numbers' != 'Histogram of Random Numbers'\n- Distribution of Random Numbers\n? ^    ^^^^^^^\n+ Histogram of Random Numbers\n? ^   ++ ^^\n\n"
        }
      }
    ],
    "BigCodeBench/93": [
      {
        "task_id": "BigCodeBench/93",
        "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n\n    pass\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on a dataset and generate a scatter plot.\n    \n    Parameters:\n    -----------\n    data : array-like or DataFrame\n        The input data to perform PCA on\n    n_components : int, default=2\n        Number of principal components to keep\n        \n    Returns:\n    --------\n    DataFrame: The transformed data with principal components\n    Axes: The matplotlib Axes object containing the scatter plot\n    \n    Raises:\n    -------\n    ValueError: If n_components is not a positive integer\n    \"\"\"\n    \n    # Validate n_components\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n    \n    # Convert data to numpy array if it's a DataFrame\n    if isinstance(data, pd.DataFrame):\n        data_array = data.values\n        feature_names = data.columns.tolist()\n    else:\n        data_array = np.array(data)\n        feature_names = [f\"Feature_{i}\" for i in range(data_array.shape[1])]\n    \n    # Check if n_components is valid for the dataset\n    if n_components > min(data_array.shape):\n        raise ValueError(f\"n_components ({n_components}) cannot be larger than min(n_samples, n_features)\")\n    \n    # Perform PCA\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data_array)\n    \n    # Create DataFrame with transformed data\n    pc_columns = [f\"PC{i+1}\" for i in range(n_components)]\n    transformed_df = pd.DataFrame(transformed_data, columns=pc_columns)\n    \n    # Create scatter plot\n    fig, ax = plt.subplots(figsize=(8, 6))\n    \n    if n_components >= 2:\n        # 2D scatter plot for first two components\n        ax.scatter(transformed_data[:, 0], transformed_data[:, 1], alpha=0.6)\n        ax.set_xlabel('PC1')\n        ax.set_ylabel('PC2')\n        ax.set_title('PCA Scatter Plot')\n    else:\n        # 1D scatter plot if only one component\n        ax.scatter(transformed_data[:, 0], np.zeros_like(transformed_data[:, 0]), alpha=0.6)\n        ax.set_xlabel('PC1')\n        ax.set_ylabel('Value')\n        ax.set_title('PCA Scatter Plot (1D)')\n    \n    ax.grid(True, alpha=0.3)\n    \n    # Add explained variance ratio to the plot\n    explained_var_ratio = pca.explained_variance_ratio_\n    if n_components >= 2:\n        ax.text(0.02, 0.98, f'Explained Variance:\\nPC1: {explained_var_ratio[0]:.2%}\\nPC2: {explained_var_ratio[1]:.2%}',\n                transform=ax.transAxes, verticalalignment='top',\n                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n    else:\n        ax.text(0.02, 0.98, f'Explained Variance:\\nPC1: {explained_var_ratio[0]:.2%}',\n                transform=ax.transAxes, verticalalignment='top',\n                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n    \n    return transformed_df, ax",
        "status": "fail",
        "details": {
          "test_values": "Traceback (most recent call last):\n  File \"__test__.py\", line 122, in test_values\nAssertionError: -0.039555132621790036 != -0.36270132751314693 within 7 places (0.3231461948913569 difference) : DataFrame contents should match the expected output\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 125, in test_values\nAssertionError: -0.039555132621790036 != 0.36270132751314693 within 7 places (0.402256460134937 difference) : DataFrame contents should match the expected output\n"
        }
      }
    ],
    "BigCodeBench/239": [
      {
        "task_id": "BigCodeBench/239",
        "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n\n    pass\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n    \"\"\"\n    Given a list of tuples, extract numeric values, compute basic statistics, \n    and generate a histogram with an overlaid probability density function (PDF).\n    \n    Parameters:\n    original (list): A list of tuples containing various data types\n    \n    Returns:\n    np.array: A numpy array of the extracted numeric values.\n    dict: Basic statistics for the array including mean, standard deviation, minimum, and maximum.\n    Axes: A matplotlib Axes object showing the histogram with overlaid PDF.\n    \"\"\"\n    # Extract numeric values from the list of tuples\n    numeric_values = []\n    for tup in original:\n        for item in tup:\n            # Check if the item is numeric (int or float)\n            if isinstance(item, (int, float)) and not isinstance(item, bool):\n                numeric_values.append(item)\n    \n    # Convert to numpy array\n    numeric_array = np.array(numeric_values)\n    \n    # Handle empty array case\n    if len(numeric_array) == 0:\n        # Create empty plot\n        fig, ax = plt.subplots()\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Density')\n        ax.set_title('Histogram with PDF')\n        return np.array([]), {}, ax\n    \n    # Compute basic statistics\n    statistics = {\n        'mean': np.mean(numeric_array),\n        'standard_deviation': np.std(numeric_array, ddof=1) if len(numeric_array) > 1 else 0,\n        'minimum': np.min(numeric_array),\n        'maximum': np.max(numeric_array)\n    }\n    \n    # Create histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    \n    # Plot histogram\n    n, bins, patches = ax.hist(numeric_array, bins='auto', density=True, \n                               alpha=0.6, color='blue', edgecolor='black')\n    \n    # Calculate and plot PDF\n    # Use kernel density estimation for the PDF\n    if len(numeric_array) > 1:\n        # Create a range of x values for the PDF\n        x_range = np.linspace(numeric_array.min(), numeric_array.max(), 100)\n        \n        # Fit a normal distribution to the data\n        mu, sigma = statistics['mean'], statistics['standard_deviation']\n        \n        # Calculate the PDF values\n        pdf_values = stats.norm.pdf(x_range, mu, sigma)\n        \n        # Plot the PDF\n        ax.plot(x_range, pdf_values, 'r-', linewidth=2, label='PDF')\n        ax.legend()\n    \n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.set_title('Histogram with PDF')\n    \n    return numeric_array, statistics, ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 88, in test_case_1\nAssertionError: {'mean': 2.5, 'standard_deviation': 1.2909944487358056, 'm[20 chars]': 4} != {'mean': 2.5, 'std': 1.118033988749895, 'min': 1, 'max': 4}\n+ {'max': 4, 'mean': 2.5, 'min': 1, 'std': 1.118033988749895}\n- {'maximum': 4,\n-  'mean': 2.5,\n-  'minimum': 1,\n-  'standard_deviation': 1.2909944487358056}\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 95, in test_case_2\nAssertionError: {'mean': 15.0, 'standard_deviation': 7.0710678118654755, 'm[22 chars]: 20} != {'mean': 15.0, 'std': 5.0, 'min': 10, 'max': 20}\n+ {'max': 20, 'mean': 15.0, 'min': 10, 'std': 5.0}\n- {'maximum': 20,\n-  'mean': 15.0,\n-  'minimum': 10,\n-  'standard_deviation': 7.0710678118654755}\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 102, in test_case_3\nAssertionError: {'mean': -10.0, 'standard_deviation': 5.0, 'minimum': -15, 'maximum': -5} != {'mean': -10.0, 'std': 4.08248290463863, 'min': -15, 'max': -5}\n- {'maximum': -5, 'mean': -10.0, 'minimum': -15, 'standard_deviation': 5.0}\n+ {'max': -5, 'mean': -10.0, 'min': -15, 'std': 4.08248290463863}\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 109, in test_case_4\nAssertionError: {'mean': 0.0, 'standard_deviation': 0.0, 'minimum': 0, 'maximum': 0} != {'mean': 0.0, 'std': 0.0, 'min': 0, 'max': 0}\n- {'maximum': 0, 'mean': 0.0, 'minimum': 0, 'standard_deviation': 0.0}\n?      ----                       ----         -- -------------\n\n+ {'max': 0, 'mean': 0.0, 'min': 0, 'std': 0.0}\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 116, in test_case_5\nAssertionError: {'mean': 6.5, 'standard_deviation': 1.0, 'minimum': 5.5, 'maximum': 7.5} != {'mean': 6.5, 'std': 0.816496580927726, 'min': 5.5, 'max': 7.5}\n- {'maximum': 7.5, 'mean': 6.5, 'minimum': 5.5, 'standard_deviation': 1.0}\n+ {'max': 7.5, 'mean': 6.5, 'min': 5.5, 'std': 0.816496580927726}\n"
        }
      }
    ],
    "BigCodeBench/313": [
      {
        "task_id": "BigCodeBench/313",
        "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\ndef task_func(directory):\n\n    pass\nimport re\nimport os\nimport shutil\nfrom datetime import datetime\ndef task_func(directory):\n    \"\"\"\n    Organize files in a directory based on the first text that is not enclosed in square brackets.\n    Move the files to subdirectories named after this text. If no matching text is found, the file is not moved.\n    \n    Args:\n        directory (str): The directory path to organize\n        \n    Returns:\n        tuple:\n            str: The directory path with organized files.\n            dict: A dictionary where keys are the created subdirectories and values are lists of files moved to them.\n    \"\"\"\n    # Dictionary to track moved files\n    moved_files = {}\n    \n    # Ensure the directory exists\n    if not os.path.exists(directory):\n        raise ValueError(f\"Directory {directory} does not exist\")\n    \n    # Get all files in the directory (not subdirectories)\n    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n    \n    for filename in files:\n        # Extract the base name without extension\n        base_name = os.path.splitext(filename)[0]\n        \n        # Pattern to find text not in square brackets\n        # This regex finds sequences of characters that are not within square brackets\n        # First, we need to remove all text within square brackets\n        text_without_brackets = re.sub(r'\\[.*?\\]', '', base_name)\n        \n        # Now find the first non-empty word/text\n        match = re.search(r'\\S+', text_without_brackets.strip())\n        \n        if match:\n            # Get the first matching text\n            folder_name = match.group(0)\n            \n            # Create subdirectory if it doesn't exist\n            subdirectory = os.path.join(directory, folder_name)\n            if not os.path.exists(subdirectory):\n                os.makedirs(subdirectory)\n            \n            # Move the file\n            source = os.path.join(directory, filename)\n            destination = os.path.join(subdirectory, filename)\n            \n            try:\n                shutil.move(source, destination)\n                \n                # Track the moved file\n                if folder_name not in moved_files:\n                    moved_files[folder_name] = []\n                moved_files[folder_name].append(filename)\n            except Exception as e:\n                print(f\"Error moving file {filename}: {e}\")\n    \n    return directory, moved_files",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 102, in test_case_1\nAssertionError: 'example' not found in {'test_file1': ['test_file1.txt']}\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 112, in test_case_2\nAssertionError: 'example' not found in {'test_file1': ['test_file1.txt'], 'test_file2': ['test_file2.txt'], 'test_file3': ['test_file3.txt']}\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 125, in test_case_4\nAssertionError: {'test_file1': ['test_file1.txt']} != {}\n- {'test_file1': ['test_file1.txt']}\n+ {}\n"
        }
      }
    ],
    "BigCodeBench/267": [
      {
        "task_id": "BigCodeBench/267",
        "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n\n    pass\nimport numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n    \"\"\"\n    Given a dictionary \"data\", this function performs the following operations:\n    1. Adds a new key \"a\" with the value 1 to the dictionary.\n    2. Generates a signal based on the values in \"data\".\n    3. Runs a Fast Fourier Transform (FFT) on the signal.\n    4. Plots and returns the FFT of the signal with a title of 'FFT of the signal'.\n    \n    Parameters:\n    data (dict): Input dictionary\n    sample_rate (int): Sample rate for the signal (default: 8000)\n    \n    Returns:\n    tuple: A tuple containing:\n        ndarray: The FFT of the signal.\n        Axes: The plot of the FFT.\n    \"\"\"\n    # Step 1: Add a new key \"a\" with value 1 to the dictionary\n    data['a'] = 1\n    \n    # Step 2: Generate a signal based on the values in \"data\"\n    # Create a time array\n    duration = 1.0  # 1 second duration\n    t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)\n    \n    # Generate signal as sum of sine waves based on dictionary values\n    signal = np.zeros_like(t)\n    for key, value in data.items():\n        if isinstance(value, (int, float)):\n            # Use the value as frequency multiplier\n            frequency = value * 100  # Base frequency of 100 Hz\n            signal += np.sin(2 * np.pi * frequency * t)\n    \n    # Step 3: Run FFT on the signal\n    fft_signal = fftpack.fft(signal)\n    \n    # Calculate frequency bins\n    freq = fftpack.fftfreq(len(signal), 1/sample_rate)\n    \n    # Step 4: Plot the FFT\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot only the positive frequencies (first half of the FFT)\n    positive_freq_idx = freq >= 0\n    ax.plot(freq[positive_freq_idx], np.abs(fft_signal[positive_freq_idx]))\n    \n    ax.set_title('FFT of the signal')\n    ax.set_xlabel('Frequency (Hz)')\n    ax.set_ylabel('Magnitude')\n    ax.grid(True)\n    \n    # Set x-axis limit to show relevant frequency range\n    ax.set_xlim(0, sample_rate/2)  # Nyquist frequency\n    \n    plt.tight_layout()\n    \n    return fft_signal, ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 81, in test_case_1\nAssertionError: 'FFT of the signal' != 'FFT of the Signal'\n- FFT of the signal\n?            ^\n+ FFT of the Signal\n?            ^\n\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 95, in test_case_2\nAssertionError: 'FFT of the signal' != 'FFT of the Signal'\n- FFT of the signal\n?            ^\n+ FFT of the Signal\n?            ^\n\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 109, in test_case_3\nAssertionError: 'FFT of the signal' != 'FFT of the Signal'\n- FFT of the signal\n?            ^\n+ FFT of the Signal\n?            ^\n\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 124, in test_case_4\nAssertionError: 'FFT of the signal' != 'FFT of the Signal'\n- FFT of the signal\n?            ^\n+ FFT of the Signal\n?            ^\n\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 139, in test_case_5\nAssertionError: 'FFT of the signal' != 'FFT of the Signal'\n- FFT of the signal\n?            ^\n+ FFT of the Signal\n?            ^\n\n"
        }
      }
    ],
    "BigCodeBench/401": [
      {
        "task_id": "BigCodeBench/401",
        "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\ndef task_func(app_name):\n\n    pass\nfrom flask import Flask\nimport os\nfrom flask_mail import Mail\ndef task_func(app_name):\n    \"\"\"\n    Initializes a Flask-Mail instance for sending emails using the generated Flask application with the specified app_name.\n    \n    Note that: The details of the email server are retrieved from environment variables. If the variables do not exist, use defaults:\n    'MAIL_SERVER': 'localhost'\n    'MAIL_PORT': 25\n    'MAIL_USE_TLS': False (boolean)\n    'MAIL_USERNAME': None\n    'MAIL_PASSWORD': None\n    \n    The function should output with:\n        tuple: A tuple containing the Flask-Mail instance and the app's mail configurations.\n    \"\"\"\n    # Create Flask application\n    app = Flask(app_name)\n    \n    # Configure mail settings from environment variables with defaults\n    app.config['MAIL_SERVER'] = os.environ.get('MAIL_SERVER', 'localhost')\n    app.config['MAIL_PORT'] = int(os.environ.get('MAIL_PORT', 25))\n    \n    # Handle MAIL_USE_TLS as boolean\n    mail_use_tls = os.environ.get('MAIL_USE_TLS', 'False')\n    if isinstance(mail_use_tls, str):\n        app.config['MAIL_USE_TLS'] = mail_use_tls.lower() in ('true', '1', 'yes', 'on')\n    else:\n        app.config['MAIL_USE_TLS'] = bool(mail_use_tls)\n    \n    app.config['MAIL_USERNAME'] = os.environ.get('MAIL_USERNAME', None)\n    app.config['MAIL_PASSWORD'] = os.environ.get('MAIL_PASSWORD', None)\n    \n    # Initialize Flask-Mail\n    mail = Mail(app)\n    \n    # Extract mail configurations\n    mail_configs = {\n        'MAIL_SERVER': app.config['MAIL_SERVER'],\n        'MAIL_PORT': app.config['MAIL_PORT'],\n        'MAIL_USE_TLS': app.config['MAIL_USE_TLS'],\n        'MAIL_USERNAME': app.config['MAIL_USERNAME'],\n        'MAIL_PASSWORD': app.config['MAIL_PASSWORD']\n    }\n    \n    return (mail, mail_configs)",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/399": [
      {
        "task_id": "BigCodeBench/399",
        "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n\n    pass\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n    \"\"\"\n    Create a diagram of a sine wave and cosine wave with a given frequency.\n    \n    Parameters:\n    -----------\n    frequency : float\n        The frequency of the sine and cosine waves\n    sample_size : int, optional\n        The number of sample points (default is 10000)\n    \n    Returns:\n    --------\n    matplotlib.figure.Figure: The figure object containing the plot\n    matplotlib.axes.Axes: The axes object of the plot\n    \n    Raises:\n    -------\n    ValueError: If the frequency is negative\n    ValueError: If the sample size is non-positive\n    \"\"\"\n    \n    # Validate inputs\n    if frequency < 0:\n        raise ValueError(\"Frequency must be non-negative\")\n    \n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive\")\n    \n    # Create figure and axes\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Generate x values\n    # We'll plot over 2 periods for better visualization\n    if frequency > 0:\n        period = 1 / frequency\n        x_max = 2 * period\n    else:\n        # For frequency = 0, just plot over a fixed range\n        x_max = 2\n    \n    x = np.linspace(0, x_max, sample_size)\n    \n    # Calculate y values\n    y_sin = np.sin(2 * np.pi * frequency * x)\n    y_cos = np.cos(2 * np.pi * frequency * x)\n    \n    # Plot the waves\n    ax.plot(x, y_sin, 'b-', label=f'sin(2\u03c0\u00b7{frequency}\u00b7t)', linewidth=2)\n    ax.plot(x, y_cos, 'r-', label=f'cos(2\u03c0\u00b7{frequency}\u00b7t)', linewidth=2)\n    \n    # Add grid\n    ax.grid(True, alpha=0.3)\n    \n    # Add labels and title\n    ax.set_xlabel('Time (s)', fontsize=12)\n    ax.set_ylabel('Amplitude', fontsize=12)\n    ax.set_title(f'Sine and Cosine Waves (Frequency = {frequency} Hz)', fontsize=14)\n    \n    # Add legend\n    ax.legend(loc='upper right')\n    \n    # Set y-axis limits\n    ax.set_ylim(-1.5, 1.5)\n    \n    # Add horizontal line at y=0\n    ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n    \n    return fig, ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 84, in test_case_1\nAssertionError: 3 != 2\n"
        }
      }
    ],
    "BigCodeBench/100": [
      {
        "task_id": "BigCodeBench/100",
        "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(seed=42):\n\n    pass\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(seed=42):\n    \"\"\"\n    Generates a plot of random time series data for the past 30 days with \n    reproducibility controlled by an optional seed parameter. The plot is \n    styled with Arial font for better readability.\n    \n    Parameters:\n    seed (int): Random seed for reproducibility. Default is 42.\n    \n    Returns:\n    matplotlib.axes.Axes: The Axes object containing a line plot of the time series data.\n    The plot will have 'Date' as the x-axis label, 'Value' as the y-axis label,\n    and 'Random Time Series Data' as the title.\n    \n    Raises:\n    ValueError: If there is an issue generating the data or plot.\n    \"\"\"\n    try:\n        # Set random seed for reproducibility\n        random.seed(seed)\n        \n        # Generate dates for the past 30 days\n        end_date = datetime.now()\n        date_range = pd.date_range(end=end_date, periods=30, freq='D')\n        \n        # Generate random values\n        values = [random.uniform(0, 100) for _ in range(30)]\n        \n        # Create DataFrame\n        df = pd.DataFrame({\n            'Date': date_range,\n            'Value': values\n        })\n        \n        # Create the plot\n        fig, ax = plt.subplots(figsize=(10, 6))\n        \n        # Plot the time series data\n        ax.plot(df['Date'], df['Value'], marker='o', linestyle='-', linewidth=2, markersize=6)\n        \n        # Set labels and title with Arial font\n        ax.set_xlabel('Date', fontname='Arial', fontsize=12)\n        ax.set_ylabel('Value', fontname='Arial', fontsize=12)\n        ax.set_title('Random Time Series Data', fontname='Arial', fontsize=14, fontweight='bold')\n        \n        # Format x-axis to show dates nicely\n        ax.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m-%d'))\n        plt.xticks(rotation=45, ha='right', fontname='Arial')\n        \n        # Set y-axis tick font\n        for tick in ax.yaxis.get_major_ticks():\n            tick.label.set_fontname('Arial')\n        \n        # Add grid for better readability\n        ax.grid(True, alpha=0.3, linestyle='--')\n        \n        # Adjust layout to prevent label cutoff\n        plt.tight_layout()\n        \n        return ax\n        \n    except Exception as e:\n        raise ValueError(f\"Error generating data or plot: {str(e)}\")",
        "status": "fail",
        "details": {
          "test_value": "Traceback (most recent call last):\n  File \"__test__.py\", line 111, in test_value\nAssertionError: Lists differ: [81, 14, 3, 94, 35, 31, 28, 17, 94, 13, 86[69 chars], 69] != [63.942679845788376, 2.5010755222666936, 2[531 chars]8911]\n\nFirst differing element 0:\n81\n63.942679845788376\n\nDiff is 874 characters long. Set self.maxDiff to None to see it. : DataFrame contents should match the expected output\n"
        }
      }
    ],
    "BigCodeBench/177": [
      {
        "task_id": "BigCodeBench/177",
        "solution": "import re\nimport nltk\nfrom string import punctuation\ndef task_func(df):\n\n    pass\nimport re\nimport nltk\nfrom string import punctuation\ndef task_func(df):\n    \"\"\"\n    Extracts articles whose titles contain specific case-insensitive keywords (\"like\" or \"what\") \n    from a DataFrame and analyzes the frequency of each word in the content of these articles, \n    excluding punctuation.\n    \n    Parameters:\n    df: pandas DataFrame with 'Title' and 'Content' columns\n    \n    Returns:\n    dict: A dictionary with keys as words and values as their corresponding frequency, \n          excluding any punctuation marks.\n    \n    Raises:\n    ValueError: If the DataFrame is empty or does not contain the necessary columns \n                'Title' and 'Content'.\n    \"\"\"\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    # Check if required columns exist\n    if 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'Title' and 'Content' columns\")\n    \n    # Filter articles with titles containing 'like' or 'what' (case-insensitive)\n    pattern = r'\\b(like|what)\\b'\n    filtered_df = df[df['Title'].str.contains(pattern, case=False, regex=True, na=False)]\n    \n    # If no articles match the criteria, return empty dictionary\n    if filtered_df.empty:\n        return {}\n    \n    # Combine all content from filtered articles\n    all_content = ' '.join(filtered_df['Content'].astype(str))\n    \n    # Remove punctuation and convert to lowercase\n    # Create translation table to remove punctuation\n    translator = str.maketrans('', '', punctuation)\n    cleaned_content = all_content.translate(translator).lower()\n    \n    # Split into words\n    words = cleaned_content.split()\n    \n    # Count word frequencies\n    word_freq = {}\n    for word in words:\n        if word:  # Skip empty strings\n            word_freq[word] = word_freq.get(word, 0) + 1\n    \n    return word_freq",
        "status": "fail",
        "details": {
          "test_case_sensitive_handling": "Traceback (most recent call last):\n  File \"__test__.py\", line 128, in test_case_sensitive_handling\nAssertionError: {'technological': 1, 'growth': 1, 'is': 1, [85 chars]': 1} != {'Technological': 1, 'growth': 1, 'is': 1, [85 chars]': 1}\n+ {'Technological': 1,\n+  'These': 1,\n- {'advancements': 1,\n? ^\n\n+  'advancements': 1,\n? ^\n\n   'are': 1,\n   'exponential': 1,\n   'growth': 1,\n   'is': 1,\n   'like': 1,\n   'no': 1,\n-  'other': 1,\n?            ^\n\n+  'other': 1}\n?            ^\n\n-  'technological': 1,\n-  'these': 1} : Case sensitivity handling is faulty.\n",
          "test_word_frequencies": "Traceback (most recent call last):\n  File \"__test__.py\", line 91, in test_word_frequencies\nAssertionError: {'data': 2, 'science': 1, 'is': 2, 'about':[132 chars]': 1} != {'Data': 1, 'Science': 1, 'is': 2, 'about':[154 chars]': 1}\n+ {'Data': 1,\n+  'Like': 1,\n+  'Python': 1,\n+  'Science': 1,\n+  'What': 1,\n- {'about': 1,\n? ^\n\n+  'about': 1,\n? ^\n\n   'among': 1,\n   'analysis': 1,\n-  'data': 2,\n?          ^\n\n+  'data': 1,\n?          ^\n\n   'developers': 1,\n   'do': 1,\n   'is': 2,\n-  'like': 1,\n   'popular': 1,\n-  'python': 1,\n-  'science': 1,\n   'see': 1,\n   'think': 1,\n-  'what': 2,\n?          ^\n\n+  'what': 1,\n?          ^\n\n   'you': 2} : The word frequencies do not match the expected output.\n",
          "test_empty_dataframe": "Traceback (most recent call last):\n  File \"__test__.py\", line 110, in test_empty_dataframe\n  File \"__test__.py\", line 29, in task_func\nValueError: DataFrame is empty\n"
        }
      }
    ],
    "BigCodeBench/367": [
      {
        "task_id": "BigCodeBench/367",
        "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n\n    pass\nfrom datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n    \"\"\"\n    Return a bar chart of the number of activities performed on each day of the week\n    based on the provided list of activities.\n    \n    Parameters:\n    activities (list): List of datetime objects representing activities\n    \n    Returns:\n    matplotlib.axes.Axes: Axes object representing the bar chart\n    \n    Raises:\n    TypeError: If the activities are not datetime objects\n    \"\"\"\n    # Check if all activities are datetime objects\n    for activity in activities:\n        if not isinstance(activity, datetime):\n            raise TypeError(\"If the activities are not datetime objects.\")\n    \n    # Count activities by day of week\n    day_counts = defaultdict(int)\n    \n    for activity in activities:\n        # Get day name (Monday, Tuesday, etc.)\n        day_name = activity.strftime('%A')\n        day_counts[day_name] += 1\n    \n    # Define the order of days\n    days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    \n    # Prepare data for plotting\n    days = []\n    counts = []\n    \n    for day in days_order:\n        days.append(day)\n        counts.append(day_counts.get(day, 0))\n    \n    # Create the bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(days, counts)\n    \n    # Set labels and title\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n    \n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45, ha='right')\n    \n    # Adjust layout to prevent label cutoff\n    plt.tight_layout()\n    \n    return ax",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/360": [
      {
        "task_id": "BigCodeBench/360",
        "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n\n    pass\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n    \"\"\"\n    Load data from an Excel spreadsheet (.xlsx), calculate the mean and standard deviation \n    of each column, and draw a bar chart.\n    \n    Parameters:\n    file_location (str): Path to the Excel file\n    sheet_name (str): Name of the sheet to read\n    \n    Returns:\n    dict: A dictionary with mean and standard deviation of each column.\n    matplotlib.figure.Figure: The figure object containing the bar chart.\n    \n    Raises:\n    FileNotFoundError: If the Excel file does not exist at the specified path.\n    ValueError: If the specified sheet does not exist in the workbook.\n    \"\"\"\n    \n    # Check if file exists\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(f\"The file {file_location} does not exist.\")\n    \n    try:\n        # Load the Excel file\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError as e:\n        # Check if the error is due to sheet not found\n        if \"Worksheet\" in str(e) and \"not found\" in str(e):\n            raise ValueError(f\"The sheet '{sheet_name}' does not exist in the workbook.\")\n        else:\n            raise e\n    \n    # Calculate mean and standard deviation for numeric columns only\n    numeric_df = df.select_dtypes(include=[np.number])\n    \n    # Calculate statistics\n    means = numeric_df.mean()\n    stds = numeric_df.std()\n    \n    # Create dictionary with results\n    results = {}\n    for col in numeric_df.columns:\n        results[col] = {\n            'mean': means[col],\n            'std': stds[col]\n        }\n    \n    # Create bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Prepare data for plotting\n    columns = list(numeric_df.columns)\n    mean_values = [means[col] for col in columns]\n    std_values = [stds[col] for col in columns]\n    \n    # Set the width of bars and positions\n    x = np.arange(len(columns))\n    width = 0.35\n    \n    # Create bars\n    bars1 = ax.bar(x - width/2, mean_values, width, label='Mean', alpha=0.8)\n    bars2 = ax.bar(x + width/2, std_values, width, label='Std Dev', alpha=0.8)\n    \n    # Add labels and title\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xticks(x)\n    ax.set_xticklabels(columns, rotation=45, ha='right')\n    ax.legend()\n    \n    # Adjust layout to prevent label cutoff\n    plt.tight_layout()\n    \n    return results, fig",
        "status": "fail",
        "details": {
          "test_correct_mean_and_std_values": "Traceback (most recent call last):\n  File \"__test__.py\", line 128, in test_correct_mean_and_std_values\nAssertionError: {'A':[19 chars]d': 14.142135623730951}, 'B': {'mean': 30.0, '[21 chars]951}} != {'A':[19 chars]d': 10.0}, 'B': {'mean': 30.0, 'std': 10.0}}\n+ {'A': {'mean': 20.0, 'std': 10.0}, 'B': {'mean': 30.0, 'std': 10.0}}\n- {'A': {'mean': 20.0, 'std': 14.142135623730951},\n-  'B': {'mean': 30.0, 'std': 14.142135623730951}}\n",
          "test_value": "Traceback (most recent call last):\n  File \"__test__.py\", line 138, in test_value\nAssertionError: {'A':[19 chars]d': 10.0}, 'B': {'mean': 30.0, 'std': 10.0}} != {'A':[19 chars]d': 14.142135623730951}, 'B': {'mean': 30.0, '[21 chars]951}}\n- {'A': {'mean': 20.0, 'std': 10.0}, 'B': {'mean': 30.0, 'std': 10.0}}\n+ {'A': {'mean': 20.0, 'std': 14.142135623730951},\n+  'B': {'mean': 30.0, 'std': 14.142135623730951}}\n"
        }
      }
    ],
    "BigCodeBench/409": [
      {
        "task_id": "BigCodeBench/409",
        "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n\n    pass\nimport os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n    \"\"\"\n    Calculate the mean, median, and standard deviation of the data from a specific column in an Excel file.\n    \n    Parameters:\n    excel_file_path (str): The directory path where the Excel file is located\n    file_name (str): The name of the Excel file\n    column_name (str): The name of the column to analyze\n    \n    Returns:\n    dict: A dictionary with the mean, median, and standard deviation\n    \n    Raises:\n    FileNotFoundError: If the Excel file does not exist at the specified path\n    ValueError: If the specified column is not found in the Excel file\n    \"\"\"\n    \n    # Construct the full file path\n    full_path = os.path.join(excel_file_path, file_name)\n    \n    # Check if the file exists\n    if not os.path.exists(full_path):\n        raise FileNotFoundError(f\"Excel file not found at: {full_path}\")\n    \n    try:\n        # Read the Excel file\n        df = pd.read_excel(full_path)\n        \n        # Check if the column exists\n        if column_name not in df.columns:\n            raise ValueError(f\"Column '{column_name}' not found in the Excel file. Available columns: {list(df.columns)}\")\n        \n        # Extract the column data\n        column_data = df[column_name]\n        \n        # Remove any NaN values for calculation\n        column_data = column_data.dropna()\n        \n        # Calculate statistics\n        mean_value = np.mean(column_data)\n        median_value = np.median(column_data)\n        std_value = np.std(column_data, ddof=1)  # Using sample standard deviation\n        \n        # Return the results as a dictionary\n        return {\n            'mean': mean_value,\n            'median': median_value,\n            'standard_deviation': std_value\n        }\n        \n    except Exception as e:\n        # Re-raise specific exceptions we want to handle\n        if isinstance(e, ValueError):\n            raise\n        # For any other exception related to reading the file\n        raise ValueError(f\"Error reading Excel file: {str(e)}\")",
        "status": "fail",
        "details": {
          "test_file_with_different_data_types": "Traceback (most recent call last):\n  File \"__test__.py\", line 85, in tearDown\nOSError: [Errno 39] Directory not empty: './test_data/'\n",
          "test_invalid_column_name": "Traceback (most recent call last):\n  File \"__test__.py\", line 85, in tearDown\nOSError: [Errno 39] Directory not empty: './test_data/'\n",
          "test_non_existent_file": "Traceback (most recent call last):\n  File \"__test__.py\", line 85, in tearDown\nOSError: [Errno 39] Directory not empty: './test_data/'\n",
          "test_normal_functionality": "Traceback (most recent call last):\n  File \"__test__.py\", line 85, in tearDown\nOSError: [Errno 39] Directory not empty: './test_data/'\n"
        }
      }
    ],
    "BigCodeBench/374": [
      {
        "task_id": "BigCodeBench/374",
        "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n\n    pass\nimport regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n    \"\"\"\n    Protects all double quotes in all Excel (.xlsx) files in the specified directory \n    by prefixing them with a double backslash.\n    \n    Args:\n        directory_path (str): Path to the directory containing Excel files. \n                             Defaults to './xlsx_files/'\n    \n    Returns:\n        int: The number of Excel files processed.\n    \"\"\"\n    # Ensure the directory path ends with a separator\n    if not directory_path.endswith(os.sep):\n        directory_path += os.sep\n    \n    # Find all .xlsx files in the directory\n    xlsx_pattern = os.path.join(directory_path, '*.xlsx')\n    xlsx_files = glob.glob(xlsx_pattern)\n    \n    files_processed = 0\n    \n    for file_path in xlsx_files:\n        try:\n            # Load the workbook\n            workbook = load_workbook(file_path)\n            modified = False\n            \n            # Process each worksheet\n            for sheet_name in workbook.sheetnames:\n                worksheet = workbook[sheet_name]\n                \n                # Process each cell in the worksheet\n                for row in worksheet.iter_rows():\n                    for cell in row:\n                        if cell.value is not None and isinstance(cell.value, str):\n                            # Check if the cell contains double quotes\n                            if '\"' in cell.value:\n                                # Replace unescaped double quotes with escaped ones\n                                # This regex finds double quotes that are NOT already preceded by \\\\\n                                new_value = re.sub(r'(?<!\\\\\\\\)\"', r'\\\\\"', cell.value)\n                                if new_value != cell.value:\n                                    cell.value = new_value\n                                    modified = True\n            \n            # Save the workbook if any modifications were made\n            if modified:\n                workbook.save(file_path)\n            \n            # Close the workbook\n            workbook.close()\n            \n            files_processed += 1\n            \n        except Exception as e:\n            print(f\"Error processing file {file_path}: {str(e)}\")\n            continue\n    \n    return files_processed",
        "status": "fail",
        "details": {
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 158, in test_case_4\nAssertionError: FileNotFoundError not raised\n"
        }
      }
    ],
    "BigCodeBench/302": [
      {
        "task_id": "BigCodeBench/302",
        "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Constants\nCOLUMNS = ['Date', 'Value']\ndef task_func(df, plot=False):\n\n    pass\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df, plot=False):\n    \"\"\"\n    Processes a pandas DataFrame by splitting lists in the 'Value' column into separate columns,\n    calculates the Pearson correlation coefficient between these columns, and optionally \n    visualizes the correlation matrix using a heatmap.\n    \n    Parameters:\n    df (DataFrame): Input DataFrame with 'Date' and 'Value' columns where 'Value' contains lists\n    plot (bool): If True, displays a correlation heatmap\n    \n    Returns:\n    DataFrame: A pandas DataFrame containing the correlation coefficients among the lists in the 'Value' column.\n    Axes (optional): A matplotlib Axes object containing the heatmap plot, returned if 'plot' is True.\n    \n    Raises:\n    ValueError: If the DataFrame input is empty or have invalid 'Value'\n    \"\"\"\n    \n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    # Check if 'Value' column exists\n    if 'Value' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'Value' column\")\n    \n    # Check if all values in 'Value' column are lists\n    if not all(isinstance(val, list) for val in df['Value']):\n        raise ValueError(\"All values in 'Value' column must be lists\")\n    \n    # Check if all lists have the same length\n    list_lengths = [len(val) for val in df['Value']]\n    if len(set(list_lengths)) > 1:\n        raise ValueError(\"All lists in 'Value' column must have the same length\")\n    \n    # Check if lists are not empty\n    if any(len(val) == 0 for val in df['Value']):\n        raise ValueError(\"Lists in 'Value' column cannot be empty\")\n    \n    # Create a new DataFrame with expanded values\n    # Each list element becomes a separate column\n    expanded_data = {}\n    for i, row in df.iterrows():\n        for j, value in enumerate(row['Value']):\n            col_name = f'Value_{j}'\n            if col_name not in expanded_data:\n                expanded_data[col_name] = []\n            expanded_data[col_name].append(value)\n    \n    # Create DataFrame from expanded data\n    expanded_df = pd.DataFrame(expanded_data)\n    \n    # Calculate correlation matrix\n    correlation_matrix = expanded_df.corr()\n    \n    # Create heatmap if requested\n    if plot:\n        fig, ax = plt.subplots(figsize=(10, 8))\n        sns.heatmap(correlation_matrix, \n                    annot=True, \n                    cmap='coolwarm', \n                    center=0, \n                    ax=ax,\n                    fmt='.2f',\n                    square=True,\n                    linewidths=0.5,\n                    cbar_kws={\"shrink\": 0.8})\n        ax.set_title('Correlation Heatmap')\n        plt.tight_layout()\n        return correlation_matrix, ax\n    \n    return correlation_matrix",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/34": [
      {
        "task_id": "BigCodeBench/34",
        "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n\n    pass\nimport re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    \"\"\"\n    Create a word cloud from text after removing URLs and plot it.\n    \n    Parameters:\n    text (str): The input text from which to generate the word cloud.\n    \n    Returns:\n    WordCloud object: The generated word cloud.\n    \n    Raises:\n    ValueError: If there are no words available to generate a word cloud after removing URLs.\n    \"\"\"\n    # Regular expression pattern to match URLs\n    url_pattern = r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+(?:[/](?:[-\\w._~!$&\\'()*+,;=:@]|%[\\da-fA-F]{2})*)*(?:\\?(?:[-\\w._~!$&\\'()*+,;=:@/?]|%[\\da-fA-F]{2})*)?(?:#(?:[-\\w._~!$&\\'()*+,;=:@/?]|%[\\da-fA-F]{2})*)?|www\\.(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+(?:[/](?:[-\\w._~!$&\\'()*+,;=:@]|%[\\da-fA-F]{2})*)*(?:\\?(?:[-\\w._~!$&\\'()*+,;=:@/?]|%[\\da-fA-F]{2})*)?(?:#(?:[-\\w._~!$&\\'()*+,;=:@/?]|%[\\da-fA-F]{2})*)?'\n    \n    # Remove URLs from the text\n    cleaned_text = re.sub(url_pattern, '', text)\n    \n    # Remove extra whitespace\n    cleaned_text = ' '.join(cleaned_text.split())\n    \n    # Check if there are any words left after removing URLs\n    if not cleaned_text.strip():\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Create WordCloud object\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(cleaned_text)\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.title('Word Cloud (URLs removed)')\n    plt.tight_layout(pad=0)\n    plt.show()\n    \n    return wordcloud",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/341": [
      {
        "task_id": "BigCodeBench/341",
        "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n\n    pass\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n    \"\"\"\n    This function takes a pandas DataFrame and a column name as input and generates \n    two subplots in one matplotlib figure: the first subplot is a histogram \n    (with a kernel density estimate for numerical data), and the second is a box plot, \n    representing the distribution of the values in the specified column.\n    \n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        The input DataFrame\n    col : str\n        The column name to visualize\n        \n    Returns:\n    --------\n    matplotlib.figure.Figure\n        A matplotlib figure object containing the histogram and box plot\n        \n    Raises:\n    -------\n    ValueError\n        If df is not a DataFrame, is empty, or doesn't contain the specified column\n    \"\"\"\n    \n    # Validate input\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a DataFrame\")\n    \n    if df.empty:\n        raise ValueError(\"The DataFrame cannot be empty\")\n    \n    if col not in df.columns:\n        raise ValueError(f\"Column '{col}' not found in DataFrame\")\n    \n    # Create figure with two subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # First subplot: Histogram with KDE for numerical data\n    if pd.api.types.is_numeric_dtype(df[col]):\n        # For numerical data, plot histogram with KDE\n        sns.histplot(data=df, x=col, kde=True, ax=ax1)\n        ax1.set_title(f'Histogram with KDE of {col}')\n    else:\n        # For categorical data, plot count plot (histogram without KDE)\n        sns.countplot(data=df, x=col, ax=ax1)\n        ax1.set_title(f'Count Plot of {col}')\n        # Rotate x-axis labels if they're too long\n        ax1.tick_params(axis='x', rotation=45)\n    \n    ax1.set_xlabel(col)\n    ax1.set_ylabel('Frequency')\n    \n    # Second subplot: Box plot\n    if pd.api.types.is_numeric_dtype(df[col]):\n        # For numerical data, create box plot\n        sns.boxplot(data=df, y=col, ax=ax2)\n        ax2.set_title(f'Box Plot of {col}')\n        ax2.set_ylabel(col)\n    else:\n        # For categorical data, we can't create a traditional box plot\n        # Instead, show a message\n        ax2.text(0.5, 0.5, f'Box plot not applicable\\nfor categorical data', \n                ha='center', va='center', transform=ax2.transAxes)\n        ax2.set_title(f'Box Plot of {col} (N/A for categorical)')\n        ax2.set_xticks([])\n        ax2.set_yticks([])\n    \n    # Adjust layout to prevent overlap\n    plt.tight_layout()\n    \n    return fig",
        "status": "fail",
        "details": {
          "test_categorical_data": "Traceback (most recent call last):\n  File \"__test__.py\", line 108, in test_categorical_data\nAssertionError: False is not true\n"
        }
      }
    ],
    "BigCodeBench/89": [
      {
        "task_id": "BigCodeBench/89",
        "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data, column, outlier_z_score):\n\n    pass\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    Identifies and removes outliers from a specified column of a dataset based on the Z-score.\n    It standardizes the column, calculates Z-scores, and removes data points where the Z-score \n    exceeds a threshold. The function also visualizes the data before and after outlier removal.\n    \n    Parameters:\n    data (array-like): The dataset containing the data\n    column (int): The column index to check for outliers\n    outlier_z_score (float): The Z-score threshold for identifying outliers\n    \n    Returns:\n    tuple: A tuple containing the original data, the data without outliers, and the indices of the outliers.\n    \"\"\"\n    # Convert data to numpy array if it isn't already\n    data = np.array(data)\n    \n    # Extract the specified column\n    column_data = data[:, column].reshape(-1, 1)\n    \n    # Standardize the column data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(column_data)\n    \n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(column_data.flatten()))\n    \n    # Identify outliers (where Z-score exceeds threshold)\n    outlier_indices = np.where(z_scores > outlier_z_score)[0]\n    \n    # Create mask for non-outliers\n    non_outlier_mask = z_scores <= outlier_z_score\n    \n    # Filter data to remove outliers\n    data_without_outliers = data[non_outlier_mask]\n    \n    # Create visualization\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Plot 1: Data with Outliers\n    ax1.scatter(range(len(data)), data[:, column], alpha=0.6)\n    ax1.scatter(outlier_indices, data[outlier_indices, column], \n                color='red', s=100, alpha=0.8, label='Outliers')\n    ax1.set_title('Data with Outliers')\n    ax1.set_xlabel('Index')\n    ax1.set_ylabel(f'Column {column} Values')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Plot 2: Data without Outliers\n    ax2.scatter(range(len(data_without_outliers)), data_without_outliers[:, column], \n                alpha=0.6, color='green')\n    ax2.set_title('Data without Outliers')\n    ax2.set_xlabel('Index')\n    ax2.set_ylabel(f'Column {column} Values')\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return data, data_without_outliers, outlier_indices",
        "status": "fail",
        "details": {
          "test_return_type": "Traceback (most recent call last):\n  File \"__test__.py\", line 96, in test_return_type\nAssertionError: array([], dtype=int64) is not an instance of <class 'tuple'>\n"
        }
      }
    ],
    "BigCodeBench/92": [
      {
        "task_id": "BigCodeBench/92",
        "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\ndef task_func(data, n_clusters=3):\n\n    pass\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\nimport numpy as np\ndef task_func(data, n_clusters=3):\n    \"\"\"\n    Perform K-means clustering on a dataset and generate a scatter plot visualizing the clusters and their centroids.\n    \n    Parameters:\n    -----------\n    data : pd.DataFrame\n        The input data for clustering. Should have at least 2 numeric columns.\n    n_clusters : int, default=3\n        The number of clusters to form.\n    \n    Returns:\n    --------\n    tuple:\n        np.ndarray: An array of cluster labels assigned to each sample.\n        plt.Axes: An Axes object with the scatter plot showing the clusters and centroids.\n    \n    Raises:\n    -------\n    ValueError: If 'data' is not a pd.DataFrame.\n    ValueError: If 'n_clusters' is not an integer greater than 1.\n    \"\"\"\n    \n    # Validate inputs\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' must be a pd.DataFrame\")\n    \n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"'n_clusters' must be an integer greater than 1\")\n    \n    # Select only numeric columns\n    numeric_data = data.select_dtypes(include=[np.number])\n    \n    if numeric_data.shape[1] < 2:\n        raise ValueError(\"Data must have at least 2 numeric columns for visualization\")\n    \n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    cluster_labels = kmeans.fit_predict(numeric_data)\n    \n    # Get cluster centroids\n    centroids = kmeans.cluster_centers_\n    \n    # Create scatter plot\n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    # Use the first two numeric columns for visualization\n    x_col = numeric_data.columns[0]\n    y_col = numeric_data.columns[1]\n    \n    # Plot data points colored by cluster\n    scatter = ax.scatter(numeric_data[x_col], \n                        numeric_data[y_col], \n                        c=cluster_labels, \n                        cmap='viridis', \n                        alpha=0.6,\n                        edgecolors='black',\n                        linewidth=0.5)\n    \n    # Plot centroids\n    ax.scatter(centroids[:, 0], \n              centroids[:, 1], \n              c='red', \n              marker='*', \n              s=300, \n              edgecolors='black',\n              linewidth=2,\n              label='Centroids')\n    \n    # Add labels and title\n    ax.set_xlabel(x_col)\n    ax.set_ylabel(y_col)\n    ax.set_title(f'K-means Clustering (k={n_clusters})')\n    ax.legend()\n    \n    # Add colorbar\n    plt.colorbar(scatter, ax=ax, label='Cluster')\n    \n    # Add grid\n    ax.grid(True, alpha=0.3)\n    \n    return cluster_labels, ax",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/82": [
      {
        "task_id": "BigCodeBench/82",
        "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\nlogin_manager = LoginManager()\ndef task_func(secret_key, template_folder):\n\n    pass\nfrom flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\nlogin_manager = LoginManager()\ndef task_func(secret_key, template_folder):\n    \"\"\"\n    Creates a Flask application with configured user authentication using Flask-Login.\n    It defines routes for login, logout, and a protected page. The user authentication\n    is managed with a simple User class and a login form using Flask-WTF.\n    \n    Args:\n        secret_key (str): Secret key for Flask application security\n        template_folder (str): Path to the templates folder\n        \n    Returns:\n        Flask: A Flask application instance configured for user authentication.\n    \"\"\"\n    \n    # Create Flask app\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    \n    # Initialize login manager\n    login_manager.init_app(app)\n    login_manager.login_view = 'login'\n    \n    # Simple User class\n    class User(UserMixin):\n        def __init__(self, id, username, password_hash):\n            self.id = id\n            self.username = username\n            self.password_hash = password_hash\n    \n    # Mock user database\n    users = {\n        'user1': User(id=1, username='user1', password_hash=generate_password_hash('password123')),\n        'admin': User(id=2, username='admin', password_hash=generate_password_hash('adminpass123'))\n    }\n    \n    @login_manager.user_loader\n    def load_user(user_id):\n        for user in users.values():\n            if str(user.id) == str(user_id):\n                return user\n        return None\n    \n    @app.route('/')\n    def index():\n        return render_template('index.html', current_user=current_user)\n    \n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        if current_user.is_authenticated:\n            return redirect(url_for('protected'))\n        \n        form = LoginForm()\n        if form.validate_on_submit():\n            username = form.username.data\n            password = form.password.data\n            \n            user = users.get(username)\n            if user and check_password_hash(user.password_hash, password):\n                login_user(user)\n                return redirect(url_for('protected'))\n            else:\n                return render_template('login.html', form=form, error='Invalid username or password')\n        \n        return render_template('login.html', form=form)\n    \n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('index'))\n    \n    @app.route('/protected')\n    @login_required\n    def protected():\n        return render_template('protected.html', username=current_user.username)\n    \n    # Create basic templates if they don't exist\n    import os\n    os.makedirs(template_folder, exist_ok=True)\n    \n    # Create basic template files\n    with open(os.path.join(template_folder, 'base.html'), 'w') as f:\n        f.write('''<!DOCTYPE html>\n<html>\n<head>\n    <title>Flask Auth App</title>\n</head>\n<body>\n    <nav>\n        {% if current_user.is_authenticated %}\n            <a href=\"{{ url_for('protected') }}\">Protected</a> |\n            <a href=\"{{ url_for('logout') }}\">Logout</a>\n        {% else %}\n            <a href=\"{{ url_for('login') }}\">Login</a>\n        {% endif %}\n    </nav>\n    <hr>\n    {% block content %}{% endblock %}\n</body>\n</html>''')\n    \n    with open(os.path.join(template_folder, 'index.html'), 'w') as f:\n        f.write('''{% extends \"base.html\" %}\n{% block content %}\n    <h1>Welcome to Flask Auth App</h1>\n    {% if current_user.is_authenticated %}\n        <p>Hello, {{ current_user.username }}!</p>\n    {% else %}\n        <p>Please <a href=\"{{ url_for('login') }}\">login</a> to access protected content.</p>\n    {% endif %}\n{% endblock %}''')",
        "status": "fail",
        "details": {
          "test_app": "Traceback (most recent call last):\n  File \"__test__.py\", line 174, in setUp\nAttributeError: 'NoneType' object has no attribute 'config'\n",
          "test_login_manager_initialization": "Traceback (most recent call last):\n  File \"__test__.py\", line 174, in setUp\nAttributeError: 'NoneType' object has no attribute 'config'\n",
          "test_login_page_accessibility": "Traceback (most recent call last):\n  File \"__test__.py\", line 174, in setUp\nAttributeError: 'NoneType' object has no attribute 'config'\n",
          "test_logout_route_redirects_to_login": "Traceback (most recent call last):\n  File \"__test__.py\", line 174, in setUp\nAttributeError: 'NoneType' object has no attribute 'config'\n",
          "test_protected_route_access": "Traceback (most recent call last):\n  File \"__test__.py\", line 174, in setUp\nAttributeError: 'NoneType' object has no attribute 'config'\n",
          "test_secret_key": "Traceback (most recent call last):\n  File \"__test__.py\", line 174, in setUp\nAttributeError: 'NoneType' object has no attribute 'config'\n"
        }
      }
    ],
    "BigCodeBench/214": [
      {
        "task_id": "BigCodeBench/214",
        "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n\n    pass\nimport random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    \"\"\"\n    Generate a random RGB image and view it.\n    \n    Parameters:\n    - seed (int): Random seed for reproducibility\n    - image_size (tuple): Size of the image (height, width, channels)\n    - range_low (int): Lower bound for pixel values\n    - range_high (int): Upper bound for pixel values\n    \n    Returns:\n    - ax (matplotlib.axes.Axes): Axes object of the plot\n    - image (numpy.ndarray): The numpy array of the generated image\n    \n    Raises:\n    - ValueError: If range_low is not less than range_high\n    \"\"\"\n    # Check if range_low is less than range_high\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n    \n    # Set random seeds for reproducibility\n    random.seed(seed)\n    np.random.seed(seed)\n    \n    # Generate random RGB image\n    image = np.random.randint(range_low, range_high + 1, size=image_size, dtype=np.uint8)\n    \n    # Create figure and axis\n    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n    \n    # Display the image\n    ax.imshow(image)\n    ax.set_title(f'Random RGB Image ({image_size[0]}x{image_size[1]})')\n    ax.axis('off')\n    \n    # Show the plot\n    plt.show()\n    \n    return ax, image",
        "status": "fail",
        "details": {
          "test_image_size_and_type": "Traceback (most recent call last):\n  File \"__test__.py\", line 67, in test_image_size_and_type\nAssertionError: Lists differ: [[[102, 220, 225], [95, 179, 61], [234, 203,[6230 chars]88]]] != [[[57, 12, 140], [125, 114, 71], [52, 44, 21[6305 chars]45]]]\n\nFirst differing element 0:\n[[102, 220, 225], [95, 179, 61], [234, 203,[269 chars]221]]\n[[57, 12, 140], [125, 114, 71], [52, 44, 21[266 chars] 36]]\n\nDiff is 16643 characters long. Set self.maxDiff to None to see it. : DataFrame contents should match the expected output\n"
        }
      }
    ],
    "BigCodeBench/454": [
      {
        "task_id": "BigCodeBench/454",
        "solution": "import os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n\n    pass\nimport os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n    \"\"\"\n    Moves files with a specified extension from a source directory to a destination directory.\n    This function searches for files in the source directory that match the given extension.\n    If a file with the same name already exists in the destination directory, it is not moved.\n    \n    Args:\n        src_dir (str): The source directory path\n        dest_dir (str): The destination directory path\n        ext (str): The file extension to search for (e.g., '.txt', 'txt')\n    \n    Returns:\n        list: A list of the full paths of files that were successfully moved. If a file was not moved\n        because it already exists in the destination directory, it will not be included in this list.\n    \n    Raises:\n        FileNotFoundError: if either the source or destination directory does not exist\n    \"\"\"\n    # Check if source directory exists\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist\")\n    \n    # Check if destination directory exists\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(f\"Destination directory '{dest_dir}' does not exist\")\n    \n    # Ensure extension starts with a dot\n    if not ext.startswith('.'):\n        ext = '.' + ext\n    \n    # Find all files with the specified extension in the source directory\n    pattern = os.path.join(src_dir, f'*{ext}')\n    files_to_move = glob.glob(pattern)\n    \n    moved_files = []\n    \n    for src_file_path in files_to_move:\n        # Get the filename\n        filename = os.path.basename(src_file_path)\n        \n        # Construct the destination file path\n        dest_file_path = os.path.join(dest_dir, filename)\n        \n        # Check if file already exists in destination\n        if not os.path.exists(dest_file_path):\n            # Move the file\n            shutil.move(src_file_path, dest_file_path)\n            moved_files.append(dest_file_path)\n    \n    return moved_files",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/458": [
      {
        "task_id": "BigCodeBench/458",
        "solution": "import json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n\n    pass\nimport json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n    \"\"\"\n    Load a JSON string into a dictionary, normalize the dictionary by doubling \n    the numerical values, and then create a Pandas DataFrame from the dictionary.\n    \n    Args:\n        json_str (str): A JSON string to process\n        \n    Returns:\n        DataFrame: A pandas DataFrame created from the dictionary.\n    \"\"\"\n    # Handle empty string\n    if not json_str or json_str.strip() == '':\n        return pd.DataFrame()\n    \n    try:\n        # Load JSON string into dictionary\n        data = json.loads(json_str)\n        \n        # If not a dictionary, return empty DataFrame\n        if not isinstance(data, dict):\n            return pd.DataFrame()\n        \n        # If empty dictionary, return empty DataFrame\n        if not data:\n            return pd.DataFrame()\n        \n        # Normalize the dictionary by doubling numerical values\n        normalized_data = {}\n        \n        for key, value in data.items():\n            if isinstance(value, (int, float)):\n                # Direct numerical value - double it\n                normalized_data[key] = float(value * 2)\n            elif isinstance(value, list):\n                # List of values - double numerical ones\n                normalized_list = []\n                for item in value:\n                    if isinstance(item, (int, float)):\n                        normalized_list.append(float(item * 2))\n                    else:\n                        # Keep non-numerical items as-is\n                        normalized_list.append(item)\n                normalized_data[key] = normalized_list\n            elif isinstance(value, str):\n                # String that might contain a number\n                # Use regex to find numbers (including decimals)\n                number_match = re.match(r'^-?\\d+\\.?\\d*$', value.strip())\n                if number_match:\n                    # String represents a number - convert and double it\n                    num_value = float(value)\n                    normalized_data[key] = float(num_value * 2)\n                else:\n                    # Not a number string - keep as-is\n                    normalized_data[key] = value\n            else:\n                # Other types - keep as-is\n                normalized_data[key] = value\n        \n        # Create DataFrame from normalized dictionary\n        # Handle the case where values might be lists of different lengths\n        df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in normalized_data.items()]))\n        \n        return df\n        \n    except (json.JSONDecodeError, ValueError):\n        # Invalid JSON or other errors - return empty DataFrame\n        return pd.DataFrame()",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 86, in test_case_1\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 1224, in assert_frame_equal\n    assert_series_equal(\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 1014, in assert_series_equal\n    _testing.assert_almost_equal(\n  File \"pandas/_libs/testing.pyx\", line 52, in pandas._libs.testing.assert_almost_equal\n  File \"pandas/_libs/testing.pyx\", line 172, in pandas._libs.testing.assert_almost_equal\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 599, in raise_assert_detail\n    raise AssertionError(msg)\nAssertionError: DataFrame.iloc[:, 1] (column name=\"b\") are different\n\nDataFrame.iloc[:, 1] (column name=\"b\") values are different (66.66667 %)\n[index]: [0, 1, 2]\n[left]:  [9.8, nan, nan]\n[right]: [9.8, 9.8, 9.8]\nAt positional index 1, first diff: nan != 9.8\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 101, in test_case_3\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 1224, in assert_frame_equal\n    assert_series_equal(\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 1014, in assert_series_equal\n    _testing.assert_almost_equal(\n  File \"pandas/_libs/testing.pyx\", line 52, in pandas._libs.testing.assert_almost_equal\n  File \"pandas/_libs/testing.pyx\", line 172, in pandas._libs.testing.assert_almost_equal\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 599, in raise_assert_detail\n    raise AssertionError(msg)\nAssertionError: DataFrame.iloc[:, 1] (column name=\"b\") are different\n\nDataFrame.iloc[:, 1] (column name=\"b\") values are different (66.66667 %)\n[index]: [0, 1, 2]\n[left]:  [9.8, nan, nan]\n[right]: [9.8, 9.8, 9.8]\nAt positional index 1, first diff: nan != 9.8\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 109, in test_case_5\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 1224, in assert_frame_equal\n    assert_series_equal(\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 1014, in assert_series_equal\n    _testing.assert_almost_equal(\n  File \"pandas/_libs/testing.pyx\", line 52, in pandas._libs.testing.assert_almost_equal\n  File \"pandas/_libs/testing.pyx\", line 172, in pandas._libs.testing.assert_almost_equal\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 599, in raise_assert_detail\n    raise AssertionError(msg)\nAssertionError: DataFrame.iloc[:, 1] (column name=\"c\") are different\n\nDataFrame.iloc[:, 1] (column name=\"c\") values are different (66.66667 %)\n[index]: [0, 1, 2]\n[left]:  [9.8, nan, nan]\n[right]: [9.8, 9.8, 9.8]\nAt positional index 1, first diff: nan != 9.8\n"
        }
      }
    ],
    "BigCodeBench/37": [
      {
        "task_id": "BigCodeBench/37",
        "solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, target_column):\n\n    pass\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\ndef task_func(df, target_column):\n    \"\"\"\n    Train a random forest classifier to perform classification and plot feature importance.\n    \n    Parameters:\n    df (pd.DataFrame): Input dataframe containing features and target column\n    target_column (str): Name of the target column for classification\n    \n    Returns:\n    sklearn.ensemble.RandomForestClassifier: The trained random forest classifier\n    matplotlib.axes.Axes: The Axes object of the plotted data\n    \"\"\"\n    \n    # Separate features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    # Initialize and train the Random Forest Classifier\n    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf_classifier.fit(X, y)\n    \n    # Get feature importances\n    feature_importances = rf_classifier.feature_importances_\n    feature_names = X.columns.tolist()\n    \n    # Create a dataframe for feature importances\n    importance_df = pd.DataFrame({\n        'feature': feature_names,\n        'importance': feature_importances\n    })\n    \n    # Sort by importance in descending order\n    importance_df = importance_df.sort_values('importance', ascending=False)\n    \n    # Create the plot\n    plt.figure(figsize=(10, 6))\n    ax = plt.gca()\n    \n    # Create horizontal bar plot\n    plt.barh(importance_df['feature'], importance_df['importance'])\n    \n    # Set labels and title\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    \n    # Invert y-axis to have highest importance at the top\n    plt.gca().invert_yaxis()\n    \n    # Adjust layout to prevent label cutoff\n    plt.tight_layout()\n    \n    return rf_classifier, ax",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/445": [
      {
        "task_id": "BigCodeBench/445",
        "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n\n    pass\nimport numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n    \"\"\"\n    Calculate the Voronoi diagram for a number of points in 2D and plot it.\n    \n    Parameters:\n    -----------\n    points : array-like\n        2D array of points with shape (n_points, 2)\n    seed : int, optional\n        Random seed for jittering (default=0)\n    \n    Returns:\n    --------\n    tuple (vor, ax): A tuple containing:\n        vor (Voronoi): A Voronoi object representing the Voronoi diagram of the points.\n        ax (Axes): The axes of the plotted Voronoi diagram.\n    \n    Raises:\n    -------\n    ValueError: If input is invalid (wrong type or shape)\n    \"\"\"\n    \n    # Convert points to numpy array\n    try:\n        points = np.asarray(points, dtype=float)\n    except (ValueError, TypeError) as e:\n        raise ValueError(f\"Invalid input type: {e}\")\n    \n    # Validate shape\n    if points.ndim != 2:\n        raise ValueError(f\"Points must be a 2D array, got {points.ndim}D array\")\n    \n    if points.shape[1] != 2:\n        raise ValueError(f\"Points must have 2 coordinates per point, got {points.shape[1]}\")\n    \n    if points.shape[0] < 3:\n        raise ValueError(f\"Need at least 3 points for Voronoi diagram, got {points.shape[0]}\")\n    \n    # Apply jittering to avoid degenerate cases\n    np.random.seed(seed)\n    jitter = np.random.normal(0, 1e-10, points.shape)\n    jittered_points = points + jitter\n    \n    # Calculate Voronoi diagram\n    vor = Voronoi(jittered_points)\n    \n    # Create plot\n    fig, ax = plt.subplots(figsize=(8, 6))\n    \n    # Plot the Voronoi diagram\n    voronoi_plot_2d(vor, ax=ax, show_vertices=True, line_colors='blue', \n                    line_width=2, line_alpha=0.6, point_size=8)\n    \n    # Set labels and title\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title('Voronoi Diagram')\n    ax.grid(True, alpha=0.3)\n    \n    # Adjust plot limits to show all regions nicely\n    margin = 0.1\n    x_min, y_min = points.min(axis=0)\n    x_max, y_max = points.max(axis=0)\n    x_range = x_max - x_min\n    y_range = y_max - y_min\n    ax.set_xlim(x_min - margin * x_range, x_max + margin * x_range)\n    ax.set_ylim(y_min - margin * y_range, y_max + margin * y_range)\n    \n    return vor, ax",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/501": [
      {
        "task_id": "BigCodeBench/501",
        "solution": "import xlwt\nimport os\nimport pandas as pd\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n\n    pass\nimport xlwt\nimport os\nimport pandas as pd\nimport json\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    \"\"\"\n    Convert JSON strings to an Excel file, including handling empty JSON arrays.\n    \n    This function takes a JSON string and converts it into an Excel file with the\n    specified filename. If the JSON string represents an empty array, the function\n    creates an Excel file with no data rows.\n    \n    Args:\n        json_str: JSON string to convert\n        filename: Name of the output Excel file\n        sheet_name: Name of the Excel sheet (default: \"sheet1\")\n    \n    Returns:\n        str: The absolute path of the created Excel file\n        \n    Raises:\n        ValueError: If `json_str` is not valid JSON\n        TypeError: If `json_str` is not a string, bytes, or bytearray\n        Exception: For other general errors related to file writing\n    \"\"\"\n    \n    # Check if json_str is of valid type\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n    \n    try:\n        # Parse JSON string\n        data = json.loads(json_str)\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Invalid JSON: {e}\")\n    \n    try:\n        # Convert to DataFrame\n        if isinstance(data, list):\n            if len(data) == 0:\n                # Empty array - create empty DataFrame\n                df = pd.DataFrame()\n            else:\n                # Non-empty array\n                df = pd.DataFrame(data)\n        elif isinstance(data, dict):\n            # Single object - wrap in list\n            df = pd.DataFrame([data])\n        else:\n            # Other types (like single values) - create DataFrame with single value\n            df = pd.DataFrame([{\"value\": data}])\n        \n        # Ensure filename has .xls extension\n        if not filename.endswith('.xls'):\n            filename += '.xls'\n        \n        # Create workbook and sheet\n        workbook = xlwt.Workbook()\n        sheet = workbook.add_sheet(sheet_name)\n        \n        # Write headers if DataFrame has columns\n        if len(df.columns) > 0:\n            for col_idx, col_name in enumerate(df.columns):\n                sheet.write(0, col_idx, str(col_name))\n            \n            # Write data rows\n            for row_idx, row in df.iterrows():\n                for col_idx, value in enumerate(row):\n                    # Handle None/NaN values\n                    if pd.isna(value):\n                        sheet.write(row_idx + 1, col_idx, '')\n                    else:\n                        sheet.write(row_idx + 1, col_idx, str(value))\n        \n        # Save the workbook\n        workbook.save(filename)\n        \n        # Return absolute path\n        return os.path.abspath(filename)\n        \n    except Exception as e:\n        # Re-raise any other exceptions\n        raise Exception(f\"Error writing Excel file: {e}\")",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/492": [
      {
        "task_id": "BigCodeBench/492",
        "solution": "import pandas as pd\nfrom datetime import datetime\nimport random\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n\n    pass\nimport pandas as pd\nfrom datetime import datetime\nimport random\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    \"\"\"\n    Generate sales data for five products from a given epoch time up to the current time.\n    \n    Args:\n        epoch_milliseconds: Epoch time in milliseconds\n        random_seed: Random seed for reproducibility\n        products: List of product names\n    \n    Returns:\n        pd.DataFrame: A DataFrame containing sales data with columns 'Product' (string), \n        'Date' (datetime), and 'Sales' (integer). Sales quantity is randomly sampled \n        from range [10, 50].\n    \"\"\"\n    # Set random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Validate input\n    if not isinstance(epoch_milliseconds, (int, float)):\n        raise ValueError(\"epoch_milliseconds must be a number\")\n    \n    if epoch_milliseconds < 0:\n        raise ValueError(\"epoch_milliseconds must be non-negative\")\n    \n    # Convert epoch milliseconds to datetime\n    try:\n        start_date = datetime.fromtimestamp(epoch_milliseconds / 1000)\n    except (ValueError, OSError) as e:\n        raise ValueError(f\"Invalid epoch_milliseconds: {e}\")\n    \n    # Get current datetime\n    end_date = datetime.now()\n    \n    # Check if start date is in the future\n    if start_date > end_date:\n        raise ValueError(\"Start date cannot be in the future\")\n    \n    # Initialize lists to store data\n    dates = []\n    product_names = []\n    sales = []\n    \n    # Generate data for each day from start_date to end_date\n    current_date = start_date.replace(hour=0, minute=0, second=0, microsecond=0)\n    end_date = end_date.replace(hour=0, minute=0, second=0, microsecond=0)\n    \n    while current_date <= end_date:\n        # Generate sales for each product for this date\n        for product in products:\n            dates.append(current_date)\n            product_names.append(product)\n            sales.append(random.randint(10, 50))\n        \n        # Move to next day\n        current_date = current_date + pd.Timedelta(days=1)\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'Product': product_names,\n        'Date': dates,\n        'Sales': sales\n    })\n    \n    return df",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 89, in test_case_1\nAssertionError: Timestamp('2021-09-10 00:00:00') != datetime.datetime(2021, 9, 10, 16, 0)\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 101, in test_case_2\nAssertionError: Timestamp('2025-08-31 00:00:00') != datetime.datetime(2025, 8, 31, 19, 13, 51, 238030)\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 113, in test_case_3\nAssertionError: Timestamp('2025-08-04 00:00:00') != datetime.datetime(2025, 8, 4, 19, 13, 51, 238899)\n",
          "test_case_6": "Traceback (most recent call last):\n  File \"__test__.py\", line 137, in test_case_6\nAssertionError: ValueError not raised\n",
          "test_case_7": "Traceback (most recent call last):\n  File \"__test__.py\", line 141, in test_case_7\nAssertionError: ValueError not raised\n"
        }
      }
    ],
    "BigCodeBench/509": [
      {
        "task_id": "BigCodeBench/509",
        "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n\n    pass\nimport pandas as pd\nimport csv\nfrom difflib import ndiff\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    \"\"\"\n    Compare two CSV files and create a difference report.\n    \n    This function compares two CSV files line by line and provides a detailed report\n    of the differences. It represents each difference with a line number, a status\n    indicator, and the content of that line.\n    \n    Parameters:\n    file_path1 (str): Path to the first CSV file\n    file_path2 (str): Path to the second CSV file\n    delimiter (str): Delimiter used in the CSV files (default: ',')\n    quotechar (str): Quote character used in the CSV files (default: '\"')\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the differences containing:\n        - 'Line Number': The line number where the difference occurs\n        - 'Status': Type of difference (' ', '-', '+')\n        - 'Content': The actual text content of the line\n    \n    Raises:\n    FileNotFoundError: If either of the files cannot be found\n    ValueError: If either of the files is empty\n    Exception: For other IO related errors\n    \"\"\"\n    \n    try:\n        # Read the first file\n        try:\n            with open(file_path1, 'r', encoding='utf-8') as f1:\n                lines1 = f1.readlines()\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"File not found: {file_path1}\")\n        except Exception as e:\n            raise Exception(f\"Error reading file {file_path1}: {str(e)}\")\n        \n        # Read the second file\n        try:\n            with open(file_path2, 'r', encoding='utf-8') as f2:\n                lines2 = f2.readlines()\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"File not found: {file_path2}\")\n        except Exception as e:\n            raise Exception(f\"Error reading file {file_path2}: {str(e)}\")\n        \n        # Check if files are empty\n        if len(lines1) == 0:\n            raise ValueError(f\"File is empty: {file_path1}\")\n        if len(lines2) == 0:\n            raise ValueError(f\"File is empty: {file_path2}\")\n        \n        # Strip newline characters but preserve the content\n        lines1 = [line.rstrip('\\n\\r') for line in lines1]\n        lines2 = [line.rstrip('\\n\\r') for line in lines2]\n        \n        # Use ndiff to compare the lines\n        diff = list(ndiff(lines1, lines2))\n        \n        # Process the differences\n        differences = []\n        line_num1 = 0\n        line_num2 = 0\n        \n        for line in diff:\n            if line.startswith('  '):  # No change\n                line_num1 += 1\n                line_num2 += 1\n                differences.append({\n                    'Line Number': line_num1,\n                    'Status': ' ',\n                    'Content': line[2:]\n                })\n            elif line.startswith('- '):  # Line in file1 but not in file2\n                line_num1 += 1\n                differences.append({\n                    'Line Number': line_num1,\n                    'Status': '-',\n                    'Content': line[2:]\n                })\n            elif line.startswith('+ '):  # Line in file2 but not in file1\n                line_num2 += 1\n                differences.append({\n                    'Line Number': line_num2,\n                    'Status': '+',\n                    'Content': line[2:]\n                })\n            # Skip lines starting with '?' as they are hints from ndiff\n        \n        # Create DataFrame\n        df = pd.DataFrame(differences)\n        \n        # If no differences were found, create an empty DataFrame with the correct columns\n        if df.empty:\n            df = pd.DataFrame(columns=['Line Number', 'Status', 'Content'])\n        \n        return df\n        \n    except FileNotFoundError:\n        raise\n    except ValueError:\n        raise\n    except Exception as e:\n        raise Exception(f\"IO Error: {str(e)}\")",
        "status": "fail",
        "details": {
          "test_difference_report": "Traceback (most recent call last):\n  File \"__test__.py\", line 166, in test_difference_report\nAssertionError: Lists differ: ['1, ,name,age', '2,-,Alice,30', '2,+,Alic[15 chars],25'] != [\"1, ,('name', 'age')\", \"2,-,('Alice', '30[43 chars]5')\"]\n\nFirst differing element 0:\n'1, ,name,age'\n\"1, ,('name', 'age')\"\n\n- ['1, ,name,age', '2,-,Alice,30', '2,+,Alice,31', '3, ,Bob,25']\n+ [\"1, ,('name', 'age')\",\n+  \"2,-,('Alice', '30')\",\n+  \"3,+,('Alice', '31')\",\n+  \"4, ,('Bob', '25')\"]\n"
        }
      }
    ],
    "BigCodeBench/513": [
      {
        "task_id": "BigCodeBench/513",
        "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n\n    pass\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    \"\"\"\n    Analyze a list of fitness data, calculate statistics for a specified column,\n    and draw a line chart.\n    \n    Parameters:\n    column (str): The column name to analyze\n    data (list): List of dictionaries containing fitness data\n    \n    Returns:\n    tuple: A tuple containing:\n        dict: A dictionary with the sum, mean, min, max of the column\n        matplotlib.axes.Axes: The Axes object of the plotted line chart\n    \n    Raises:\n    KeyError: If the specified column is not valid\n    ValueError: If the data list is empty or if any numeric values are negative\n    \"\"\"\n    \n    # Check if data is empty\n    if not data:\n        raise ValueError(\"Data list is empty\")\n    \n    # Convert data to DataFrame\n    df = pd.DataFrame(data)\n    \n    # Check if column exists\n    if column not in df.columns:\n        raise KeyError(f\"Column '{column}' is not valid\")\n    \n    # Define numeric columns to validate\n    numeric_columns = ['steps', 'calories_burned', 'distance_walked']\n    \n    # Validate non-negative values for numeric columns\n    for col in numeric_columns:\n        if col in df.columns:\n            if (df[col] < 0).any():\n                raise ValueError(f\"Negative values found in column '{col}'\")\n    \n    # Calculate statistics for the specified column\n    column_data = df[column]\n    stats = {\n        'sum': column_data.sum(),\n        'mean': column_data.mean(),\n        'min': column_data.min(),\n        'max': column_data.max()\n    }\n    \n    # Create line chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot the data\n    if 'date' in df.columns:\n        x_data = df['date']\n    else:\n        x_data = range(len(df))\n    \n    ax.plot(x_data, column_data, marker='o', linestyle='-', linewidth=2)\n    \n    # Set labels and title\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column)\n    ax.set_title(f'Line Chart of {column}')\n    \n    # Rotate x-axis labels if they are dates\n    if 'date' in df.columns:\n        plt.xticks(rotation=45, ha='right')\n    \n    # Add grid for better readability\n    ax.grid(True, alpha=0.3)\n    \n    # Adjust layout to prevent label cutoff\n    plt.tight_layout()\n    \n    return stats, ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 95, in test_case_1\n  File \"__test__.py\", line 38, in task_func\nKeyError: \"Column 'Steps' is not valid\"\n",
          "test_case_11": "Traceback (most recent call last):\n  File \"__test__.py\", line 177, in test_case_11\n  File \"__test__.py\", line 38, in task_func\nKeyError: \"Column 'Steps' is not valid\"\n",
          "test_case_12": "Traceback (most recent call last):\n  File \"__test__.py\", line 187, in test_case_12\n  File \"__test__.py\", line 38, in task_func\nKeyError: \"Column 'Distance Walked' is not valid\"\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 106, in test_case_2\n  File \"__test__.py\", line 38, in task_func\nKeyError: \"Column 'Calories Burned' is not valid\"\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 114, in test_case_3\n  File \"__test__.py\", line 38, in task_func\nKeyError: \"Column 'Distance Walked' is not valid\"\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 124, in test_case_4\n  File \"__test__.py\", line 38, in task_func\nKeyError: \"Column 'Steps' is not valid\"\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 134, in test_case_5\n  File \"__test__.py\", line 38, in task_func\nKeyError: \"Column 'Calories Burned' is not valid\"\n",
          "test_case_7": "Traceback (most recent call last):\n  File \"__test__.py\", line 148, in test_case_7\n  File \"__test__.py\", line 38, in task_func\nKeyError: \"Column 'Steps' is not valid\"\n",
          "test_case_8": "Traceback (most recent call last):\n  File \"__test__.py\", line 152, in test_case_8\n  File \"__test__.py\", line 38, in task_func\nKeyError: \"Column 'Steps' is not valid\"\n",
          "test_case_9": "Traceback (most recent call last):\n  File \"__test__.py\", line 161, in test_case_9\n  File \"__test__.py\", line 38, in task_func\nKeyError: \"Column 'Steps' is not valid\"\n"
        }
      }
    ],
    "BigCodeBench/503": [
      {
        "task_id": "BigCodeBench/503",
        "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n\n    pass\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    \"\"\"\n    Create a DataFrame of stock prices for a specified number of days in the past using random data.\n    \n    Parameters:\n    -----------\n    days_in_past : int, optional\n        Number of days to generate data for (default is 7)\n    stock_names : list, optional\n        List of stock ticker symbols (default is [\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"])\n    random_seed : int, optional\n        Random seed for reproducibility (default is 0)\n    \n    Returns:\n    --------\n    DataFrame: A pandas DataFrame containing random stock prices for the specified number of days.\n    Prices are floats in [0.0,1.0).\n    \"\"\"\n    # Set random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Generate date range\n    end_date = datetime.now().date()\n    start_date = end_date - timedelta(days=days_in_past - 1)\n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    \n    # Generate random stock prices\n    # Shape: (days_in_past, number of stocks)\n    prices = np.random.random((days_in_past, len(stock_names)))\n    \n    # Create DataFrame\n    df = pd.DataFrame(\n        data=prices,\n        index=date_range,\n        columns=stock_names\n    )\n    \n    return df",
        "status": "fail",
        "details": {
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 114, in test_case_4\nAssertionError: ValueError not raised\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 120, in test_case_5\nAssertionError: ValueError not raised\n"
        }
      }
    ],
    "BigCodeBench/526": [
      {
        "task_id": "BigCodeBench/526",
        "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file=\"data.json\"):\n\n    pass\nimport json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file=\"data.json\"):\n    \"\"\"\n    Read a list of dictionaries from a JSON file, calculate the mean and median \n    for each key (ignoring non-numeric or missing values), and convert the results \n    into a Pandas DataFrame.\n    \n    Args:\n        input_file (str): Path to the JSON file containing list of dictionaries\n        \n    Returns:\n        df (pd.DataFrame): A DataFrame indexed and sorted by the variable names (keys) \n        from the input data, containing columns 'mean' and 'median'.\n    \"\"\"\n    # Read JSON file\n    try:\n        with open(input_file, 'r') as f:\n            data = json.load(f)\n    except FileNotFoundError:\n        print(f\"File {input_file} not found\")\n        return pd.DataFrame(columns=['mean', 'median'])\n    except json.JSONDecodeError:\n        print(f\"Invalid JSON in {input_file}\")\n        return pd.DataFrame(columns=['mean', 'median'])\n    \n    # Check if data is a list\n    if not isinstance(data, list):\n        print(\"JSON file must contain a list of dictionaries\")\n        return pd.DataFrame(columns=['mean', 'median'])\n    \n    # Collect all numeric values for each key\n    values_by_key = defaultdict(list)\n    \n    for item in data:\n        if isinstance(item, dict):\n            for key, value in item.items():\n                # Check if value is numeric (int or float)\n                if isinstance(value, (int, float)) and not isinstance(value, bool):\n                    # Also check for NaN values\n                    if not (isinstance(value, float) and np.isnan(value)):\n                        values_by_key[key].append(value)\n    \n    # Calculate mean and median for each key\n    results = {}\n    for key, values in values_by_key.items():\n        if values:  # Only calculate if there are numeric values\n            results[key] = {\n                'mean': np.mean(values),\n                'median': np.median(values)\n            }\n    \n    # Create DataFrame from results\n    if results:\n        df = pd.DataFrame.from_dict(results, orient='index')\n        # Sort by index (key names)\n        df = df.sort_index()\n    else:\n        # Return empty DataFrame with correct columns if no numeric data found\n        df = pd.DataFrame(columns=['mean', 'median'])\n    \n    return df",
        "status": "fail",
        "details": {
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 107, in test_case_3\nAssertionError: Lists differ: ['a'] != ['a', 'b']\n\nSecond list contains 1 additional elements.\nFirst extra element 1:\n'b'\n\n- ['a']\n+ ['a', 'b']\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 119, in test_case_5\nAssertionError: Lists differ: ['a', 'c'] != ['a', 'b', 'c']\n\nFirst differing element 1:\n'c'\n'b'\n\nSecond list contains 1 additional elements.\nFirst extra element 2:\n'c'\n\n- ['a', 'c']\n+ ['a', 'b', 'c']\n?       +++++\n\n",
          "test_case_8": "Traceback (most recent call last):\n  File \"__test__.py\", line 158, in test_case_8\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexing.py\", line 1096, in __getitem__\n    return self.obj._get_value(*key, takeable=self._takeable)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3877, in _get_value\n    row = self.index.get_loc(index)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/range.py\", line 349, in get_loc\n    raise KeyError(key)\nKeyError: 'a'\n"
        }
      }
    ],
    "BigCodeBench/567": [
      {
        "task_id": "BigCodeBench/567",
        "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n\n    pass\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    \"\"\"\n    This function draws a histogram to visualize the frequency distribution of numeric values \n    provided in a string format, with 'Value' on the x-axis, 'Frequency' on the y-axis and \n    'Histogram of Values' as the title.\n    \n    Parameters:\n    data (str or list): String of numeric values separated by spaces/commas or a list of strings\n    \n    Returns:\n    ax (matplotlib.axes._axes.Axes): The Axes object of the created histogram.\n    \"\"\"\n    # Convert string data to numeric array\n    if isinstance(data, str):\n        # Handle both space and comma separated values\n        data = data.replace(',', ' ')\n        # Split and convert to float\n        numeric_data = np.array([float(x) for x in data.split() if x])\n    elif isinstance(data, list):\n        # If data is already a list, convert elements to float\n        numeric_data = np.array([float(x) for x in data])\n    else:\n        # If data is already numeric array\n        numeric_data = np.array(data)\n    \n    # Create figure and axis\n    fig, ax = plt.subplots()\n    \n    # Calculate bins as specified\n    bins = np.arange(numeric_data.min(), numeric_data.max() + 2) - 0.5\n    \n    # Create histogram\n    ax.hist(numeric_data, bins=bins, edgecolor='black')\n    \n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n    \n    return ax",
        "status": "fail",
        "details": {
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 72, in test_case_3\nAssertionError: Lists differ: [6.4, 6.6000000000000005, 6.800000000000001, 7.0, 7.2, 7.4, 7.6000000000000005] != [7]\n\nFirst differing element 0:\n6.4\n7\n\nFirst list contains 6 additional elements.\nFirst extra element 1:\n6.6000000000000005\n\n- [6.4, 6.6000000000000005, 6.800000000000001, 7.0, 7.2, 7.4, 7.6000000000000005]\n+ [7]\n",
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 54, in test_case_1\n  File \"__test__.py\", line 27, in task_func\n  File \"__test__.py\", line 27, in <listcomp>\nValueError: could not convert string to float: '1-2-3-4-5'\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 61, in test_case_2\n  File \"__test__.py\", line 27, in task_func\n  File \"__test__.py\", line 27, in <listcomp>\nValueError: could not convert string to float: '5-5-5-5-5'\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 75, in test_case_4\n  File \"__test__.py\", line 27, in task_func\n  File \"__test__.py\", line 27, in <listcomp>\nValueError: could not convert string to float: '2-8-4-10-1'\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 82, in test_case_5\n  File \"__test__.py\", line 27, in task_func\n  File \"__test__.py\", line 27, in <listcomp>\nValueError: could not convert string to float: '1-50-100-150'\n"
        }
      }
    ],
    "BigCodeBench/592": [
      {
        "task_id": "BigCodeBench/592",
        "solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n\n    pass\nimport csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint, uniform\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Create sensor data for the specified number of hours and save it in a CSV file\n    with columns 'Time', 'Temperature', 'Humidity' and 'Pressure'.\n    \n    Args:\n        hours (int): Number of hours to generate data for.\n        output_dir (str): Directory to save the output CSV file.\n    \n    Returns:\n        str: Path to the generated CSV file.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Generate filename with timestamp\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f\"sensor_data_{timestamp}.csv\"\n    filepath = os.path.join(output_dir, filename)\n    \n    # Open CSV file for writing\n    with open(filepath, 'w', newline='') as csvfile:\n        # Create CSV writer\n        fieldnames = ['Time'] + SENSORS\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        # Write header\n        writer.writeheader()\n        \n        # Generate data for each hour\n        start_time = datetime.now()\n        \n        # Generate data points (one per minute for each hour)\n        for hour in range(hours):\n            for minute in range(60):\n                # Calculate current time\n                current_time = start_time + timedelta(hours=hour, minutes=minute)\n                \n                # Generate sensor data\n                # Temperature: 15-30\u00b0C with some variation\n                temperature = round(uniform(15.0, 30.0), 2)\n                \n                # Humidity: 30-80% with some variation\n                humidity = round(uniform(30.0, 80.0), 2)\n                \n                # Pressure: 980-1020 hPa with some variation\n                pressure = round(uniform(980.0, 1020.0), 2)\n                \n                # Write row to CSV\n                row = {\n                    'Time': current_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n                    'Temperature': temperature,\n                    'Humidity': humidity,\n                    'Pressure': pressure\n                }\n                writer.writerow(row)\n    \n    print(f\"Sensor data generated successfully!\")\n    print(f\"File saved to: {filepath}\")\n    print(f\"Generated {hours * 60} data points for {hours} hour(s)\")\n    \n    return filepath",
        "status": "fail",
        "details": {
          "test_csv_file_creation": "Traceback (most recent call last):\n  File \"__test__.py\", line 94, in test_csv_file_creation\nAssertionError: False is not true\n",
          "test_file_path_return": "Traceback (most recent call last):\n  File \"__test__.py\", line 110, in test_file_path_return\nAssertionError: './output/sensor_data_20250903_191351.csv' != './output/sensor_data.csv'\n- ./output/sensor_data_20250903_191351.csv\n?                     ----------------\n+ ./output/sensor_data.csv\n\n",
          "test_csv_file_header": "Traceback (most recent call last):\n  File \"__test__.py\", line 103, in test_csv_file_header\nFileNotFoundError: [Errno 2] No such file or directory: './output/sensor_data.csv'\n",
          "test_csv_file_rows": "Traceback (most recent call last):\n  File \"__test__.py\", line 98, in test_csv_file_rows\nFileNotFoundError: [Errno 2] No such file or directory: './output/sensor_data.csv'\n",
          "test_no_hours_data": "Traceback (most recent call last):\n  File \"__test__.py\", line 114, in test_no_hours_data\nFileNotFoundError: [Errno 2] No such file or directory: './output/sensor_data.csv'\n"
        }
      }
    ],
    "BigCodeBench/560": [
      {
        "task_id": "BigCodeBench/560",
        "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data):\n\n    pass\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data):\n    \"\"\"\n    This function plots a bar chart of monthly data values for a single year,\n    with 'month' on the x-axis and 'value' on the y-axis.\n    \n    Parameters:\n    data (pd.DataFrame): A DataFrame with 'month' and 'value' columns\n    \n    Returns:\n    Axes object: A matplotlib.axes.Axes object representing the plot,\n    with the title formatted as 'Monthly Data for yyyy', x label as 'Month',\n    and y label as 'Value'.\n    \"\"\"\n    # Create a figure and axis\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Extract months and values\n    months = data['month']\n    values = data['value']\n    \n    # Create bar chart\n    ax.bar(months, values)\n    \n    # Determine the year from the data\n    # Assuming the data contains datetime objects or can be parsed as dates\n    if isinstance(months.iloc[0], (datetime, pd.Timestamp)):\n        year = months.iloc[0].year\n    elif isinstance(months.iloc[0], str):\n        # Try to parse the month string to get the year\n        try:\n            year = pd.to_datetime(months.iloc[0]).year\n        except:\n            # If parsing fails, use current year as default\n            year = datetime.now().year\n    else:\n        # Default to current year if year cannot be determined\n        year = datetime.now().year\n    \n    # Set title and labels\n    ax.set_title(f'Monthly Data for {year}')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Value')\n    \n    # Rotate x-axis labels for better readability if needed\n    plt.xticks(rotation=45, ha='right')\n    \n    # Adjust layout to prevent label cutoff\n    plt.tight_layout()\n    \n    return ax",
        "status": "fail",
        "details": {
          "test_basic_functionality": "Traceback (most recent call last):\n  File \"__test__.py\", line 64, in test_basic_functionality\n  File \"__test__.py\", line 27, in task_func\nTypeError: string indices must be integers\n",
          "test_empty_data": "Traceback (most recent call last):\n  File \"__test__.py\", line 84, in test_empty_data\n  File \"__test__.py\", line 27, in task_func\nTypeError: string indices must be integers\n",
          "test_full_year_data": "Traceback (most recent call last):\n  File \"__test__.py\", line 71, in test_full_year_data\n  File \"__test__.py\", line 27, in task_func\nTypeError: string indices must be integers\n",
          "test_incorrect_data_format": "Traceback (most recent call last):\n  File \"__test__.py\", line 80, in test_incorrect_data_format\n  File \"__test__.py\", line 27, in task_func\nTypeError: string indices must be integers\n",
          "test_partial_year_data": "Traceback (most recent call last):\n  File \"__test__.py\", line 75, in test_partial_year_data\n  File \"__test__.py\", line 27, in task_func\nTypeError: string indices must be integers\n"
        }
      }
    ],
    "BigCodeBench/511": [
      {
        "task_id": "BigCodeBench/511",
        "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n\n    pass\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    \"\"\"\n    Analyze a list of employee data and calculate statistics for a given column.\n    If the data list is empty, the sum will be 0 and mean, min, and max values will be NaN.\n    The function also visualizes the data with a pie chart, using the Age column as labels.\n    \n    Parameters:\n    column (str): The column name to analyze\n    data (list): List of dictionaries containing employee data\n    \n    Returns:\n    tuple: A tuple containing:\n        dict: A dictionary with the 'sum', 'mean', 'min', and 'max' of the column.\n        Axes object: The pie chart visualizing the column data.\n    \"\"\"\n    \n    # Initialize statistics dictionary\n    stats = {\n        'sum': 0,\n        'mean': np.nan,\n        'min': np.nan,\n        'max': np.nan\n    }\n    \n    # Create figure and axis for pie chart\n    fig, ax = plt.subplots(figsize=(8, 6))\n    \n    # Handle empty data\n    if not data:\n        # Create empty pie chart\n        ax.text(0.5, 0.5, 'No data available', \n                horizontalalignment='center',\n                verticalalignment='center',\n                transform=ax.transAxes)\n        ax.set_xlim(-1, 1)\n        ax.set_ylim(-1, 1)\n        return stats, ax\n    \n    # Convert data to DataFrame\n    df = pd.DataFrame(data)\n    \n    # Check if column exists in the data\n    if column not in df.columns:\n        # Create empty pie chart with error message\n        ax.text(0.5, 0.5, f'Column \"{column}\" not found', \n                horizontalalignment='center',\n                verticalalignment='center',\n                transform=ax.transAxes)\n        ax.set_xlim(-1, 1)\n        ax.set_ylim(-1, 1)\n        return stats, ax\n    \n    # Get column values\n    column_values = pd.to_numeric(df[column], errors='coerce')\n    \n    # Filter out NaN values for statistics calculation\n    valid_values = column_values.dropna()\n    \n    if len(valid_values) > 0:\n        # Calculate statistics\n        stats['sum'] = float(valid_values.sum())\n        stats['mean'] = float(valid_values.mean())\n        stats['min'] = float(valid_values.min())\n        stats['max'] = float(valid_values.max())\n        \n        # Create pie chart\n        # Use Age column as labels if it exists, otherwise use index\n        if 'Age' in df.columns:\n            labels = df['Age'].astype(str).tolist()\n        else:\n            labels = [f'Employee {i+1}' for i in range(len(df))]\n        \n        # Handle negative values for pie chart (use absolute values)\n        pie_values = column_values.fillna(0).abs()\n        \n        # Only create pie chart if there are positive values\n        if pie_values.sum() > 0:\n            ax.pie(pie_values, labels=labels, autopct='%1.1f%%', startangle=90)\n            ax.set_title(f'{column} Distribution')\n        else:\n            ax.text(0.5, 0.5, 'All values are zero or invalid', \n                    horizontalalignment='center',\n                    verticalalignment='center',\n                    transform=ax.transAxes)\n            ax.set_xlim(-1, 1)\n            ax.set_ylim(-1, 1)\n    else:\n        # No valid numeric values\n        ax.text(0.5, 0.5, 'No valid numeric values', \n                horizontalalignment='center',\n                verticalalignment='center',\n                transform=ax.transAxes)\n        ax.set_xlim(-1, 1)\n        ax.set_ylim(-1, 1)\n    \n    return stats, ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 120, in test_case_1\nAssertionError: {'sum': 0, 'mean': nan, 'min': nan, 'max': nan} != {'sum': 500000, 'mean': 100000.0, 'min': 50000, 'max': 150000}\n- {'max': nan, 'mean': nan, 'min': nan, 'sum': 0}\n+ {'max': 150000, 'mean': 100000.0, 'min': 50000, 'sum': 500000}\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 133, in test_case_2\nAssertionError: {'sum': 0, 'mean': nan, 'min': nan, 'max': nan} != {'sum': 41, 'mean': 8.2, 'min': 3, 'max': 13}\n- {'max': nan, 'mean': nan, 'min': nan, 'sum': 0}\n?         ^^^          ^^^         ^^^         ^\n\n+ {'max': 13, 'mean': 8.2, 'min': 3, 'sum': 41}\n?         ^^          ^^^         ^         ^^\n\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 144, in test_case_3\nAssertionError: {'sum': 0, 'mean': nan, 'min': nan, 'max': nan} != {'sum': 185, 'mean': 37.0, 'min': 27, 'max': 47}\n- {'max': nan, 'mean': nan, 'min': nan, 'sum': 0}\n?         ^^^          ^^^         ^^^         ^\n\n+ {'max': 47, 'mean': 37.0, 'min': 27, 'sum': 185}\n?         ^^          ^^^^         ^^         ^^^\n\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 156, in test_case_5\nAssertionError: {'sum': 0, 'mean': nan, 'min': nan, 'max': nan} != {'sum': 30, 'mean': 30.0, 'min': 30, 'max': 30}\n- {'max': nan, 'mean': nan, 'min': nan, 'sum': 0}\n?         ^^^          ^^^         ^^^\n\n+ {'max': 30, 'mean': 30.0, 'min': 30, 'sum': 30}\n?         ^^          ^^^^         ^^         +\n\n",
          "test_case_6": "Traceback (most recent call last):\n  File \"__test__.py\", line 164, in test_case_6\nAssertionError: KeyError not raised\n",
          "test_case_7": "Traceback (most recent call last):\n  File \"__test__.py\", line 177, in test_case_7\nAssertionError: 0 != 5 : The number of pie slices does not match the number of data points.\n"
        }
      }
    ],
    "BigCodeBench/579": [
      {
        "task_id": "BigCodeBench/579",
        "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(csv_file):\n\n    pass\nimport unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(csv_file):\n    \"\"\"\n    Reads a CSV file, normalizes the text in it to ASCII, counts the words, \n    and returns the 10 most common words along with their frequencies as a \n    matplotlib bar plot and a list of tuples.\n    \n    Args:\n        csv_file (str): Path to the CSV file\n        \n    Returns:\n        tuple: A tuple containing matplotlib.axes.Axes object for the bar plot \n               and a list of the 10 most common words with their frequencies.\n               \n    Raises:\n        FileNotFoundError: If the CSV file cannot be found at the specified path.\n        IOError: If there is an error in reading the file.\n    \"\"\"\n    try:\n        # Read the CSV file\n        all_text = []\n        with open(csv_file, 'r', encoding='utf-8') as file:\n            csv_reader = csv.reader(file)\n            for row in csv_reader:\n                # Concatenate all cells in the row\n                row_text = ' '.join(row)\n                all_text.append(row_text)\n                \n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The CSV file '{csv_file}' cannot be found at the specified path.\")\n    except Exception as e:\n        raise IOError(f\"Error reading the file: {str(e)}\")\n    \n    # Combine all text\n    combined_text = ' '.join(all_text)\n    \n    # Normalize text to ASCII\n    normalized_text = unicodedata.normalize('NFKD', combined_text)\n    ascii_text = normalized_text.encode('ascii', 'ignore').decode('ascii')\n    \n    # Extract words (split by whitespace and remove punctuation)\n    # Convert to lowercase for consistent counting\n    words = []\n    current_word = []\n    \n    for char in ascii_text.lower():\n        if char.isalnum():\n            current_word.append(char)\n        else:\n            if current_word:\n                words.append(''.join(current_word))\n                current_word = []\n    \n    # Don't forget the last word if the text doesn't end with punctuation\n    if current_word:\n        words.append(''.join(current_word))\n    \n    # Count words\n    word_counter = Counter(words)\n    \n    # Get the 10 most common words\n    most_common_10 = word_counter.most_common(10)\n    \n    # Create bar plot\n    if most_common_10:\n        words_list, frequencies = zip(*most_common_10)\n    else:\n        words_list, frequencies = [], []\n    \n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(words_list, frequencies)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 Most Common Words')\n    \n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45, ha='right')\n    \n    # Adjust layout to prevent label cutoff\n    plt.tight_layout()\n    \n    return ax, most_common_10",
        "status": "fail",
        "details": {
          "test_normalized_text": "Traceback (most recent call last):\n  File \"__test__.py\", line 134, in test_normalized_text\n  File \"__test__.py\", line 90, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/pyplot.py\", line 2322, in tight_layout\n    return gcf().tight_layout(pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/figure.py\", line 3494, in tight_layout\n    engine.execute(self)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/layout_engine.py\", line 175, in execute\n    kwargs = get_tight_layout_figure(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/_tight_layout.py\", line 266, in get_tight_layout_figure\n    kwargs = _auto_adjust_subplotpars(fig, renderer,\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/_tight_layout.py\", line 82, in _auto_adjust_subplotpars\n    bb += [martist._get_tightbbox_for_layout_only(ax, renderer)]\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/artist.py\", line 1415, in _get_tightbbox_for_layout_only\n    return obj.get_tightbbox(*args, **{**kwargs, \"for_layout_only\": True})\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axes/_base.py\", line 4387, in get_tightbbox\n    ba = martist._get_tightbbox_for_layout_only(axis, renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/artist.py\", line 1415, in _get_tightbbox_for_layout_only\n    return obj.get_tightbbox(*args, **{**kwargs, \"for_layout_only\": True})\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 1320, in get_tightbbox\n    self._update_label_position(renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 2308, in _update_label_position\n    bboxes, bboxes2 = self._get_tick_boxes_siblings(renderer=renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 2104, in _get_tick_boxes_siblings\n    tlb, tlb2 = axis._get_ticklabel_bboxes(ticks_to_draw, renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 1299, in _get_ticklabel_bboxes\n    return ([tick.label1.get_window_extent(renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 1299, in <listcomp>\n    return ([tick.label1.get_window_extent(renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 959, in get_window_extent\n    bbox, info, descent = self._get_layout(self._renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 378, in _get_layout\n    _, lp_h, lp_d = _get_text_metrics_with_cache(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 97, in _get_text_metrics_with_cache\n    return _get_text_metrics_with_cache_impl(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 105, in _get_text_metrics_with_cache_impl\n    return renderer_ref().get_text_width_height_descent(text, fontprop, ismath)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py\", line 233, in get_text_width_height_descent\n    font = self._prepare_font(prop)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py\", line 267, in _prepare_font\n    font = get_font(_fontManager._find_fonts_by_props(font_prop))\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/_api/deprecation.py\", line 297, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/font_manager.py\", line 1520, in get_font\n    return _get_font(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/font_manager.py\", line 1461, in _get_font\n    return ft2font.FT2Font(\nRuntimeError: In FT2Font: Can not load face (invalid stream operation; error code 0x55)\n",
          "test_plot_output": "Traceback (most recent call last):\n  File \"__test__.py\", line 127, in test_plot_output\n  File \"__test__.py\", line 90, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/pyplot.py\", line 2322, in tight_layout\n    return gcf().tight_layout(pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/figure.py\", line 3494, in tight_layout\n    engine.execute(self)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/layout_engine.py\", line 175, in execute\n    kwargs = get_tight_layout_figure(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/_tight_layout.py\", line 266, in get_tight_layout_figure\n    kwargs = _auto_adjust_subplotpars(fig, renderer,\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/_tight_layout.py\", line 82, in _auto_adjust_subplotpars\n    bb += [martist._get_tightbbox_for_layout_only(ax, renderer)]\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/artist.py\", line 1415, in _get_tightbbox_for_layout_only\n    return obj.get_tightbbox(*args, **{**kwargs, \"for_layout_only\": True})\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axes/_base.py\", line 4387, in get_tightbbox\n    ba = martist._get_tightbbox_for_layout_only(axis, renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/artist.py\", line 1415, in _get_tightbbox_for_layout_only\n    return obj.get_tightbbox(*args, **{**kwargs, \"for_layout_only\": True})\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 1320, in get_tightbbox\n    self._update_label_position(renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 2308, in _update_label_position\n    bboxes, bboxes2 = self._get_tick_boxes_siblings(renderer=renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 2104, in _get_tick_boxes_siblings\n    tlb, tlb2 = axis._get_ticklabel_bboxes(ticks_to_draw, renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 1299, in _get_ticklabel_bboxes\n    return ([tick.label1.get_window_extent(renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 1299, in <listcomp>\n    return ([tick.label1.get_window_extent(renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 959, in get_window_extent\n    bbox, info, descent = self._get_layout(self._renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 378, in _get_layout\n    _, lp_h, lp_d = _get_text_metrics_with_cache(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 97, in _get_text_metrics_with_cache\n    return _get_text_metrics_with_cache_impl(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 105, in _get_text_metrics_with_cache_impl\n    return renderer_ref().get_text_width_height_descent(text, fontprop, ismath)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py\", line 233, in get_text_width_height_descent\n    font = self._prepare_font(prop)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py\", line 267, in _prepare_font\n    font = get_font(_fontManager._find_fonts_by_props(font_prop))\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/_api/deprecation.py\", line 297, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/font_manager.py\", line 1520, in get_font\n    return _get_font(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/font_manager.py\", line 1461, in _get_font\n    return ft2font.FT2Font(\nRuntimeError: In FT2Font: Can not load face (invalid stream operation; error code 0x55)\n",
          "test_valid_csv_file": "Traceback (most recent call last):\n  File \"__test__.py\", line 111, in test_valid_csv_file\n  File \"__test__.py\", line 90, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/pyplot.py\", line 2322, in tight_layout\n    return gcf().tight_layout(pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/figure.py\", line 3494, in tight_layout\n    engine.execute(self)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/layout_engine.py\", line 175, in execute\n    kwargs = get_tight_layout_figure(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/_tight_layout.py\", line 266, in get_tight_layout_figure\n    kwargs = _auto_adjust_subplotpars(fig, renderer,\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/_tight_layout.py\", line 82, in _auto_adjust_subplotpars\n    bb += [martist._get_tightbbox_for_layout_only(ax, renderer)]\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/artist.py\", line 1415, in _get_tightbbox_for_layout_only\n    return obj.get_tightbbox(*args, **{**kwargs, \"for_layout_only\": True})\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axes/_base.py\", line 4387, in get_tightbbox\n    ba = martist._get_tightbbox_for_layout_only(axis, renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/artist.py\", line 1415, in _get_tightbbox_for_layout_only\n    return obj.get_tightbbox(*args, **{**kwargs, \"for_layout_only\": True})\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 1320, in get_tightbbox\n    self._update_label_position(renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 2308, in _update_label_position\n    bboxes, bboxes2 = self._get_tick_boxes_siblings(renderer=renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 2104, in _get_tick_boxes_siblings\n    tlb, tlb2 = axis._get_ticklabel_bboxes(ticks_to_draw, renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 1299, in _get_ticklabel_bboxes\n    return ([tick.label1.get_window_extent(renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 1299, in <listcomp>\n    return ([tick.label1.get_window_extent(renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 959, in get_window_extent\n    bbox, info, descent = self._get_layout(self._renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 378, in _get_layout\n    _, lp_h, lp_d = _get_text_metrics_with_cache(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 97, in _get_text_metrics_with_cache\n    return _get_text_metrics_with_cache_impl(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 105, in _get_text_metrics_with_cache_impl\n    return renderer_ref().get_text_width_height_descent(text, fontprop, ismath)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py\", line 233, in get_text_width_height_descent\n    font = self._prepare_font(prop)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py\", line 267, in _prepare_font\n    font = get_font(_fontManager._find_fonts_by_props(font_prop))\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/_api/deprecation.py\", line 297, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/font_manager.py\", line 1520, in get_font\n    return _get_font(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/font_manager.py\", line 1461, in _get_font\n    return ft2font.FT2Font(\nRuntimeError: In FT2Font: Can not load face (invalid stream operation; error code 0x55)\n"
        }
      }
    ],
    "BigCodeBench/594": [
      {
        "task_id": "BigCodeBench/594",
        "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime\nfrom random import randint\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n\n    pass\nimport csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generate weather data for the specified number of hours, save it in a CSV file \n    with columns 'Time' and 'Condition' and back up the file to a backup directory.\n    \n    Args:\n        hours (int): Number of hours to generate weather data for\n        output_dir (str): Directory to save the output file\n        \n    Returns:\n        str: The path of the generated CSV file\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Create backup directory\n    backup_dir = os.path.join(output_dir, 'backup')\n    os.makedirs(backup_dir, exist_ok=True)\n    \n    # Generate filename with timestamp\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    filename = f'weather_data_{timestamp}.csv'\n    filepath = os.path.join(output_dir, filename)\n    \n    # Generate weather data\n    weather_data = []\n    start_time = datetime.now()\n    \n    for hour in range(hours):\n        current_time = start_time + timedelta(hours=hour)\n        # Randomly select a weather condition\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS) - 1)]\n        weather_data.append({\n            'Time': current_time.strftime('%Y-%m-%d %H:%M:%S'),\n            'Condition': condition\n        })\n    \n    # Write data to CSV file\n    with open(filepath, 'w', newline='') as csvfile:\n        fieldnames = ['Time', 'Condition']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        # Write header\n        writer.writeheader()\n        \n        # Write data rows\n        for row in weather_data:\n            writer.writerow(row)\n    \n    # Create backup copy\n    backup_filepath = os.path.join(backup_dir, filename)\n    shutil.copy2(filepath, backup_filepath)\n    \n    return filepath",
        "status": "fail",
        "details": {
          "test_backup_file_creation": "Traceback (most recent call last):\n  File \"__test__.py\", line 133, in test_backup_file_creation\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 940, in assert_called_once_with\n    raise AssertionError(msg)\nAssertionError: Expected 'copy' to be called once. Called 0 times.\n",
          "test_task_func_checks_backup_directory_exists": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 103, in test_task_func_checks_backup_directory_exists\nAssertionError: 'output/backup' != 'output'\n- output/backup\n+ output\n : Expected output/backup, got output\n",
          "test_task_func_copies_to_backup_directory": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 111, in test_task_func_copies_to_backup_directory\nTypeError: 'NoneType' object is not subscriptable\n",
          "test_task_func_writes_correct_header": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/shutil.py\", line 142, in _fastcopy_sendfile\n    sent = os.sendfile(outfd, infd, offset, blocksize)\nOSError: [Errno 9] Bad file descriptor: <MagicMock name='open().name' id='139918975398672'> -> <MagicMock name='open().name' id='139918975398672'>\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 122, in test_task_func_writes_correct_header\n  File \"__test__.py\", line 70, in task_func\n  File \"/usr/local/lib/python3.10/shutil.py\", line 434, in copy2\n    copyfile(src, dst, follow_symlinks=follow_symlinks)\n  File \"/usr/local/lib/python3.10/shutil.py\", line 267, in copyfile\n    _fastcopy_sendfile(fsrc, fdst)\n  File \"/usr/local/lib/python3.10/shutil.py\", line 159, in _fastcopy_sendfile\n    if offset == 0 and os.lseek(outfd, 0, os.SEEK_CUR) == 0:\nOSError: [Errno 29] Illegal seek\n"
        }
      }
    ],
    "BigCodeBench/528": [
      {
        "task_id": "BigCodeBench/528",
        "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n\n    pass\nimport csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    \"\"\"\n    Identifies duplicate rows from a CSV file using the csv library, \n    converts duplicated rows into a pandas DataFrame, then plots using matplotlib.\n    \n    Args:\n        file_path (str): Path to the CSV file\n        \n    Returns:\n        dict: A dictionary with duplicate rows as keys and their counts as values.\n        Axes: A matplotlib Axes object with the bar chart of duplicate rows.\n        \n    Raises:\n        ValueError: If the file is not a CSV file\n    \"\"\"\n    # Check if file has .csv extension\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"File must have a .csv extension\")\n    \n    # Read CSV file and collect all rows\n    rows = []\n    with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n        csv_reader = csv.reader(csvfile)\n        # Skip header if exists (optional - you can modify this behavior)\n        header = next(csv_reader, None)\n        \n        for row in csv_reader:\n            # Convert row to tuple so it can be used as dictionary key\n            rows.append(tuple(row))\n    \n    # Count occurrences of each row\n    row_counts = Counter(rows)\n    \n    # Filter only duplicate rows (count > 1)\n    duplicate_rows = {row: count for row, count in row_counts.items() if count > 1}\n    \n    # If no duplicates found, return empty dict and empty plot\n    if not duplicate_rows:\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.set_title('No Duplicate Rows Found')\n        ax.set_xlabel('Rows')\n        ax.set_ylabel('Count')\n        return {}, ax\n    \n    # Convert duplicate rows to pandas DataFrame for easier plotting\n    # Create a list of dictionaries for DataFrame construction\n    df_data = []\n    for row, count in duplicate_rows.items():\n        # Convert row tuple to string for display\n        row_str = ', '.join(str(item) for item in row)\n        df_data.append({'Row': row_str, 'Count': count})\n    \n    df = pd.DataFrame(df_data)\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    # Create bar chart\n    bars = ax.bar(range(len(df)), df['Count'])\n    \n    # Customize the plot\n    ax.set_xlabel('Duplicate Rows', fontsize=12)\n    ax.set_ylabel('Count', fontsize=12)\n    ax.set_title('Duplicate Rows in CSV File', fontsize=14, fontweight='bold')\n    \n    # Set x-axis labels\n    ax.set_xticks(range(len(df)))\n    ax.set_xticklabels(df['Row'], rotation=45, ha='right')\n    \n    # Add value labels on top of bars\n    for i, bar in enumerate(bars):\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height,\n                f'{int(height)}',\n                ha='center', va='bottom')\n    \n    # Adjust layout to prevent label cutoff\n    plt.tight_layout()\n    \n    return duplicate_rows, ax",
        "status": "fail",
        "details": {
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 126, in test_case_2\nAssertionError: 'Duplicate Rows in CSV File' != 'Duplicate Entries'\n- Duplicate Rows in CSV File\n+ Duplicate Entries\n\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 134, in test_case_3\nAssertionError: <Axes: title={'center': 'No Duplicate Rows Found'}, xlabel='Rows', ylabel='Count'> is not None\n"
        }
      }
    ],
    "BigCodeBench/443": [
      {
        "task_id": "BigCodeBench/443",
        "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n\n    pass\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    Calculate the product of a matrix 'P' and a 3D tensor 'T', flatten the result,\n    apply KMeans clustering to the flattened data, and visualize it.\n    \n    Parameters:\n    -----------\n    P : np.ndarray\n        2D matrix\n    T : np.ndarray\n        3D tensor\n    n_clusters : int, default=3\n        Number of clusters for KMeans\n    random_state : int, default=0\n        Random state for reproducibility\n    n_init : int, default=10\n        Number of time the k-means algorithm will be run with different centroid seeds\n    \n    Returns:\n    --------\n    cluster_result : np.ndarray\n        The result of KMeans clustering (cluster labels)\n    ax : matplotlib.axes.Axes\n        The visualization of the KMeans clustering\n    \"\"\"\n    \n    # Calculate the product of matrix P and tensor T\n    # For matrix-tensor multiplication, we need to handle the dimensions properly\n    # We'll multiply P with each 2D slice of T along the first dimension\n    result = np.zeros((P.shape[0], T.shape[1], T.shape[2]))\n    \n    for i in range(T.shape[0]):\n        result += P[:, i:i+1] @ T[i:i+1, :, :]\n    \n    # Flatten the result\n    flattened_data = result.flatten()\n    \n    # Reshape to 2D array for KMeans (n_samples, n_features)\n    # Since we have 1D data, we reshape to (n_samples, 1)\n    X = flattened_data.reshape(-1, 1)\n    \n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_labels = kmeans.fit_predict(X)\n    \n    # Create visualization\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Create scatter plot with different colors for each cluster\n    scatter = ax.scatter(range(len(flattened_data)), flattened_data, \n                        c=cluster_labels, cmap='viridis', alpha=0.6, s=50)\n    \n    # Add cluster centers\n    centers = kmeans.cluster_centers_.flatten()\n    center_indices = []\n    for center in centers:\n        # Find the closest point to each center\n        closest_idx = np.argmin(np.abs(flattened_data - center))\n        center_indices.append(closest_idx)\n    \n    ax.scatter(center_indices, centers, c='red', marker='x', s=200, linewidths=3,\n              label='Cluster Centers')\n    \n    # Customize the plot\n    ax.set_xlabel('Data Point Index')\n    ax.set_ylabel('Value')\n    ax.set_title('KMeans Clustering Visualization')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # Add colorbar\n    plt.colorbar(scatter, ax=ax, label='Cluster')\n    \n    return cluster_labels, ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 116, in test_case_1\n  File \"__test__.py\", line 54, in task_func\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 1)\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 123, in test_case_2\n  File \"__test__.py\", line 54, in task_func\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 1)\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 139, in test_case_3\n  File \"__test__.py\", line 54, in task_func\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 1)\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 147, in test_case_4\n  File \"__test__.py\", line 54, in task_func\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 1)\n",
          "test_case_6": "Traceback (most recent call last):\n  File \"__test__.py\", line 160, in test_case_6\n  File \"__test__.py\", line 51, in task_func\nIndexError: tuple index out of range\n"
        }
      }
    ],
    "BigCodeBench/553": [
      {
        "task_id": "BigCodeBench/553",
        "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(a, b):\n\n    pass\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(a, b):\n    \"\"\"\n    Generate a pandas DataFrame with random values based on lists 'a' and 'b', \n    and plot it as a bar chart.\n    \n    Parameters:\n    - a: list, sets the DataFrame's row indices\n    - b: list, its length determines the number of columns\n    \n    Returns:\n    - matplotlib.axes.Axes: The Axes object of the plotted bar chart\n    \"\"\"\n    # Determine the number of rows and columns\n    num_rows = len(a)\n    num_cols = len(b)\n    \n    # Ensure we don't exceed the available column names\n    if num_cols > len(COLUMNS):\n        raise ValueError(f\"Length of list 'b' ({num_cols}) exceeds available columns ({len(COLUMNS)})\")\n    \n    # Generate random data\n    data = np.random.rand(num_rows, num_cols)\n    \n    # Create DataFrame with specified row indices and column names\n    df = pd.DataFrame(data, index=a, columns=COLUMNS[:num_cols])\n    \n    # Create the bar chart\n    ax = df.plot(kind='bar', figsize=(10, 6))\n    \n    # Customize the plot\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Values')\n    ax.set_title('Bar Chart of Random Values')\n    ax.legend(title='Columns')\n    \n    # Rotate x-axis labels if needed for better readability\n    plt.xticks(rotation=45, ha='right')\n    \n    # Adjust layout to prevent label cutoff\n    plt.tight_layout()\n    \n    return ax",
        "status": "fail",
        "details": {
          "test_both_lists_empty": "Traceback (most recent call last):\n  File \"__test__.py\", line 72, in test_both_lists_empty\n  File \"__test__.py\", line 40, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_core.py\", line 975, in __call__\n    return plot_backend.plot(data, kind=kind, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_matplotlib/__init__.py\", line 71, in plot\n    plot_obj.generate()\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_matplotlib/core.py\", line 446, in generate\n    self._compute_plot_data()\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_matplotlib/core.py\", line 632, in _compute_plot_data\n    raise TypeError(\"no numeric data to plot\")\nTypeError: no numeric data to plot\n",
          "test_empty_a_list": "Traceback (most recent call last):\n  File \"__test__.py\", line 64, in test_empty_a_list\n  File \"__test__.py\", line 40, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_core.py\", line 975, in __call__\n    return plot_backend.plot(data, kind=kind, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_matplotlib/__init__.py\", line 71, in plot\n    plot_obj.generate()\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_matplotlib/core.py\", line 455, in generate\n    self._post_plot_logic(ax, self.data)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_matplotlib/core.py\", line 1750, in _post_plot_logic\n    s_edge = self.ax_pos[0] - 0.25 + self.lim_offset\nIndexError: index 0 is out of bounds for axis 0 with size 0\n",
          "test_empty_b_list": "Traceback (most recent call last):\n  File \"__test__.py\", line 68, in test_empty_b_list\n  File \"__test__.py\", line 40, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_core.py\", line 975, in __call__\n    return plot_backend.plot(data, kind=kind, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_matplotlib/__init__.py\", line 71, in plot\n    plot_obj.generate()\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_matplotlib/core.py\", line 446, in generate\n    self._compute_plot_data()\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_matplotlib/core.py\", line 632, in _compute_plot_data\n    raise TypeError(\"no numeric data to plot\")\nTypeError: no numeric data to plot\n"
        }
      }
    ],
    "BigCodeBench/590": [
      {
        "task_id": "BigCodeBench/590",
        "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(url):\n\n    pass\nimport urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(url):\n    \"\"\"\n    Extracts the text and href attributes of all anchor tags from a given URL's HTML content,\n    and returns this data in a pandas DataFrame along with the time of data extraction.\n    \n    Parameters:\n    url (str): The URL from which to fetch HTML content\n    \n    Returns:\n    pandas.DataFrame: A DataFrame with columns 'text', 'href', and 'fetch_time'. Each row\n    corresponds to an anchor tag in the HTML, with 'text' and 'href' containing\n    the text and the hyperlink reference of the anchor tag, respectively.\n    'fetch_time' contains the timestamp of when the data was fetched in the format\n    'YYYY-MM-DD HH:MM:SS'.\n    \n    Raises:\n    ValueError: If the provided URL is invalid or empty.\n    URLError: If there is an issue with network connectivity or the server.\n    \"\"\"\n    \n    # Validate URL\n    if not url or not isinstance(url, str) or url.strip() == \"\":\n        raise ValueError(\"URL cannot be empty or invalid\")\n    \n    # Clean the URL\n    url = url.strip()\n    \n    # Check if URL has a valid format (basic check)\n    if not url.startswith(('http://', 'https://')):\n        raise ValueError(\"URL must start with http:// or https://\")\n    \n    try:\n        # Fetch HTML content from the URL\n        response = urllib.request.urlopen(url)\n        html_content = response.read().decode('utf-8')\n        \n        # Get current timestamp\n        fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        \n        # Parse HTML content using PyQuery\n        doc = pq(html_content)\n        \n        # Extract anchor tags\n        anchor_data = []\n        \n        # Find all anchor tags\n        anchors = doc('a')\n        \n        for anchor in anchors:\n            anchor_elem = pq(anchor)\n            text = anchor_elem.text() or \"\"  # Get text content, empty string if None\n            href = anchor_elem.attr('href') or \"\"  # Get href attribute, empty string if None\n            \n            anchor_data.append({\n                'text': text,\n                'href': href,\n                'fetch_time': fetch_time\n            })\n        \n        # Create DataFrame\n        df = pd.DataFrame(anchor_data)\n        \n        # If no anchors found, return empty DataFrame with correct columns\n        if df.empty:\n            df = pd.DataFrame(columns=['text', 'href', 'fetch_time'])\n        \n        return df\n        \n    except urllib.error.URLError as e:\n        # Re-raise URLError for network issues\n        raise e\n    except Exception as e:\n        # For any other exceptions, check if it's URL-related\n        if \"unknown url type\" in str(e).lower():\n            raise ValueError(f\"Invalid URL format: {url}\")\n        else:\n            raise e",
        "status": "fail",
        "details": {
          "test_dynamic_comparison": "Traceback (most recent call last):\n  File \"__test__.py\", line 127, in test_dynamic_comparison\n  File \"__test__.py\", line 115, in fetch_and_parse_url\n  File \"/usr/local/lib/python3.10/urllib/request.py\", line 216, in urlopen\n    return opener.open(url, data, timeout)\n  File \"/usr/local/lib/python3.10/urllib/request.py\", line 525, in open\n    response = meth(req, response)\n  File \"/usr/local/lib/python3.10/urllib/request.py\", line 634, in http_response\n    response = self.parent.error(\n  File \"/usr/local/lib/python3.10/urllib/request.py\", line 563, in error\n    return self._call_chain(*args)\n  File \"/usr/local/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n    result = func(*args)\n  File \"/usr/local/lib/python3.10/urllib/request.py\", line 643, in http_error_default\n    raise HTTPError(req.full_url, code, msg, hdrs, fp)\nurllib.error.HTTPError: HTTP Error 403: Forbidden\n",
          "test_fetch_time_format": "Traceback (most recent call last):\n  File \"__test__.py\", line 143, in test_fetch_time_format\n  File \"__test__.py\", line 82, in task_func\n  File \"__test__.py\", line 45, in task_func\n  File \"/usr/local/lib/python3.10/urllib/request.py\", line 216, in urlopen\n    return opener.open(url, data, timeout)\n  File \"/usr/local/lib/python3.10/urllib/request.py\", line 525, in open\n    response = meth(req, response)\n  File \"/usr/local/lib/python3.10/urllib/request.py\", line 634, in http_response\n    response = self.parent.error(\n  File \"/usr/local/lib/python3.10/urllib/request.py\", line 563, in error\n    return self._call_chain(*args)\n  File \"/usr/local/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n    result = func(*args)\n  File \"/usr/local/lib/python3.10/urllib/request.py\", line 643, in http_error_default\n    raise HTTPError(req.full_url, code, msg, hdrs, fp)\nurllib.error.HTTPError: HTTP Error 403: Forbidden\n",
          "test_valid_url": "Traceback (most recent call last):\n  File \"__test__.py\", line 96, in test_valid_url\n  File \"__test__.py\", line 82, in task_func\n  File \"__test__.py\", line 45, in task_func\n  File \"/usr/local/lib/python3.10/urllib/request.py\", line 216, in urlopen\n    return opener.open(url, data, timeout)\n  File \"/usr/local/lib/python3.10/urllib/request.py\", line 525, in open\n    response = meth(req, response)\n  File \"/usr/local/lib/python3.10/urllib/request.py\", line 634, in http_response\n    response = self.parent.error(\n  File \"/usr/local/lib/python3.10/urllib/request.py\", line 563, in error\n    return self._call_chain(*args)\n  File \"/usr/local/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n    result = func(*args)\n  File \"/usr/local/lib/python3.10/urllib/request.py\", line 643, in http_error_default\n    raise HTTPError(req.full_url, code, msg, hdrs, fp)\nurllib.error.HTTPError: HTTP Error 403: Forbidden\n"
        }
      }
    ],
    "BigCodeBench/574": [
      {
        "task_id": "BigCodeBench/574",
        "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100, noise_level=0.2):\n\n    pass\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100, noise_level=0.2):\n    \"\"\"\n    Create a noisy sine wave of a specified length and adjusts a curve using curve_fit from scipy.optimize to the data.\n    \n    Parameters:\n    array_length (int): Length of the array (default: 100)\n    noise_level (float): Standard deviation of the noise (default: 0.2)\n    \n    Returns:\n    Axes object: A plot showing the noisy sine wave and its adjusted curve.\n    \"\"\"\n    # Generate x values\n    x = np.linspace(0, 4 * np.pi, array_length)\n    \n    # Generate true sine wave\n    true_amplitude = 2.0\n    true_frequency = 1.0\n    true_phase = 0.5\n    true_offset = 0.0\n    \n    # Create noisy sine wave\n    y_true = true_amplitude * np.sin(true_frequency * x + true_phase) + true_offset\n    noise = np.random.normal(0, noise_level, array_length)\n    y_noisy = y_true + noise\n    \n    # Define the sine function to fit\n    def sine_func(x, amplitude, frequency, phase, offset):\n        return amplitude * np.sin(frequency * x + phase) + offset\n    \n    # Initial guess for parameters [amplitude, frequency, phase, offset]\n    initial_guess = [1.5, 1.0, 0.0, 0.0]\n    \n    # Fit the curve\n    try:\n        popt, pcov = curve_fit(sine_func, x, y_noisy, p0=initial_guess)\n        fitted_amplitude, fitted_frequency, fitted_phase, fitted_offset = popt\n        \n        # Generate fitted curve\n        y_fitted = sine_func(x, *popt)\n        \n    except Exception as e:\n        print(f\"Curve fitting failed: {e}\")\n        y_fitted = np.zeros_like(x)\n        popt = initial_guess\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot noisy data\n    ax.scatter(x, y_noisy, alpha=0.6, s=20, label='Noisy data', color='blue')\n    \n    # Plot fitted curve\n    ax.plot(x, y_fitted, 'r-', linewidth=2, label='Fitted curve')\n    \n    # Plot true curve for reference (optional)\n    ax.plot(x, y_true, 'g--', linewidth=1, alpha=0.7, label='True curve')\n    \n    # Add labels and title\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title(f'Noisy Sine Wave with Fitted Curve\\n' + \n                 f'Fitted: A={popt[0]:.2f}, f={popt[1]:.2f}, \u03c6={popt[2]:.2f}, offset={popt[3]:.2f}')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # Set axis limits for better visualization\n    ax.set_xlim(x[0], x[-1])\n    \n    return ax",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/593": [
      {
        "task_id": "BigCodeBench/593",
        "solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n\n    pass\nimport csv\nimport os\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generates traffic data for different vehicle types over a specified number of hours,\n    saves the data to a CSV file with columns 'Time', 'Car', 'Bus', 'Truck', and 'Bike',\n    and plots the data in a line chart with 'Time' on x-axis and 'Vehicle Count' on y-axis.\n    \n    Parameters:\n    hours (int): Number of hours to generate data for\n    output_dir (str): Directory to save the output files\n    \n    Returns:\n    tuple: Path to the CSV file and the matplotlib axes object of the line plot.\n    \"\"\"\n    \n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Generate traffic data\n    data = []\n    for hour in range(hours):\n        row = {'Time': hour}\n        for vehicle_type in VEHICLE_TYPES:\n            # Generate random vehicle count with some realistic patterns\n            if vehicle_type == 'Car':\n                # Cars have higher traffic during rush hours\n                if 7 <= hour <= 9 or 17 <= hour <= 19:\n                    count = randint(80, 150)\n                elif 0 <= hour <= 6 or 22 <= hour <= 23:\n                    count = randint(10, 30)\n                else:\n                    count = randint(40, 80)\n            elif vehicle_type == 'Bus':\n                # Buses have moderate traffic\n                if 6 <= hour <= 20:\n                    count = randint(10, 30)\n                else:\n                    count = randint(2, 10)\n            elif vehicle_type == 'Truck':\n                # Trucks have higher traffic during business hours\n                if 8 <= hour <= 17:\n                    count = randint(20, 50)\n                else:\n                    count = randint(5, 20)\n            else:  # Bike\n                # Bikes have higher traffic during daylight hours\n                if 6 <= hour <= 20:\n                    count = randint(15, 40)\n                else:\n                    count = randint(2, 10)\n            \n            row[vehicle_type] = count\n        data.append(row)\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    \n    # Generate timestamp for filename\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    csv_filename = f\"traffic_data_{timestamp}.csv\"\n    csv_path = os.path.join(output_dir, csv_filename)\n    \n    # Save to CSV\n    df.to_csv(csv_path, index=False)\n    \n    # Create line plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot each vehicle type\n    for vehicle_type in VEHICLE_TYPES:\n        ax.plot(df['Time'], df[vehicle_type], marker='o', label=vehicle_type, linewidth=2)\n    \n    # Customize the plot\n    ax.set_xlabel('Time (Hour)', fontsize=12)\n    ax.set_ylabel('Vehicle Count', fontsize=12)\n    ax.set_title('Traffic Data by Vehicle Type Over Time', fontsize=14, fontweight='bold')\n    ax.legend(loc='best')\n    ax.grid(True, alpha=0.3)\n    ax.set_xticks(range(0, hours, max(1, hours//10)))\n    \n    # Tight layout for better appearance\n    plt.tight_layout()\n    \n    return csv_path, ax",
        "status": "fail",
        "details": {
          "test_dataframe_content": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 130, in test_dataframe_content\nAssertionError: './output/traffic_data_20250903_191351.csv' != './output/traffic_data.csv'\n- ./output/traffic_data_20250903_191351.csv\n?                      ----------------\n+ ./output/traffic_data.csv\n\n",
          "test_directory_creation": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 146, in test_directory_creation\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 929, in assert_called_with\n    raise AssertionError(_error_message()) from cause\nAssertionError: expected call not found.\nExpected: makedirs('./output')\nActual: makedirs('./output', exist_ok=True)\n",
          "test_plot_generation": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 151, in test_plot_generation\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 898, in assert_called\n    raise AssertionError(msg)\nAssertionError: Expected 'show' to have been called.\n",
          "test_empty_dataframe_on_zero_hours": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 137, in test_empty_dataframe_on_zero_hours\n  File \"__test__.py\", line 90, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/range.py\", line 349, in get_loc\n    raise KeyError(key)\nKeyError: 'Time'\n"
        }
      }
    ],
    "BigCodeBench/477": [
      {
        "task_id": "BigCodeBench/477",
        "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n\n    pass\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    \"\"\"\n    Create a DataFrame with N rows and 3 columns: \"x\" and \"y\" with random values,\n    and \"category\" with random categories from CATEGORIES list.\n    Each category is guaranteed to appear at least once if N >= len(CATEGORIES).\n    \n    Parameters:\n    -----------\n    N : int, default=100\n        Number of rows in the DataFrame\n    CATEGORIES : list, default=[\"A\", \"B\", \"C\", \"D\", \"E\"]\n        List of categories to choose from\n    seed : int, default=42\n        Random seed for reproducibility\n    \n    Returns:\n    --------\n    tuple: A tuple containing:\n        DataFrame: The generated DataFrame.\n        Axes: The Axes object of the scatter plot.\n    \"\"\"\n    # Set random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate random x and y values\n    x_values = np.random.randn(N)\n    y_values = np.random.randn(N)\n    \n    # Generate categories\n    num_categories = len(CATEGORIES)\n    \n    if N >= num_categories:\n        # Ensure each category appears at least once\n        # First, add one of each category\n        categories = list(CATEGORIES)\n        # Then fill the rest randomly\n        remaining = N - num_categories\n        categories.extend(np.random.choice(CATEGORIES, size=remaining))\n        # Shuffle to randomize order\n        np.random.shuffle(categories)\n    else:\n        # If N < num_categories, sample without replacement\n        categories = np.random.choice(CATEGORIES, size=N, replace=False)\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'x': x_values,\n        'y': y_values,\n        'category': categories\n    })\n    \n    # Create scatter plot\n    fig, ax = plt.subplots(figsize=(8, 6))\n    \n    # Create a color map for categories\n    unique_categories = df['category'].unique()\n    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_categories)))\n    color_map = dict(zip(unique_categories, colors))\n    \n    # Plot each category with its own color\n    for category in unique_categories:\n        mask = df['category'] == category\n        ax.scatter(df.loc[mask, 'x'], \n                  df.loc[mask, 'y'], \n                  label=category,\n                  color=color_map[category],\n                  alpha=0.7,\n                  s=50)\n    \n    # Customize plot\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Scatter Plot of x vs y by Category')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    return df, ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 97, in test_case_1\nAssertionError: False is not true\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 106, in test_case_2\nAssertionError: False is not true\n"
        }
      }
    ],
    "BigCodeBench/530": [
      {
        "task_id": "BigCodeBench/530",
        "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n\n    pass\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    \"\"\"\n    Identify duplicate entries in a DataFrame and record the age distribution for the duplicate names.\n    \n    Parameters:\n    df (pd.DataFrame): DataFrame with 'name' and 'age' columns\n    \n    Returns:\n    Counter: Age distribution among duplicate names\n    plt.Axes or None: Histogram plot displaying age distribution, or None if there are no duplicates\n    \n    Raises:\n    ValueError: If the DataFrame is empty or if age is negative\n    \"\"\"\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    # Check for negative ages\n    if (df['age'] < 0).any():\n        raise ValueError(\"Age cannot be negative\")\n    \n    # Round down ages to nearest integer\n    df['age'] = np.floor(df['age']).astype(int)\n    \n    # Find duplicate names\n    name_counts = df['name'].value_counts()\n    duplicate_names = name_counts[name_counts > 1].index.tolist()\n    \n    # If no duplicates, return empty Counter and None\n    if not duplicate_names:\n        return Counter(), None\n    \n    # Get ages of duplicate names\n    duplicate_ages = df[df['name'].isin(duplicate_names)]['age'].tolist()\n    \n    # Create Counter for age distribution\n    age_counter = Counter(duplicate_ages)\n    \n    # Create histogram\n    if duplicate_ages:\n        fig, ax = plt.subplots(figsize=(10, 6))\n        \n        # Calculate bins based on min and max ages, adjusted by 0.5\n        min_age = min(duplicate_ages)\n        max_age = max(duplicate_ages)\n        \n        # Create bins ensuring integer ages fall squarely within bins\n        bins = np.arange(min_age - 0.5, max_age + 1.5, 1)\n        \n        # Plot histogram\n        ax.hist(duplicate_ages, bins=bins, edgecolor='black', alpha=0.7)\n        ax.set_xlabel('Age')\n        ax.set_ylabel('Count')\n        ax.set_title('Age Distribution for Duplicate Names')\n        ax.set_xticks(range(min_age, max_age + 1))\n        ax.grid(True, alpha=0.3)\n        \n        return age_counter, ax\n    else:\n        return age_counter, None",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/532": [
      {
        "task_id": "BigCodeBench/532",
        "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=4):\n\n    pass\nimport numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, bins=4):\n    \"\"\"\n    Identify and count duplicate values in a DataFrame's 'value' column. \n    This function also plots a histogram for all values in the 'value' column \n    and overlays a normal distribution curve on the histogram.\n    \n    Parameters:\n    df (pd.DataFrame): DataFrame containing a 'value' column\n    bins (int): Number of bins for the histogram (default: 4)\n    \n    Returns:\n    tuple: A tuple containing:\n        Counter: A Counter object with the count of each duplicate value.\n        Axes: A matplotlib.axes.Axes object that represents the plot\n        of the histogram with the 'value' column data. If applicable,\n        a normal distribution curve fitted to the data is overlaid. The\n        histogram's bars are green with 60% opacity, and the normal\n        distribution curve is black with a linewidth of 2. The plot is\n        titled \"Distribution\", with \"Value\" as the x-axis label and\n        \"Frequency\" as the y-axis label.\n    \"\"\"\n    # Get the 'value' column data\n    values = df['value'].values\n    \n    # Count duplicates (values that appear more than once)\n    value_counts = Counter(values)\n    duplicates = Counter({k: v for k, v in value_counts.items() if v > 1})\n    \n    # Create the plot\n    fig, ax = plt.subplots()\n    \n    # Plot histogram\n    n, bins_edges, patches = ax.hist(values, bins=bins, density=True, \n                                     alpha=0.6, color='green', edgecolor='black')\n    \n    # Calculate mean and standard deviation for normal distribution\n    mu = np.mean(values)\n    sigma = np.std(values)\n    \n    # Create x values for the normal distribution curve\n    x = np.linspace(values.min(), values.max(), 100)\n    \n    # Plot normal distribution curve\n    ax.plot(x, norm.pdf(x, mu, sigma), 'k-', linewidth=2)\n    \n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution')\n    \n    return duplicates, ax",
        "status": "fail",
        "details": {
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 89, in test_case_3\n  File \"__test__.py\", line 53, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/numpy/core/_methods.py\", line 44, in _amin\n    return umr_minimum(a, axis, None, out, keepdims, initial, where)\nValueError: zero-size array to reduction operation minimum which has no identity\n"
        }
      }
    ],
    "BigCodeBench/15": [
      {
        "task_id": "BigCodeBench/15",
        "solution": "import subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n\n    pass\nimport subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n    \"\"\"\n    Execute a list of shell commands read from a CSV file and save the outputs in separate files.\n    \n    Args:\n        commands_file_path (str): Path to the CSV file containing shell commands\n        output_dir_path (str): Path to the directory where output files will be saved\n    \n    Returns:\n        list of str: A list of paths to the output files created in the output directory\n    \n    Raises:\n        FileNotFoundError: If the commands_file_path does not exist\n    \"\"\"\n    # Check if the commands file exists\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist\")\n    \n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir_path, exist_ok=True)\n    \n    output_files = []\n    \n    # Read commands from CSV file\n    with open(commands_file_path, 'r', newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        \n        for index, row in enumerate(reader):\n            if not row:  # Skip empty rows\n                continue\n                \n            # Get the command (assuming it's in the first column)\n            command = row[0].strip()\n            \n            # Create output file path\n            output_filename = f'command_{index}_output.txt'\n            output_file_path = os.path.join(output_dir_path, output_filename)\n            output_files.append(output_file_path)\n            \n            try:\n                # Execute the command\n                result = subprocess.run(\n                    command,\n                    shell=True,\n                    capture_output=True,\n                    text=True,\n                    check=False  # Don't raise exception on non-zero exit code\n                )\n                \n                # Write output to file\n                with open(output_file_path, 'w') as outfile:\n                    if result.returncode == 0:\n                        # Command succeeded - write stdout\n                        outfile.write(result.stdout)\n                    else:\n                        # Command failed - write error message and exit code\n                        error_msg = f\"Command failed with exit code {result.returncode}\\n\"\n                        error_msg += f\"Command: {command}\\n\"\n                        error_msg += f\"Error output:\\n{result.stderr}\"\n                        if result.stdout:\n                            error_msg += f\"\\nStandard output:\\n{result.stdout}\"\n                        outfile.write(error_msg)\n                        \n            except Exception as e:\n                # Handle any other exceptions during command execution\n                with open(output_file_path, 'w') as outfile:\n                    error_msg = f\"Error executing command: {command}\\n\"\n                    error_msg += f\"Exception: {type(e).__name__}: {str(e)}\"\n                    outfile.write(error_msg)\n    \n    return output_files",
        "status": "fail",
        "details": {
          "test_command_failure_with_specific_exit_code": "Traceback (most recent call last):\n  File \"__test__.py\", line 154, in test_command_failure_with_specific_exit_code\nAssertionError: 'Error executing command' not found in 'Command failed with exit code 1\\nCommand: exit 1\\nError output:\\n'\n"
        }
      }
    ],
    "BigCodeBench/486": [
      {
        "task_id": "BigCodeBench/486",
        "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\ndef task_func(start_time, end_time, step, trend, seed=42):\n\n    pass\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, trend, seed=42):\n    \"\"\"\n    Generate a time series from a given epoch start time to end time with a specified step and trend.\n    The time series is plotted with timestamps on the x-axis ('Time') and values on the y-axis ('Value').\n    The values are generated from a normal distribution, and a linear trend is added based on the provided trend value.\n    \n    Parameters:\n    - start_time (int): Epoch timestamp for the start time\n    - end_time (int): Epoch timestamp for the end time\n    - step (int): Time step in seconds between consecutive points\n    - trend (float): Linear trend coefficient to be added to the values\n    - seed (int): Random seed for reproducibility (default: 42)\n    \n    Returns:\n    - ax (matplotlib.pyplot.Axes): The Axes object of the generated plot, with the x-axis labeled 'Time' and y-axis labeled 'Value'.\n    \"\"\"\n    # Set random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate timestamps\n    timestamps = list(range(start_time, end_time + 1, step))\n    \n    # Convert epoch timestamps to datetime objects\n    datetime_objects = [datetime.fromtimestamp(ts) for ts in timestamps]\n    \n    # Generate random values from normal distribution\n    n_points = len(timestamps)\n    random_values = np.random.normal(loc=0, scale=1, size=n_points)\n    \n    # Add linear trend\n    trend_values = np.arange(n_points) * trend\n    values = random_values + trend_values\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'Time': datetime_objects,\n        'Value': values\n    })\n    \n    # Create plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['Time'], df['Value'], marker='o', linestyle='-', markersize=4)\n    \n    # Set labels\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    ax.set_title('Time Series with Linear Trend')\n    \n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45)\n    \n    # Adjust layout to prevent label cutoff\n    plt.tight_layout()\n    \n    return ax",
        "status": "fail",
        "details": {
          "test_case_6": "Traceback (most recent call last):\n  File \"__test__.py\", line 133, in test_case_6\nAssertionError: Exception not raised\n",
          "test_case_8": "Traceback (most recent call last):\n  File \"__test__.py\", line 143, in test_case_8\nAssertionError: False is not true\n"
        }
      }
    ],
    "BigCodeBench/424": [
      {
        "task_id": "BigCodeBench/424",
        "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n\n    pass\nimport cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    \"\"\"\n    Reads an RGB image, applies K-means clustering to segment the image into 'n_clusters' regions,\n    and saves each region as a separate image. The function returns numpy arrays of the original\n    and segmented images.\n    \n    Parameters:\n    - image_path (str): Path to the input image file\n    - n_clusters (int): Number of clusters for K-means segmentation\n    - random_seed (int): Random seed for reproducibility\n    \n    Returns:\n    - tuple: A tuple containing two numpy arrays. The first array represents the original RGB image,\n             and the second array represents the segmented image, with each pixel's color replaced by\n             the centroid of the cluster it belongs to.\n    \n    Raises:\n    - FileNotFoundError: If the image file does not exist at the specified path\n    - ValueError: If 'n_clusters' is not a positive integer\n    \"\"\"\n    \n    # Validate n_clusters\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be a positive integer\")\n    \n    # Check if file exists\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"Image file not found at path: {image_path}\")\n    \n    # Read the image in RGB format (OpenCV reads in BGR by default)\n    image_bgr = cv2.imread(image_path)\n    if image_bgr is None:\n        raise FileNotFoundError(f\"Unable to read image file at path: {image_path}\")\n    \n    # Convert BGR to RGB\n    original_image = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n    \n    # Get image dimensions\n    height, width, channels = original_image.shape\n    \n    # Reshape image to be a list of pixels\n    pixels = original_image.reshape(-1, 3)\n    \n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed, n_init=10)\n    kmeans.fit(pixels)\n    \n    # Get cluster labels for each pixel\n    labels = kmeans.labels_\n    \n    # Get cluster centers (centroids)\n    centers = kmeans.cluster_centers_.astype(np.uint8)\n    \n    # Create segmented image by replacing each pixel with its cluster center\n    segmented_pixels = centers[labels]\n    segmented_image = segmented_pixels.reshape(height, width, channels)\n    \n    # Save each region as a separate image\n    base_name = os.path.splitext(os.path.basename(image_path))[0]\n    output_dir = os.path.dirname(image_path)\n    if not output_dir:\n        output_dir = '.'\n    \n    for cluster_id in range(n_clusters):\n        # Create a mask for the current cluster\n        mask = (labels.reshape(height, width) == cluster_id).astype(np.uint8) * 255\n        \n        # Create an image showing only the current cluster region\n        cluster_region = np.zeros_like(original_image)\n        cluster_mask_3d = np.stack([mask, mask, mask], axis=2)\n        cluster_region = np.where(cluster_mask_3d > 0, original_image, 0)\n        \n        # Save the cluster region\n        output_path = os.path.join(output_dir, f\"{base_name}_cluster_{cluster_id}.jpg\")\n        # Convert RGB back to BGR for saving with OpenCV\n        cluster_region_bgr = cv2.cvtColor(cluster_region, cv2.COLOR_RGB2BGR)\n        cv2.imwrite(output_path, cluster_region_bgr)\n    \n    return original_image, segmented_image",
        "status": "fail",
        "details": {
          "test_normal_functionality": "Traceback (most recent call last):\n  File \"__test__.py\", line 125, in test_normal_functionality\nAssertionError: False is not true : The arrays should be equal\n",
          "test_single_cluster_returns_original_image": "Traceback (most recent call last):\n  File \"__test__.py\", line 150, in test_single_cluster_returns_original_image\n  File \"/usr/local/lib/python3.10/site-packages/numpy/testing/_private/utils.py\", line 934, in assert_array_equal\n    assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,\n  File \"/usr/local/lib/python3.10/site-packages/numpy/testing/_private/utils.py\", line 844, in assert_array_compare\n    raise AssertionError(msg)\nAssertionError: \nArrays are not equal\nThe original and segmented images should be identical when n_clusters is set to 1.\nMismatched elements: 296 / 300 (98.7%)\nMax absolute difference: 255\nMax relative difference: 1.04508197\n x: array([[[255, 252, 253],\n        [251, 246, 243],\n        [255, 254, 248],...\n y: array([[[247, 247, 244],\n        [247, 247, 244],\n        [247, 247, 244],...\n"
        }
      }
    ],
    "BigCodeBench/636": [
      {
        "task_id": "BigCodeBench/636",
        "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(rows):\n\n    pass\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(rows):\n    \"\"\"\n    Create a Pandas DataFrame with random integer values between 0 and 9 for a given number of rows.\n    Count the non-zero values in each column and visualize this information using a bar plot.\n    \n    Parameters:\n    rows (int): Number of rows for the DataFrame\n    \n    Returns:\n    tuple: A tuple containing the following elements:\n        DataFrame: The generated DataFrame with random integer values.\n        Axes: The matplotlib Axes object containing the bar plot.\n    \"\"\"\n    # Create DataFrame with random integers between 0 and 9\n    data = np.random.randint(0, 10, size=(rows, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Count non-zero values in each column\n    non_zero_counts = (df != 0).sum()\n    \n    # Create bar plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    bars = ax.bar(non_zero_counts.index, non_zero_counts.values)\n    \n    # Customize the plot\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Count of Non-Zero Values')\n    ax.set_title(f'Non-Zero Value Counts by Column (Total Rows: {rows})')\n    ax.set_ylim(0, rows * 1.1)  # Set y-axis limit slightly above max possible value\n    \n    # Add value labels on top of bars\n    for bar in bars:\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height,\n                f'{int(height)}',\n                ha='center', va='bottom')\n    \n    # Add grid for better readability\n    ax.grid(True, axis='y', alpha=0.3)\n    \n    # Tight layout to prevent label cutoff\n    plt.tight_layout()\n    \n    return df, ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 64, in test_case_1\nAssertionError: 5 != 0\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 77, in test_case_4\n  File \"__test__.py\", line 27, in task_func\n  File \"mtrand.pyx\", line 748, in numpy.random.mtrand.RandomState.randint\n  File \"_bounded_integers.pyx\", line 1256, in numpy.random._bounded_integers._rand_int64\nValueError: negative dimensions are not allowed\n"
        }
      }
    ],
    "BigCodeBench/720": [
      {
        "task_id": "BigCodeBench/720",
        "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\ndef task_func():\n\n    pass\nimport os\nimport csv\nimport random\nfrom datetime import datetime\ndef task_func():\n    \"\"\"\n    Create and delete a CSV file \"task_func_data/Output.txt\" with sensor data for temperature and humidity.\n    The data is generated randomly, written in append mode, and the file is deleted after use.\n    \n    Returns:\n        str: Returns the path to the CSV file \"task_func_data/Output.txt\" before deletion.\n    \"\"\"\n    # Define the directory and file path\n    directory = \"task_func_data\"\n    filename = \"Output.txt\"\n    filepath = os.path.join(directory, filename)\n    \n    # Create directory if it doesn't exist\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    \n    # Generate random sensor data\n    num_records = 10  # Number of records to generate\n    \n    # Open file in append mode and write CSV data\n    with open(filepath, 'a', newline='') as csvfile:\n        csv_writer = csv.writer(csvfile)\n        \n        # Write header if file is empty\n        if os.path.getsize(filepath) == 0:\n            csv_writer.writerow(['Timestamp', 'Temperature (\u00b0C)', 'Humidity (%)'])\n        \n        # Generate and write random sensor data\n        for _ in range(num_records):\n            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            temperature = round(random.uniform(15.0, 35.0), 2)  # Random temperature between 15-35\u00b0C\n            humidity = round(random.uniform(30.0, 80.0), 2)     # Random humidity between 30-80%\n            \n            csv_writer.writerow([timestamp, temperature, humidity])\n    \n    # Store the filepath to return before deletion\n    return_path = filepath\n    \n    # Delete the file\n    if os.path.exists(filepath):\n        os.remove(filepath)\n    \n    # Remove the directory if it's empty\n    if os.path.exists(directory) and not os.listdir(directory):\n        os.rmdir(directory)\n    \n    return return_path",
        "status": "fail",
        "details": {
          "test_file_existence": "Traceback (most recent call last):\n  File \"__test__.py\", line 75, in tearDown\nFileNotFoundError: [Errno 2] No such file or directory: 'task_func_data/Output.txt'\n",
          "test_data_appending": "Traceback (most recent call last):\n  File \"__test__.py\", line 75, in tearDown\nFileNotFoundError: [Errno 2] No such file or directory: 'task_func_data/Output.txt'\n",
          "test_file_content": "Traceback (most recent call last):\n  File \"__test__.py\", line 75, in tearDown\nFileNotFoundError: [Errno 2] No such file or directory: 'task_func_data/Output.txt'\n",
          "test_headers_only_once": "Traceback (most recent call last):\n  File \"__test__.py\", line 75, in tearDown\nFileNotFoundError: [Errno 2] No such file or directory: 'task_func_data/Output.txt'\n",
          "test_return_value": "Traceback (most recent call last):\n  File \"__test__.py\", line 75, in tearDown\nFileNotFoundError: [Errno 2] No such file or directory: 'task_func_data/Output.txt'\n"
        }
      }
    ],
    "BigCodeBench/678": [
      {
        "task_id": "BigCodeBench/678",
        "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\ndef task_func(path):\n\n    pass\nimport pandas as pd\nimport json\nimport os\nimport shutil\ndef task_func(path):\n    \"\"\"\n    Processes JSON files in a directory. The function reads each JSON file alphabetically \n    into a DataFrame and inserts a \"Source\" column that specifies the filename. \n    The processed files are then moved to a \"processed\" subdirectory.\n    \n    Args:\n        path (str): Path to the directory containing JSON files\n        \n    Returns:\n        df (pandas.DataFrame): A DataFrame containing the data from all processed files.\n    \"\"\"\n    # Initialize empty list to store dataframes\n    dfs = []\n    \n    # Check if path exists, if not return empty DataFrame\n    if not os.path.exists(path):\n        return pd.DataFrame()\n    \n    # Get all JSON files in the directory\n    json_files = [f for f in os.listdir(path) if f.endswith('.json') and os.path.isfile(os.path.join(path, f))]\n    \n    # Sort files alphabetically\n    json_files.sort()\n    \n    # If no JSON files found, return empty DataFrame\n    if not json_files:\n        return pd.DataFrame()\n    \n    # Create processed subdirectory if it doesn't exist\n    processed_dir = os.path.join(path, 'processed')\n    if not os.path.exists(processed_dir):\n        os.makedirs(processed_dir)\n    \n    # Process each JSON file\n    for filename in json_files:\n        file_path = os.path.join(path, filename)\n        \n        try:\n            # Read JSON file\n            with open(file_path, 'r') as f:\n                data = json.load(f)\n            \n            # Convert to DataFrame\n            if isinstance(data, list):\n                df = pd.DataFrame(data)\n            elif isinstance(data, dict):\n                # If it's a single dict, wrap it in a list\n                df = pd.DataFrame([data])\n            else:\n                # Skip if data format is not supported\n                continue\n            \n            # Add Source column with filename\n            df['Source'] = filename\n            \n            # Append to list of dataframes\n            dfs.append(df)\n            \n            # Move file to processed directory\n            shutil.move(file_path, os.path.join(processed_dir, filename))\n            \n        except Exception as e:\n            # Skip files that can't be processed\n            print(f\"Error processing {filename}: {str(e)}\")\n            continue\n    \n    # Combine all dataframes\n    if dfs:\n        result_df = pd.concat(dfs, ignore_index=True)\n    else:\n        result_df = pd.DataFrame()\n    \n    return result_df",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/760": [
      {
        "task_id": "BigCodeBench/760",
        "solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n\n    pass\nimport pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    \n    # Set random seed if provided\n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n    \n    # Combine all names\n    all_names = latin_names + other_names\n    \n    # Generate data for 100 records\n    records = []\n    \n    for i in range(1, 101):\n        # Generate ID\n        record_id = i\n        \n        # Randomly select a name\n        name = np.random.choice(all_names)\n        \n        # Fix improperly encoded Latin characters if any\n        # This handles cases where names might have encoding issues\n        try:\n            # Try to encode and decode to fix potential encoding issues\n            name = name.encode('utf-8').decode('utf-8')\n        except:\n            pass\n        \n        # Generate random date of birth\n        # Calculate days range\n        start_date = datetime.datetime(start_year, 1, 1)\n        end_date = datetime.datetime(end_year, 12, 31)\n        \n        # Generate random date between start and end\n        time_between_dates = end_date - start_date\n        days_between_dates = time_between_dates.days\n        random_number_of_days = np.random.randint(0, days_between_dates + 1)\n        random_date = start_date + datetime.timedelta(days=random_number_of_days)\n        \n        # Extract year from date of birth\n        birth_year = random_date.year\n        \n        # Create email using lowercase name, birth year, and domain\n        email = f\"{name.lower()}{birth_year}@{email_domain}\"\n        \n        # Add record to list\n        records.append({\n            'ID': record_id,\n            'Name': name,\n            'Date of Birth': random_date,\n            'Email': email\n        })\n    \n    # Create DataFrame\n    df = pd.DataFrame(records)\n    \n    return df",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/723": [
      {
        "task_id": "BigCodeBench/723",
        "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n\n    pass\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    \"\"\"\n    Scrape data from a given URL and save the scraped data to a CSV file.\n    \n    Args:\n        url (str): The URL to scrape data from\n        \n    Returns:\n        CSV_FILE_PATH (str): The path of the CSV file where the scraped data is saved.\n    \"\"\"\n    try:\n        # Send HTTP request and get the response\n        with urllib.request.urlopen(url) as response:\n            html_content = response.read()\n        \n        # Parse the HTML content\n        soup = BeautifulSoup(html_content, 'html.parser')\n        \n        # Extract all text content, links, and basic metadata\n        data_to_save = []\n        \n        # Extract page title\n        title = soup.find('title')\n        if title:\n            data_to_save.append(['Title', title.get_text().strip()])\n        \n        # Extract meta description\n        meta_desc = soup.find('meta', attrs={'name': 'description'})\n        if meta_desc:\n            data_to_save.append(['Meta Description', meta_desc.get('content', '').strip()])\n        \n        # Extract all headings\n        for i in range(1, 7):\n            headings = soup.find_all(f'h{i}')\n            for idx, heading in enumerate(headings):\n                data_to_save.append([f'H{i}_{idx+1}', heading.get_text().strip()])\n        \n        # Extract all paragraphs\n        paragraphs = soup.find_all('p')\n        for idx, para in enumerate(paragraphs[:20]):  # Limit to first 20 paragraphs\n            text = para.get_text().strip()\n            if text:  # Only save non-empty paragraphs\n                data_to_save.append([f'Paragraph_{idx+1}', text])\n        \n        # Extract all links\n        links = soup.find_all('a', href=True)\n        for idx, link in enumerate(links[:20]):  # Limit to first 20 links\n            link_text = link.get_text().strip()\n            link_url = link['href']\n            if link_text:  # Only save links with text\n                data_to_save.append([f'Link_{idx+1}', f'{link_text} ({link_url})'])\n        \n        # Extract all images\n        images = soup.find_all('img', src=True)\n        for idx, img in enumerate(images[:10]):  # Limit to first 10 images\n            img_src = img['src']\n            img_alt = img.get('alt', 'No alt text')\n            data_to_save.append([f'Image_{idx+1}', f'{img_alt} ({img_src})'])\n        \n        # If no specific data found, extract all text\n        if not data_to_save:\n            all_text = soup.get_text()\n            # Clean up the text\n            lines = [line.strip() for line in all_text.splitlines() if line.strip()]\n            for idx, line in enumerate(lines[:50]):  # Limit to first 50 lines\n                data_to_save.append([f'Text_Line_{idx+1}', line])\n        \n        # Write data to CSV file\n        with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n            csv_writer = csv.writer(csvfile)\n            \n            # Write header\n            csv_writer.writerow(['Type', 'Content'])\n            \n            # Write data\n            csv_writer.writerows(data_to_save)\n        \n        return CSV_FILE_PATH\n        \n    except urllib.error.URLError as e:\n        print(f\"Error accessing URL: {e}\")\n        # Create empty CSV file in case of error\n        with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n            csv_writer = csv.writer(csvfile)\n            csv_writer.writerow(['Error', f'Failed to access URL: {str(e)}'])\n        return CSV_FILE_PATH\n        \n    except Exception as e:\n        print(f\"Error scraping data: {e}\")\n        # Create CSV file with error message\n        with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n            csv_writer = csv.writer(csvfile)\n            csv_writer.writerow(['Error', f'Failed to scrape data: {str(e)}'])\n        return CSV_FILE_PATH",
        "status": "fail",
        "details": {
          "test_correct_scraping": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 127, in test_correct_scraping\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 898, in assert_called\n    raise AssertionError(msg)\nAssertionError: Expected 'writerows' to have been called.\n",
          "test_invalid_url": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 130, in test_invalid_url\nAssertionError: Exception not raised\n",
          "test_no_table": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 146, in test_no_table\nAssertionError: Exception not raised\n",
          "test_overwrite_existing_csv": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 161, in test_overwrite_existing_csv\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 941, in assert_called_once_with\n    return self.assert_called_with(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 929, in assert_called_with\n    raise AssertionError(_error_message()) from cause\nAssertionError: expected call not found.\nExpected: open('scraped_data.csv', 'w')\nActual: open('scraped_data.csv', 'w', newline='', encoding='utf-8')\n"
        }
      }
    ],
    "BigCodeBench/800": [
      {
        "task_id": "BigCodeBench/800",
        "solution": "import csv\nimport os\nfrom collections import Counter\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n\n    pass\nimport csv\nimport os\nfrom collections import Counter\nCSV_FILE_PATH = 'match_data.csv'\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    \"\"\"\n    Count the total number of goals and penalties from a CSV file and update it with the given goals and penalties.\n    \n    Args:\n        goals (int): Number of goals to add to the total\n        penalties (int): Number of penalties to add to the total\n        csv_file_path (str): Path to the CSV file containing match data\n    \n    Returns:\n        count (Counter): A Counter object with total counts of goals and penalties\n    \"\"\"\n    # Initialize counter\n    count = Counter()\n    \n    # Check if CSV file exists\n    if os.path.exists(csv_file_path):\n        try:\n            with open(csv_file_path, 'r', newline='') as file:\n                reader = csv.DictReader(file)\n                \n                # Sum up goals and penalties from CSV\n                for row in reader:\n                    if 'goals' in row:\n                        count['goals'] += int(row['goals'])\n                    if 'penalties' in row:\n                        count['penalties'] += int(row['penalties'])\n        except Exception as e:\n            print(f\"Error reading CSV file: {e}\")\n    \n    # Add the provided goals and penalties\n    count['goals'] += goals\n    count['penalties'] += penalties\n    \n    return count",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 75, in test_case_1\n  File \"__test__.py\", line 57, in task_func\nTypeError: unsupported operand type(s) for +=: 'int' and 'dict'\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 85, in test_case_2\n  File \"__test__.py\", line 57, in task_func\nTypeError: unsupported operand type(s) for +=: 'int' and 'dict'\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 95, in test_case_3\n  File \"__test__.py\", line 57, in task_func\nTypeError: unsupported operand type(s) for +=: 'int' and 'dict'\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 106, in test_case_4\n  File \"__test__.py\", line 57, in task_func\nTypeError: unsupported operand type(s) for +=: 'int' and 'dict'\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 116, in test_case_5\n  File \"__test__.py\", line 57, in task_func\nTypeError: unsupported operand type(s) for +=: 'int' and 'dict'\n"
        }
      }
    ],
    "BigCodeBench/854": [
      {
        "task_id": "BigCodeBench/854",
        "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\ndef task_func(numbers):\n\n    pass\nfrom functools import reduce\nfrom itertools import permutations\nimport math\ndef task_func(numbers):\n    \"\"\"\n    Generate all permutations of a given list of numbers and calculate the sum \n    of the factorials of each number in each permutation.\n    \n    Args:\n        numbers: A list of non-negative integers\n        \n    Returns:\n        list of int: A list containing the sums of the factorials of each number\n                     in each permutation.\n        list of list of int: A list containing all permutations of numbers.\n        \n    Raises:\n        TypeError: If numbers is not a list of integers.\n        ValueError: If input numbers are negative.\n    \"\"\"\n    # Check if input is a list\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input must be a list of integers\")\n    \n    # Check if all elements are integers and non-negative\n    for num in numbers:\n        if not isinstance(num, int):\n            raise TypeError(\"All elements must be integers\")\n        if num < 0:\n            raise ValueError(\"All numbers must be non-negative\")\n    \n    # Handle empty list case\n    if not numbers:\n        return [], []\n    \n    # Generate all permutations\n    perms = list(permutations(numbers))\n    \n    # Calculate sum of factorials for each permutation\n    factorial_sums = []\n    for perm in perms:\n        # Calculate factorial for each number in the permutation\n        factorials = [math.factorial(num) for num in perm]\n        # Sum the factorials\n        factorial_sum = sum(factorials)\n        factorial_sums.append(factorial_sum)\n    \n    return factorial_sums, perms",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/845": [
      {
        "task_id": "BigCodeBench/845",
        "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(text1, text2):\n\n    pass\nimport re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(text1, text2):\n    \"\"\"\n    Calculate the similarity values between two texts based on the cosine similarity \n    and the Levenshtein ratio.\n    \n    Args:\n        text1 (str): First text to compare\n        text2 (str): Second text to compare\n    \n    Returns:\n        tuple: A tuple containing the cosine similarity and Levenshtein ratio as floats.\n        cosine similarity (float): The cosine similarity ranges from 0 to 1,\n        where 1 means identical term frequency, and 0 indicates no common terms.\n        levenshtein_ratio (float): The Levenshtein ratio also ranges from 0 to 1,\n        where 1 means the strings are identical, and 0 means they are completely different.\n    \"\"\"\n    # Clean texts by removing non-alphanumeric characters except spaces and convert to lowercase\n    cleaned_text1 = ALPHANUMERIC.sub(' ', text1).lower().strip()\n    cleaned_text2 = ALPHANUMERIC.sub(' ', text2).lower().strip()\n    \n    # Split texts into words for cosine similarity calculation\n    words1 = cleaned_text1.split()\n    words2 = cleaned_text2.split()\n    \n    # Calculate cosine similarity based on term frequency\n    if not words1 or not words2:\n        cosine_sim = 0.0\n    else:\n        # Count term frequencies\n        counter1 = Counter(words1)\n        counter2 = Counter(words2)\n        \n        # Get all unique terms\n        all_terms = set(counter1.keys()) | set(counter2.keys())\n        \n        # Create term frequency vectors\n        vector1 = np.array([counter1.get(term, 0) for term in all_terms])\n        vector2 = np.array([counter2.get(term, 0) for term in all_terms])\n        \n        # Calculate cosine similarity\n        dot_product = np.dot(vector1, vector2)\n        norm1 = np.linalg.norm(vector1)\n        norm2 = np.linalg.norm(vector2)\n        \n        if norm1 == 0 or norm2 == 0:\n            cosine_sim = 0.0\n        else:\n            cosine_sim = dot_product / (norm1 * norm2)\n    \n    # Calculate Levenshtein ratio\n    levenshtein_ratio = ratio(cleaned_text1, cleaned_text2)\n    \n    return (float(cosine_sim), float(levenshtein_ratio))",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/765": [
      {
        "task_id": "BigCodeBench/765",
        "solution": "import os\nfrom pathlib import Path\nimport shutil\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n\n    pass\nimport os\nfrom pathlib import Path\nimport shutil\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    \"\"\"\n    Process files from a dictionary by checking if the file exists, \n    and if it has content, then copies it to a target directory.\n    \n    Args:\n        kwargs (dict): Dictionary where keys are identifiers and values are file paths\n        target_dir (str): Target directory where files will be copied (default: \"non_none_files\")\n    \n    Returns:\n        copied_files (list): A list of full file paths that were copied.\n    \"\"\"\n    copied_files = []\n    \n    # Create target directory if it doesn't exist\n    target_path = Path(target_dir)\n    target_path.mkdir(parents=True, exist_ok=True)\n    \n    # Process each file in the dictionary\n    for key, file_path in kwargs.items():\n        if file_path is None:\n            continue\n            \n        # Convert to Path object for easier handling\n        source_path = Path(file_path)\n        \n        # Check if file exists\n        if not source_path.exists():\n            print(f\"File '{file_path}' does not exist. Skipping.\")\n            continue\n            \n        # Check if it's actually a file (not a directory)\n        if not source_path.is_file():\n            print(f\"'{file_path}' is not a file. Skipping.\")\n            continue\n            \n        # Check if file has content (size > 0)\n        if source_path.stat().st_size == 0:\n            print(f\"File '{file_path}' is empty. Skipping.\")\n            continue\n            \n        # Copy the file to target directory\n        try:\n            # Create destination path with the same filename\n            dest_path = target_path / source_path.name\n            \n            # Handle case where file with same name already exists\n            if dest_path.exists():\n                # Add a suffix to make it unique\n                base_name = source_path.stem\n                extension = source_path.suffix\n                counter = 1\n                while dest_path.exists():\n                    dest_path = target_path / f\"{base_name}_{counter}{extension}\"\n                    counter += 1\n            \n            # Copy the file\n            shutil.copy2(file_path, dest_path)\n            \n            # Add the full destination path to the list\n            copied_files.append(str(dest_path.absolute()))\n            print(f\"Copied '{file_path}' to '{dest_path}'\")\n            \n        except Exception as e:\n            print(f\"Error copying '{file_path}': {e}\")\n    \n    return copied_files",
        "status": "fail",
        "details": {
          "test_files_with_content": "Traceback (most recent call last):\n  File \"__test__.py\", line 98, in test_files_with_content\nAssertionError: 0 != 2\n",
          "test_files_with_no_content": "Traceback (most recent call last):\n  File \"__test__.py\", line 110, in test_files_with_no_content\nAssertionError: 0 != 1\n",
          "test_mixed_case": "Traceback (most recent call last):\n  File \"__test__.py\", line 126, in test_mixed_case\nAssertionError: 0 != 2\n"
        }
      }
    ],
    "BigCodeBench/771": [
      {
        "task_id": "BigCodeBench/771",
        "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n\n    pass\nimport re\nimport os\nfrom pathlib import Path\nimport csv\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    \"\"\"\n    Processes CSV files in a directory based on a specified pattern and creates new files \n    with altered names while preserving the content.\n    \n    Args:\n        directory (str): The directory path containing CSV files\n        pattern (str): Regular expression pattern to match CSV files\n        \n    Returns:\n        new_files (list): A list of strings, where each string is the filename of a new CSV file created by the function.\n    \"\"\"\n    new_files = []\n    \n    # Ensure directory exists\n    if not os.path.exists(directory):\n        return new_files\n    \n    # Compile the pattern\n    regex = re.compile(pattern)\n    \n    # Process each file in the directory\n    for filename in os.listdir(directory):\n        # Check if file matches the pattern\n        match = regex.match(filename)\n        if match:\n            # Extract the base name from the pattern\n            base_name = match.group(1)\n            \n            # Create new filename (e.g., \"name-123.csv\" -> \"name_processed.csv\")\n            new_filename = f\"{base_name}_processed.csv\"\n            \n            # Full paths\n            old_filepath = os.path.join(directory, filename)\n            new_filepath = os.path.join(directory, new_filename)\n            \n            # Read and write CSV content\n            try:\n                # Read the original CSV file\n                with open(old_filepath, 'r', newline='', encoding='utf-8') as infile:\n                    reader = csv.reader(infile)\n                    rows = list(reader)\n                \n                # Write to new CSV file\n                with open(new_filepath, 'w', newline='', encoding='utf-8') as outfile:\n                    writer = csv.writer(outfile)\n                    writer.writerows(rows)\n                \n                # Add to list of new files\n                new_files.append(new_filename)\n                \n            except Exception as e:\n                # Handle any errors during file processing\n                print(f\"Error processing {filename}: {str(e)}\")\n                continue\n    \n    return new_files",
        "status": "fail",
        "details": {
          "test_exception_handling": "Traceback (most recent call last):\n  File \"__test__.py\", line 118, in test_exception_handling\nAssertionError: OSError not raised\n",
          "test_file_creation_and_content": "Traceback (most recent call last):\n  File \"__test__.py\", line 87, in test_file_creation_and_content\nAssertionError: Lists differ: [] != ['Shan.csv', 'Shannon.csv']\n\nSecond list contains 2 additional elements.\nFirst extra element 0:\n'Shan.csv'\n\n- []\n+ ['Shan.csv', 'Shannon.csv']\n",
          "test_mixed_file_types": "Traceback (most recent call last):\n  File \"__test__.py\", line 111, in test_mixed_file_types\nAssertionError: Lists differ: [] != ['Shan.csv']\n\nSecond list contains 1 additional elements.\nFirst extra element 0:\n'Shan.csv'\n\n- []\n+ ['Shan.csv']\n"
        }
      }
    ],
    "BigCodeBench/857": [
      {
        "task_id": "BigCodeBench/857",
        "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n\n    pass\nimport warnings\nimport os\nimport glob\nimport shutil\nimport time\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    Transfer files from one directory (SOURCE_DIR) to another (DEST_DIR) based on the specified file extensions (EXTENSIONS).\n    It also issues warnings for files that could not be transferred due to any issues.\n    \n    Args:\n        SOURCE_DIR (str): Path to the source directory\n        DEST_DIR (str): Path to the destination directory\n        EXTENSIONS (list): List of file extensions to transfer (e.g., ['.txt', '.pdf'])\n    \n    Returns:\n        transferred_files (list): A list containing the names of files that were successfully transferred.\n    \"\"\"\n    transferred_files = []\n    \n    # Check if source directory exists\n    if not os.path.exists(SOURCE_DIR):\n        warnings.warn(f\"Source directory '{SOURCE_DIR}' does not exist.\")\n        return transferred_files\n    \n    # Create destination directory if it doesn't exist\n    if not os.path.exists(DEST_DIR):\n        try:\n            os.makedirs(DEST_DIR)\n        except Exception as e:\n            warnings.warn(f\"Could not create destination directory '{DEST_DIR}': {str(e)}\")\n            return transferred_files\n    \n    # Process each extension\n    for ext in EXTENSIONS:\n        # Ensure extension starts with a dot\n        if not ext.startswith('.'):\n            ext = '.' + ext\n        \n        # Find all files with the current extension\n        pattern = os.path.join(SOURCE_DIR, f'*{ext}')\n        files = glob.glob(pattern)\n        \n        for file_path in files:\n            if os.path.isfile(file_path):\n                filename = os.path.basename(file_path)\n                dest_path = os.path.join(DEST_DIR, filename)\n                \n                try:\n                    # Check if file already exists in destination\n                    if os.path.exists(dest_path):\n                        # Generate unique filename with timestamp\n                        base_name, extension = os.path.splitext(filename)\n                        timestamp = str(int(time.time()))\n                        new_filename = f\"{base_name}_{timestamp}{extension}\"\n                        dest_path = os.path.join(DEST_DIR, new_filename)\n                        warnings.warn(f\"File '{filename}' already exists in destination. Renaming to '{new_filename}'\")\n                    \n                    # Copy the file\n                    shutil.copy2(file_path, dest_path)\n                    transferred_files.append(filename)\n                    \n                except PermissionError:\n                    warnings.warn(f\"Permission denied: Could not transfer '{filename}'\")\n                except IOError as e:\n                    warnings.warn(f\"IO Error: Could not transfer '{filename}': {str(e)}\")\n                except Exception as e:\n                    warnings.warn(f\"Unexpected error: Could not transfer '{filename}': {str(e)}\")\n    \n    return transferred_files",
        "status": "fail",
        "details": {
          "test_all_extensions": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 134, in test_all_extensions\nAssertionError: Lists differ: ['fil[42 chars].jpg', 'file1.txt', 'file2.csv', 'file3.xlsx',[114 chars]jpg'] != ['fil[42 chars].jpg']\n\nFirst list contains 12 additional elements.\nFirst extra element 4:\n'file1.txt'\n\n+ ['file1.txt', 'file2.csv', 'file3.xlsx', 'file4.jpg']\n- ['file1.txt',\n-  'file2.csv',\n-  'file3.xlsx',\n-  'file4.jpg',\n-  'file1.txt',\n-  'file2.csv',\n-  'file3.xlsx',\n-  'file4.jpg',\n-  'file1.txt',\n-  'file2.csv',\n-  'file3.xlsx',\n-  'file4.jpg',\n-  'file1.txt',\n-  'file2.csv',\n-  'file3.xlsx',\n-  'file4.jpg']\n",
          "test_exception_handling": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 127, in test_exception_handling\nAssertionError: Lists differ: ['invalid file name.jpg'] != []\n\nFirst list contains 1 additional elements.\nFirst extra element 0:\n'invalid file name.jpg'\n\n- ['invalid file name.jpg']\n+ []\n",
          "test_successful_transfer": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 106, in test_successful_transfer\nAssertionError: Lists differ: ['file1.txt', 'file2.csv', 'file1.txt', 'file2.csv'] != ['file1.txt', 'file2.csv']\n\nFirst list contains 2 additional elements.\nFirst extra element 2:\n'file1.txt'\n\n- ['file1.txt', 'file2.csv', 'file1.txt', 'file2.csv']\n+ ['file1.txt', 'file2.csv']\n"
        }
      }
    ],
    "BigCodeBench/763": [
      {
        "task_id": "BigCodeBench/763",
        "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n# Constants\ndef task_func(input_file, output_file):\n\n    pass\nimport numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\ndef task_func(input_file, output_file):\n    \"\"\"\n    Read a list of dictionaries from a JSON file, calculate the mean and median for each key,\n    and write the results to a CSV file.\n    \n    Args:\n        input_file (str): Path to the input JSON file\n        output_file (str): Path to the output CSV file\n    \n    Returns:\n        dict: A dictionary where each key is a field from the input JSON and each value is \n              another dictionary with the mean and median of that field.\n    \"\"\"\n    # Read JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    # Collect all values for each key\n    values_by_key = defaultdict(list)\n    \n    for record in data:\n        for key, value in record.items():\n            # Only process numeric values\n            if isinstance(value, (int, float)):\n                values_by_key[key].append(value)\n    \n    # Calculate mean and median for each key\n    results = {}\n    for key, values in values_by_key.items():\n        if values:  # Only process if we have values\n            results[key] = {\n                'mean': np.mean(values),\n                'median': np.median(values)\n            }\n    \n    # Write results to CSV\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        # Write header\n        writer.writerow(['Field', 'Mean', 'Median'])\n        # Write data\n        for key, stats in results.items():\n            writer.writerow([key, stats['mean'], stats['median']])\n    \n    return results",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 113, in test_case_1\n  File \"__test__.py\", line 107, in validate_data\nKeyError: 'key'\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 116, in test_case_2\n  File \"__test__.py\", line 107, in validate_data\nKeyError: 'key'\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 119, in test_case_3\n  File \"__test__.py\", line 107, in validate_data\nKeyError: 'key'\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 122, in test_case_4\n  File \"__test__.py\", line 107, in validate_data\nKeyError: 'key'\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 125, in test_case_5\n  File \"__test__.py\", line 107, in validate_data\nKeyError: 'key'\n"
        }
      }
    ],
    "BigCodeBench/637": [
      {
        "task_id": "BigCodeBench/637",
        "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndef task_func(num_students):\n\n    pass\nfrom random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndef task_func(num_students):\n    \"\"\"\n    Generate a Pandas DataFrame that displays the grades of a randomly selected group of students \n    in multiple courses. Calculate the average grade in each course, the number of students with \n    a passing grade (>= 60), and visualize this information using a bar plot.\n    \n    Args:\n        num_students (int): Number of students to generate grades for\n        \n    Returns:\n        Tuple[pd.DataFrame, plt.Axes]: A tuple containing the generated DataFrame and the bar plot's Axes object.\n    \"\"\"\n    # Define courses\n    courses = ['Math', 'Science', 'English', 'History', 'Art']\n    \n    # Generate random student IDs\n    student_ids = [f'Student_{i+1}' for i in range(num_students)]\n    \n    # Generate random grades for each student in each course\n    data = {}\n    for course in courses:\n        # Generate random grades between 0 and 100\n        data[course] = np.random.randint(0, 101, size=num_students)\n    \n    # Create DataFrame\n    df = pd.DataFrame(data, index=student_ids)\n    \n    # Calculate statistics\n    avg_grades = df.mean()\n    passing_counts = (df >= 60).sum()\n    \n    # Create bar plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Set up bar positions\n    x = np.arange(len(courses))\n    width = 0.35\n    \n    # Create bars\n    bars1 = ax.bar(x - width/2, avg_grades, width, label='Average Grade', color='skyblue')\n    bars2 = ax.bar(x + width/2, passing_counts, width, label='Passing Count', color='lightcoral')\n    \n    # Add value labels on bars\n    for bar in bars1:\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height,\n                f'{height:.1f}', ha='center', va='bottom')\n    \n    for bar in bars2:\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height,\n                f'{int(height)}', ha='center', va='bottom')\n    \n    # Customize plot\n    ax.set_xlabel('Courses')\n    ax.set_ylabel('Values')\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_xticks(x)\n    ax.set_xticklabels(courses)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # Adjust layout\n    plt.tight_layout()\n    \n    return df, ax",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/777": [
      {
        "task_id": "BigCodeBench/777",
        "solution": "import re\nimport os\nimport zipfile\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n\n    pass\nimport re\nimport os\nimport zipfile\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    \"\"\"\n    Unzip all zip files in a directory whose name matches a certain pattern by splitting \n    the filename the last time \"-\" occurs and using the prefix part of the filename as \n    the directory to extract.\n    \n    Args:\n        directory (str): The directory containing zip files\n        pattern (str): Regular expression pattern to match zip files\n        \n    Returns:\n        list: A list of directories where the files were extracted.\n    \"\"\"\n    extracted_dirs = []\n    \n    # Check if directory exists\n    if not os.path.exists(directory):\n        return extracted_dirs\n    \n    # Get all files in the directory\n    for filename in os.listdir(directory):\n        # Check if file matches the pattern\n        if re.match(pattern, filename):\n            # Get full path of the zip file\n            zip_path = os.path.join(directory, filename)\n            \n            # Check if it's actually a file (not a directory)\n            if os.path.isfile(zip_path):\n                # Split filename at the last occurrence of \"-\"\n                # Remove the .zip extension first\n                name_without_ext = filename[:-4] if filename.endswith('.zip') else filename\n                \n                # Find the last occurrence of \"-\" and split there\n                last_dash_index = name_without_ext.rfind('-')\n                if last_dash_index != -1:\n                    # Get the prefix part before the last \"-\"\n                    prefix = name_without_ext[:last_dash_index]\n                    \n                    # Create extraction directory path\n                    extract_dir = os.path.join(directory, prefix)\n                    \n                    # Create directory if it doesn't exist\n                    os.makedirs(extract_dir, exist_ok=True)\n                    \n                    try:\n                        # Extract the zip file\n                        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                            zip_ref.extractall(extract_dir)\n                        \n                        # Add to list of extracted directories\n                        if extract_dir not in extracted_dirs:\n                            extracted_dirs.append(extract_dir)\n                            \n                    except zipfile.BadZipFile:\n                        # Skip if it's not a valid zip file\n                        continue\n                    except Exception as e:\n                        # Skip on any other extraction errors\n                        print(f\"Error extracting {filename}: {str(e)}\")\n                        continue\n    \n    return extracted_dirs",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 98, in test_case_1\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 982, in assert_has_calls\n    raise AssertionError(\nAssertionError: 'ZipFile' does not contain all of (call('/fake/test_zip_dir/sample-123.zip', 'r'), call('/fake/test_zip_dir/test_data-456.zip', 'r'), call('/fake/test_zip_dir/data_test-789.zip', 'r')) in its call list, found [] instead\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 109, in test_case_2\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 940, in assert_called_once_with\n    raise AssertionError(msg)\nAssertionError: Expected 'makedirs' to be called once. Called 0 times.\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 119, in test_case_3\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 940, in assert_called_once_with\n    raise AssertionError(msg)\nAssertionError: Expected 'makedirs' to be called once. Called 0 times.\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 142, in test_case_5\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 940, in assert_called_once_with\n    raise AssertionError(msg)\nAssertionError: Expected 'ZipFile' to be called once. Called 0 times.\n"
        }
      }
    ],
    "BigCodeBench/654": [
      {
        "task_id": "BigCodeBench/654",
        "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\ndef task_func(array, target_value):\n\n    pass\nimport matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\ndef task_func(array, target_value):\n    \"\"\"\n    Fit an exponential decay function to the indices in the array where the first column matches the target value.\n    \n    Parameters:\n    array: numpy array with at least one column\n    target_value: value to match in the first column\n    \n    Returns:\n    tuple: Containing the optimized parameters of the fitting function (popt) and the matplotlib Axes object.\n    \"\"\"\n    # Find indices where first column matches target value\n    matching_indices = np.where(array[:, 0] == target_value)[0]\n    \n    if len(matching_indices) == 0:\n        raise ValueError(f\"No values matching {target_value} found in first column\")\n    \n    # Create x values (position in the matching indices) and y values (the actual indices)\n    x_data = np.arange(len(matching_indices))\n    y_data = matching_indices.astype(float)\n    \n    # Define exponential decay function: y = a * exp(-b * x) + c\n    def exp_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n    \n    # Initial guess for parameters\n    # a: initial amplitude (difference between first and last y value)\n    # b: decay rate (start with 0.5)\n    # c: offset (approximate with last y value)\n    initial_guess = [\n        y_data[0] - y_data[-1] if len(y_data) > 1 else y_data[0],\n        0.5,\n        y_data[-1] if len(y_data) > 1 else 0\n    ]\n    \n    try:\n        # Fit the exponential decay function\n        popt, pcov = optimize.curve_fit(exp_decay, x_data, y_data, p0=initial_guess, maxfev=5000)\n    except:\n        # If fitting fails, try with different initial parameters\n        initial_guess = [y_data[0], 0.1, 0]\n        popt, pcov = optimize.curve_fit(exp_decay, x_data, y_data, p0=initial_guess, maxfev=5000)\n    \n    # Create plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot original data points\n    ax.scatter(x_data, y_data, color='blue', label='Data points', s=50)\n    \n    # Plot fitted curve\n    x_fit = np.linspace(0, len(matching_indices)-1, 100)\n    y_fit = exp_decay(x_fit, *popt)\n    ax.plot(x_fit, y_fit, 'r-', label=f'Fitted: {popt[0]:.2f} * exp(-{popt[1]:.2f} * x) + {popt[2]:.2f}', linewidth=2)\n    \n    ax.set_xlabel('Position in matching indices')\n    ax.set_ylabel('Array index')\n    ax.set_title(f'Exponential Decay Fit for indices where first column = {target_value}')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    return popt, ax",
        "status": "fail",
        "details": {
          "test_functionality": "Traceback (most recent call last):\n  File \"__test__.py\", line 47, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 794, in curve_fit\n    raise RuntimeError(\"Optimal parameters not found: \" + errmsg)\nRuntimeError: Optimal parameters not found: Number of calls to function has reached maxfev = 5000.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 102, in test_functionality\n  File \"__test__.py\", line 51, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 794, in curve_fit\n    raise RuntimeError(\"Optimal parameters not found: \" + errmsg)\nRuntimeError: Optimal parameters not found: Number of calls to function has reached maxfev = 5000.\n",
          "test_not_enough_points": "Traceback (most recent call last):\n  File \"__test__.py\", line 47, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 789, in curve_fit\n    res = leastsq(func, p0, Dfun=jac, full_output=1, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 414, in leastsq\n    raise TypeError(f\"Improper input: func input vector length N={n} must\"\nTypeError: Improper input: func input vector length N=3 must not exceed func output vector length M=1\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 99, in test_not_enough_points\n  File \"__test__.py\", line 51, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 789, in curve_fit\n    res = leastsq(func, p0, Dfun=jac, full_output=1, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 414, in leastsq\n    raise TypeError(f\"Improper input: func input vector length N={n} must\"\nTypeError: Improper input: func input vector length N=3 must not exceed func output vector length M=1\n",
          "test_return_types": "Traceback (most recent call last):\n  File \"__test__.py\", line 47, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 794, in curve_fit\n    raise RuntimeError(\"Optimal parameters not found: \" + errmsg)\nRuntimeError: Optimal parameters not found: Number of calls to function has reached maxfev = 5000.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 84, in test_return_types\n  File \"__test__.py\", line 51, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 794, in curve_fit\n    raise RuntimeError(\"Optimal parameters not found: \" + errmsg)\nRuntimeError: Optimal parameters not found: Number of calls to function has reached maxfev = 5000.\n",
          "test_target_value_found": "Traceback (most recent call last):\n  File \"__test__.py\", line 47, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 794, in curve_fit\n    raise RuntimeError(\"Optimal parameters not found: \" + errmsg)\nRuntimeError: Optimal parameters not found: Number of calls to function has reached maxfev = 5000.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 89, in test_target_value_found\n  File \"__test__.py\", line 51, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 794, in curve_fit\n    raise RuntimeError(\"Optimal parameters not found: \" + errmsg)\nRuntimeError: Optimal parameters not found: Number of calls to function has reached maxfev = 5000.\n"
        }
      }
    ],
    "BigCodeBench/618": [
      {
        "task_id": "BigCodeBench/618",
        "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\ndef task_func(goals, penalties):\n\n    pass\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties):\n    \"\"\"\n    Generate and visualize a Pandas DataFrame of football match results.\n    \n    Parameters:\n    goals (int): Number of matches to generate goals for\n    penalties (int): Number of matches to generate penalties for\n    \n    Returns:\n    pd.DataFrame: A dataframe containing match results\n    list: A list containing two seaborn plot objects (Axes) for goals and penalty costs\n    \"\"\"\n    \n    # Generate random data\n    data = []\n    \n    # Generate match results\n    for i in range(max(goals, penalties)):\n        team = TEAMS[randint(0, len(TEAMS) - 1)]\n        \n        # Generate goals if within range\n        if i < goals:\n            goals_scored = randint(0, 5)\n        else:\n            goals_scored = 0\n            \n        # Generate penalties if within range\n        if i < penalties:\n            penalties_count = randint(0, 3)\n        else:\n            penalties_count = 0\n            \n        # Calculate penalty cost\n        penalty_cost = penalties_count * PENALTY_COST\n        \n        data.append({\n            'Team': team,\n            'Goals': goals_scored,\n            'Penalties': penalties_count,\n            'Penalty Cost': penalty_cost\n        })\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    \n    # Create visualizations\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Plot 1: Goals by Team\n    goals_by_team = df.groupby('Team')['Goals'].sum().reset_index()\n    sns.barplot(data=goals_by_team, x='Team', y='Goals', ax=ax1)\n    ax1.set_title('Total Goals by Team')\n    ax1.set_xlabel('Team')\n    ax1.set_ylabel('Total Goals')\n    \n    # Plot 2: Penalty Costs by Team\n    penalty_costs_by_team = df.groupby('Team')['Penalty Cost'].sum().reset_index()\n    sns.barplot(data=penalty_costs_by_team, x='Team', y='Penalty Cost', ax=ax2)\n    ax2.set_title('Total Penalty Costs by Team')\n    ax2.set_xlabel('Team')\n    ax2.set_ylabel('Total Penalty Cost ($)')\n    \n    plt.tight_layout()\n    \n    return df, [ax1, ax2]",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 92, in test_case_1\nAssertionError: Tuples differ: (5, 4) != (5, 3)\n\nFirst differing element 1:\n4\n3\n\n- (5, 4)\n?     ^\n\n+ (5, 3)\n?     ^\n\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 134, in test_case_5\nAssertionError: False is not true\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 123, in test_case_4\n  File \"__test__.py\", line 66, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 8252, in groupby\n    return DataFrameGroupBy(\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/groupby/groupby.py\", line 931, in __init__\n    grouper, exclusions, obj = get_grouper(\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/groupby/grouper.py\", line 985, in get_grouper\n    raise KeyError(gpr)\nKeyError: 'Team'\n"
        }
      }
    ],
    "BigCodeBench/826": [
      {
        "task_id": "BigCodeBench/826",
        "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n\n    pass\nimport re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    \"\"\"\n    Move files from the source directory to the target directory based on a specified pattern.\n    \n    This function iterates through all files in the source directory, and if a file's name \n    matches the specified pattern, it is moved to the target directory.\n    \n    Args:\n        source_dir (str): Path to the source directory\n        target_dir (str): Path to the target directory\n        file_pattern (str): Regular expression pattern to match file names\n                           Default pattern matches alphanumeric filenames with txt, doc, or docx extensions\n    \n    Returns:\n        moved_files_count (int): The number of files that were successfully moved from \n                                the source directory to the target directory.\n    \"\"\"\n    moved_files_count = 0\n    \n    # Check if source directory exists\n    if not os.path.exists(source_dir):\n        print(f\"Source directory '{source_dir}' does not exist.\")\n        return moved_files_count\n    \n    # Create target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n        print(f\"Created target directory: {target_dir}\")\n    \n    # Compile the regex pattern for efficiency\n    pattern = re.compile(file_pattern)\n    \n    # Iterate through all items in the source directory\n    for item in os.listdir(source_dir):\n        source_path = os.path.join(source_dir, item)\n        \n        # Check if it's a file (not a directory)\n        if os.path.isfile(source_path):\n            # Check if the filename matches the pattern\n            if pattern.match(item):\n                target_path = os.path.join(target_dir, item)\n                \n                try:\n                    # Handle case where file already exists in target directory\n                    if os.path.exists(target_path):\n                        print(f\"Warning: File '{item}' already exists in target directory. Skipping.\")\n                        continue\n                    \n                    # Move the file\n                    shutil.move(source_path, target_path)\n                    moved_files_count += 1\n                    print(f\"Moved: {item}\")\n                    \n                except Exception as e:\n                    print(f\"Error moving file '{item}': {str(e)}\")\n    \n    print(f\"\\nTotal files moved: {moved_files_count}\")\n    return moved_files_count",
        "status": "fail",
        "details": {
          "test_directory_does_not_exist": "Traceback (most recent call last):\n  File \"__test__.py\", line 108, in test_directory_does_not_exist\nAssertionError: FileNotFoundError not raised\n"
        }
      }
    ],
    "BigCodeBench/752": [
      {
        "task_id": "BigCodeBench/752",
        "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data, target_column, test_size=0.2, random_state = 0) -> float:\n\n    pass\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    \"\"\"\n    Train a linear regression model and return the model score of the test set.\n    \n    Parameters:\n    - data: pandas DataFrame containing the training data\n    - target_column: string, name of the target column\n    - test_size: float, proportion of data to use for testing (default: 0.2)\n    - random_state: int, random state for reproducibility (default: 0)\n    \n    Returns:\n    - float: The model's score on the test set\n    \n    Raises:\n    - ValueError: If data is not a DataFrame\n    - ValueError: If data is empty\n    - ValueError: If target_column is not a column of data\n    - ValueError: If data contains values that are not numeric\n    - ValueError: If random_state is not an integer\n    - ValueError: If test_size is not between 0 and 1\n    \"\"\"\n    \n    # Validate that data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"If data is not a DataFrame.\")\n    \n    # Validate that data is not empty\n    if data.empty:\n        raise ValueError(\"If data is empty.\")\n    \n    # Validate that target_column exists in data\n    if target_column not in data.columns:\n        raise ValueError(\"If target_column ist not a column of data.\")\n    \n    # Validate that all data is numeric\n    try:\n        # Check if all columns contain numeric data\n        for col in data.columns:\n            pd.to_numeric(data[col])\n    except (ValueError, TypeError):\n        raise ValueError(\"If data contains values that are not numeric.\")\n    \n    # Validate random_state is an integer\n    if not isinstance(random_state, int):\n        raise ValueError(\"If random_state is not an integer.\")\n    \n    # Validate test_size is between 0 and 1\n    if not (0 < test_size < 1):\n        raise ValueError(\"If test_size is not between 0 and 1.\")\n    \n    # Separate features and target\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n    \n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state\n    )\n    \n    # Create and train the linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Return the model score on the test set\n    return model.score(X_test, y_test)",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/582": [
      {
        "task_id": "BigCodeBench/582",
        "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(size=1000):\n\n    pass\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(size=1000):\n    \"\"\"\n    Create a list of normally distributed random numbers and plot their histogram and PDF.\n    \n    Parameters:\n    size (int): Number of random samples to generate (default: 1000)\n    \n    Returns:\n    matplotlib.figure.Figure: A figure object containing the histogram and PDF plot.\n    \"\"\"\n    # Generate normally distributed random numbers\n    np.random.seed(42)  # For reproducibility\n    data = np.random.normal(loc=0, scale=1, size=size)\n    \n    # Create figure and axis\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot histogram\n    n, bins, patches = ax.hist(data, bins=30, density=True, alpha=0.7, \n                               color='skyblue', edgecolor='black', \n                               label='Histogram')\n    \n    # Calculate and plot the theoretical PDF\n    x = np.linspace(data.min() - 1, data.max() + 1, 1000)\n    pdf = stats.norm.pdf(x, loc=0, scale=1)\n    ax.plot(x, pdf, 'r-', linewidth=2, label='Theoretical PDF')\n    \n    # Calculate and plot the empirical PDF (fitted to the data)\n    mu, sigma = stats.norm.fit(data)\n    fitted_pdf = stats.norm.pdf(x, mu, sigma)\n    ax.plot(x, fitted_pdf, 'g--', linewidth=2, \n            label=f'Fitted PDF (\u03bc={mu:.3f}, \u03c3={sigma:.3f})')\n    \n    # Customize the plot\n    ax.set_xlabel('Value', fontsize=12)\n    ax.set_ylabel('Probability Density', fontsize=12)\n    ax.set_title(f'Histogram and PDF of {size} Normally Distributed Random Numbers', \n                 fontsize=14, fontweight='bold')\n    ax.legend(loc='upper right')\n    ax.grid(True, alpha=0.3)\n    \n    # Add some statistics as text\n    ax.text(0.02, 0.95, f'Sample mean: {np.mean(data):.3f}\\nSample std: {np.std(data, ddof=1):.3f}',\n            transform=ax.transAxes, verticalalignment='top',\n            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n    \n    plt.tight_layout()\n    \n    return fig",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 66, in test_case_1\nAssertionError: 2 != 1 : Expected 1 line for the PDF plot\n",
          "test_histogram_pdf_overlay": "Traceback (most recent call last):\n  File \"__test__.py\", line 88, in test_histogram_pdf_overlay\nAssertionError: 30 != 28 : Number of histogram bins does not match expected\n"
        }
      }
    ],
    "BigCodeBench/906": [
      {
        "task_id": "BigCodeBench/906",
        "solution": "import zipfile\nimport os\nimport re\nimport shutil\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n\n    pass\nimport zipfile\nimport os\nimport re\nimport shutil\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    Archives all processed files from a source directory to a target directory.\n    The function identifies processed files by the '_processed' suffix in the filename.\n    \n    Args:\n        source_dir: Path to the source directory containing files to archive\n        target_dir: Path to the target directory where the archive will be created\n        archive_name: Name of the archive file (default: 'archive.zip')\n    \n    Returns:\n        str: The path to the created archive\n    \"\"\"\n    # Ensure target directory exists\n    os.makedirs(target_dir, exist_ok=True)\n    \n    # Create full path for the archive\n    archive_path = os.path.join(target_dir, archive_name)\n    \n    # Create a list to store processed files\n    processed_files = []\n    \n    # Walk through the source directory\n    for root, dirs, files in os.walk(source_dir):\n        for file in files:\n            # Check if file has '_processed' suffix (before file extension)\n            # Extract filename without extension\n            name_without_ext = os.path.splitext(file)[0]\n            \n            # Check if it ends with '_processed'\n            if name_without_ext.endswith('_processed'):\n                full_path = os.path.join(root, file)\n                processed_files.append(full_path)\n    \n    # Create the archive\n    with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for file_path in processed_files:\n            # Calculate the relative path from source_dir\n            rel_path = os.path.relpath(file_path, source_dir)\n            # Add file to archive with relative path\n            zipf.write(file_path, arcname=rel_path)\n    \n    return archive_path",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/870": [
      {
        "task_id": "BigCodeBench/870",
        "solution": "import pandas as pd\nimport numpy as np\nimport itertools\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n\n    pass\nimport pandas as pd\nimport numpy as np\nimport itertools\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    \"\"\"\n    Calculate the mean of numerical values in each position across tuples in a list.\n    Non-numeric values are ignored, and means are computed only from available data.\n    \n    Parameters:\n    data_list: List of tuples containing mixed data types\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the mean values of the numerical data at each position.\n    \"\"\"\n    \n    # Handle empty list\n    if not data_list:\n        return pd.DataFrame(columns=['Mean Value'])\n    \n    # Find the maximum tuple length\n    max_length = max(len(tup) for tup in data_list)\n    \n    # Initialize lists to store numeric values for each position\n    position_values = [[] for _ in range(max_length)]\n    \n    # Collect numeric values for each position\n    for tup in data_list:\n        for i, value in enumerate(tup):\n            # Check if the value is numeric (int or float)\n            if isinstance(value, (int, float)) and not isinstance(value, bool):\n                position_values[i].append(value)\n            # Also try to convert string numbers\n            elif isinstance(value, str):\n                try:\n                    numeric_value = float(value)\n                    position_values[i].append(numeric_value)\n                except ValueError:\n                    pass\n    \n    # Calculate means for each position\n    means = []\n    for i, values in enumerate(position_values):\n        if values:\n            means.append(np.mean(values))\n        else:\n            means.append(np.nan)\n    \n    # Create DataFrame\n    index = [f'Position {i}' for i in range(max_length)]\n    df = pd.DataFrame({'Mean Value': means}, index=index)\n    \n    return df",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/346": [
      {
        "task_id": "BigCodeBench/346",
        "solution": "import subprocess\nimport os\nimport sys\nimport time\ndef task_func(script_path, wait=True, *args):\n\n    pass\nimport subprocess\nimport os\nimport sys\nimport time\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    Run a Python script as a process with predefined arguments.\n    \n    Args:\n        script_path (str): Path to the Python script to execute\n        wait (bool): If True, wait for the process to complete. If False, return None immediately.\n        *args: Additional arguments to pass to the script\n    \n    Returns:\n        int: The return code of the subprocess. If 'wait' is False, returns None.\n    \n    Raises:\n        ValueError: If the script does not exist.\n        subprocess.CalledProcessError: If the script raises an exception.\n    \"\"\"\n    # Check if the script exists\n    if not os.path.exists(script_path):\n        raise ValueError(f\"Script does not exist: {script_path}\")\n    \n    # Prepare the command to run the script\n    cmd = [sys.executable, script_path] + list(args)\n    \n    if wait:\n        # Run the process and wait for it to complete\n        try:\n            result = subprocess.run(cmd, check=True)\n            return result.returncode\n        except subprocess.CalledProcessError as e:\n            # Re-raise the exception if the script fails\n            raise e\n    else:\n        # Start the process without waiting\n        subprocess.Popen(cmd)\n        return None",
        "status": "fail",
        "details": {
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 90, in test_case_2\n  File \"__test__.py\", line 42, in task_func\n  File \"__test__.py\", line 38, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/bigcodebench/eval/utils.py\", line 61, in _run_patch\n    return original_run(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/bigcodebench/eval/utils.py\", line 174, in safe_subprocess_run\n    return original_subprocess_run(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['/usr/local/bin/python', '/tmp/tmpwy99p70a/test/script2.py']' returned non-zero exit status 1.\n"
        }
      }
    ],
    "BigCodeBench/865": [
      {
        "task_id": "BigCodeBench/865",
        "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data):\n\n    pass\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data):\n    \"\"\"\n    This function takes a list of tuples containing elements and their respective counts and weights.\n    It normalizes the counts using z-score normalization and the weights using min-max scaling.\n    Finally, it returns a pandas DataFrame with the items, normalized counts, and normalized weights.\n    \n    Parameters:\n    data: list of tuples, where each tuple contains (item, count, weight)\n    \n    Returns:\n    DataFrame: A pandas DataFrame with three columns: 'Item', 'Normalized Count', and 'Normalized Weight'.\n    Each row corresponds to an entry from the input data.\n    \"\"\"\n    # Handle empty data\n    if not data:\n        return pd.DataFrame(columns=['Item', 'Normalized Count', 'Normalized Weight'])\n    \n    # Extract items, counts, and weights from the data\n    items = [item[0] for item in data]\n    counts = [item[1] for item in data]\n    weights = [item[2] for item in data]\n    \n    # Convert to numpy arrays for processing\n    counts_array = np.array(counts)\n    weights_array = np.array(weights)\n    \n    # Normalize counts using z-score normalization\n    if len(counts) > 1 and np.std(counts_array) > 0:\n        normalized_counts = zscore(counts_array)\n    else:\n        # If standard deviation is 0 or only one element, set all to 0\n        normalized_counts = np.zeros_like(counts_array, dtype=float)\n    \n    # Normalize weights using min-max scaling\n    if len(weights) > 1:\n        scaler = MinMaxScaler()\n        # Reshape for sklearn (needs 2D array)\n        weights_reshaped = weights_array.reshape(-1, 1)\n        normalized_weights = scaler.fit_transform(weights_reshaped).flatten()\n    else:\n        # If only one element, set to 0 (or could be 1, depending on interpretation)\n        normalized_weights = np.zeros_like(weights_array, dtype=float)\n    \n    # Create and return the DataFrame\n    df = pd.DataFrame({\n        'Item': items,\n        'Normalized Count': normalized_counts,\n        'Normalized Weight': normalized_weights\n    })\n    \n    return df",
        "status": "fail",
        "details": {
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 100, in test_case_3\nAssertionError: False is not true\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 115, in test_case_5\nAssertionError: False is not true\n"
        }
      }
    ],
    "BigCodeBench/655": [
      {
        "task_id": "BigCodeBench/655",
        "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(texts, num_topics):\n\n    pass\nimport re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(texts, num_topics):\n    \"\"\"\n    Performs topic extraction from a collection of text documents using Non-Negative Matrix Factorization (NMF).\n    \n    Args:\n        texts (list of str): Collection of text documents\n        num_topics (int): Number of topics to extract\n    \n    Returns:\n        list of list of str: A list where each element is a list of words representing a topic.\n    \"\"\"\n    # Preprocess texts\n    processed_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters (excluding spaces)\n        text = ALPHANUMERIC.sub(' ', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Remove stopwords\n        words = text.split()\n        words = [word for word in words if word not in STOPWORDS]\n        processed_texts.append(' '.join(words))\n    \n    # Create TF-IDF vectorizer\n    vectorizer = TfidfVectorizer(max_features=100, min_df=1, max_df=0.95)\n    \n    # Fit and transform the processed texts\n    tfidf_matrix = vectorizer.fit_transform(processed_texts)\n    \n    # Get feature names (words)\n    feature_names = vectorizer.get_feature_names_out()\n    \n    # Apply NMF\n    nmf_model = NMF(n_components=num_topics, random_state=42, init='nndsvd')\n    nmf_model.fit(tfidf_matrix)\n    \n    # Extract topics\n    topics = []\n    for topic_idx, topic in enumerate(nmf_model.components_):\n        # Get indices of top words for this topic\n        top_word_indices = topic.argsort()[-10:][::-1]  # Get top 10 words\n        top_words = [feature_names[i] for i in top_word_indices]\n        topics.append(top_words)\n    \n    return topics",
        "status": "fail",
        "details": {
          "test_all_stopwords": "Traceback (most recent call last):\n  File \"__test__.py\", line 93, in test_all_stopwords\n  File \"__test__.py\", line 46, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 2139, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 1389, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 1295, in _count_vocab\n    raise ValueError(\nValueError: empty vocabulary; perhaps the documents only contain stop words\n",
          "test_empty_texts": "Traceback (most recent call last):\n  File \"__test__.py\", line 83, in test_empty_texts\n  File \"__test__.py\", line 46, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 2139, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 1389, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 1295, in _count_vocab\n    raise ValueError(\nValueError: empty vocabulary; perhaps the documents only contain stop words\n",
          "test_single_text": "Traceback (most recent call last):\n  File \"__test__.py\", line 88, in test_single_text\n  File \"__test__.py\", line 46, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 2139, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 1399, in fit_transform\n    raise ValueError(\"max_df corresponds to < documents than min_df\")\nValueError: max_df corresponds to < documents than min_df\n"
        }
      }
    ],
    "BigCodeBench/897": [
      {
        "task_id": "BigCodeBench/897",
        "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\ndef task_func(rolls, seed=None):\n\n    pass\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nNUMBERS = list(range(1, 7))\ndef task_func(rolls, seed=None):\n    \"\"\"\n    Simulate a number of dice rolls, calculate the frequency of each result,\n    and return both the frequency array and a histogram of the results.\n    \n    Parameters:\n    rolls (int): Number of dice rolls to simulate\n    seed (int, optional): Random seed for reproducibility\n    \n    Returns:\n    tuple: A tuple containing:\n        np.array: A numpy array with the frequency of each outcome.\n        matplotlib.Axes: Axes object representing the histogram.\n    \"\"\"\n    # Set random seed if provided\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n    \n    # Simulate dice rolls\n    dice_rolls = [random.choice(NUMBERS) for _ in range(rolls)]\n    \n    # Calculate frequency of each outcome\n    frequency = np.zeros(6, dtype=int)\n    for roll in dice_rolls:\n        frequency[roll - 1] += 1\n    \n    # Create histogram\n    fig, ax = plt.subplots(figsize=(8, 6))\n    \n    # Plot histogram\n    ax.bar(NUMBERS, frequency, color='skyblue', edgecolor='black', alpha=0.7)\n    \n    # Set labels and title\n    ax.set_xlabel('Dice Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Dice Rolls')\n    \n    # Set x-axis ticks to show all dice values\n    ax.set_xticks(NUMBERS)\n    \n    # Add grid for better readability\n    ax.grid(axis='y', alpha=0.3)\n    \n    # Set y-axis to start from 0\n    ax.set_ylim(bottom=0)\n    \n    return frequency, ax",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/879": [
      {
        "task_id": "BigCodeBench/879",
        "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ndef task_func(data, col1, col2):\n\n    pass\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ndef task_func(data, col1, col2):\n    \"\"\"\n    Perform a chi-square test of independence of variables in a contingency table.\n    \n    This function takes a DataFrame containing categorical data and two column names,\n    then constructs a contingency table from the two categorical columns and performs\n    a chi-square test of independence.\n    \n    Parameters:\n    -----------\n    data : pd.DataFrame\n        The input DataFrame containing the data\n    col1 : str\n        Name of the first categorical column\n    col2 : str\n        Name of the second categorical column\n    \n    Returns:\n    --------\n    float: The p-value of the chi-square test of independence\n    \n    Raises:\n    -------\n    ValueError: If 'data' is empty, if 'col1' or 'col2' are not in 'data',\n                if one or both of the columns do not have multiple categories,\n                or if some categories have less than 5 observations\n    TypeError: If one or both of the columns contain non-categorical data\n    \"\"\"\n    \n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"Data is empty\")\n    \n    # Check if columns exist in the DataFrame\n    if col1 not in data.columns:\n        raise ValueError(f\"Column '{col1}' not found in data\")\n    if col2 not in data.columns:\n        raise ValueError(f\"Column '{col2}' not found in data\")\n    \n    # Extract the columns\n    series1 = data[col1]\n    series2 = data[col2]\n    \n    # Check for non-categorical data\n    # We'll consider data categorical if it's string, object dtype, or categorical\n    def is_categorical(series):\n        return (series.dtype == 'object' or \n                series.dtype == 'category' or \n                pd.api.types.is_string_dtype(series))\n    \n    if not is_categorical(series1):\n        raise TypeError(f\"Column '{col1}' contains non-categorical data\")\n    if not is_categorical(series2):\n        raise TypeError(f\"Column '{col2}' contains non-categorical data\")\n    \n    # Check if columns have multiple categories\n    unique_col1 = series1.nunique()\n    unique_col2 = series2.nunique()\n    \n    if unique_col1 < 2:\n        raise ValueError(f\"Column '{col1}' must have at least 2 categories\")\n    if unique_col2 < 2:\n        raise ValueError(f\"Column '{col2}' must have at least 2 categories\")\n    \n    # Create contingency table\n    contingency_table = pd.crosstab(series1, series2)\n    \n    # Check if all cells have at least 5 observations\n    # This is a common assumption for chi-square test validity\n    if (contingency_table < 5).any().any():\n        raise ValueError(\"Some categories have less than 5 observations\")\n    \n    # Perform chi-square test\n    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n    \n    return p_value",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/917": [
      {
        "task_id": "BigCodeBench/917",
        "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n\n    pass\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    \"\"\"\n    Forecasts the share closing prices for the next 7 days using the ARIMA model and plots the forecast.\n    \n    Parameters:\n    df (pd.DataFrame): DataFrame containing historical stock data with a 'Close' column\n    \n    Returns:\n    Tuple[List[float], Axes]: A tuple containing:\n        - A list with forecasted prices for the next 7 days\n        - A matplotlib Axes object containing the subplot\n    \"\"\"\n    # Extract closing prices\n    closing_prices = df['Close'].values\n    \n    # Create and fit ARIMA model\n    # Using ARIMA(5,1,0) as a reasonable default for stock prices\n    # You can adjust these parameters based on your specific needs\n    model = ARIMA(closing_prices, order=(5, 1, 0))\n    model_fit = model.fit()\n    \n    # Forecast next 7 days\n    forecast_result = model_fit.forecast(steps=7)\n    forecast_list = forecast_result.tolist()\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    # Plot historical data\n    ax.plot(range(len(closing_prices)), closing_prices, 'b-', label='Historical Prices', linewidth=2)\n    \n    # Plot forecast\n    forecast_start = len(closing_prices)\n    forecast_range = range(forecast_start, forecast_start + 7)\n    ax.plot(forecast_range, forecast_list, 'r--', label='7-Day Forecast', linewidth=2, marker='o')\n    \n    # Add vertical line to separate historical and forecast\n    ax.axvline(x=len(closing_prices) - 1, color='gray', linestyle=':', alpha=0.7)\n    \n    # Customize the plot\n    ax.set_xlabel('Days', fontsize=12)\n    ax.set_ylabel('Closing Price', fontsize=12)\n    ax.set_title('Stock Price Forecast - Next 7 Days (ARIMA Model)', fontsize=14, fontweight='bold')\n    ax.legend(loc='best')\n    ax.grid(True, alpha=0.3)\n    \n    # Add forecast values as text annotations\n    for i, (x, y) in enumerate(zip(forecast_range, forecast_list)):\n        ax.annotate(f'${y:.2f}', \n                   xy=(x, y), \n                   xytext=(5, 5), \n                   textcoords='offset points',\n                   fontsize=8,\n                   alpha=0.7)\n    \n    plt.tight_layout()\n    \n    return forecast_list, ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Close'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 86, in test_case_1\n  File \"__test__.py\", line 27, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Close'\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Close'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 107, in test_case_2\n  File \"__test__.py\", line 27, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Close'\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Close'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 127, in test_case_3\n  File \"__test__.py\", line 27, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Close'\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Close'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 148, in test_case_4\n  File \"__test__.py\", line 27, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Close'\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Close'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 168, in test_case_5\n  File \"__test__.py\", line 27, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Close'\n"
        }
      }
    ],
    "BigCodeBench/914": [
      {
        "task_id": "BigCodeBench/914",
        "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n\n    pass\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    \"\"\"\n    Predicts the stock closing prices for the next 7 days using simple linear regression\n    and plots the data.\n    \n    Parameters:\n    df (pd.DataFrame): DataFrame with 'Date' and 'Close' columns\n    \n    Returns:\n    tuple: A tuple containing:\n        list: A list with predicted prices for the next 7 days.\n        Axes: The matplotlib Axes object containing the plot.\n    \"\"\"\n    # Make a copy to avoid modifying the original dataframe\n    df_copy = df.copy()\n    \n    # Ensure Date column is datetime\n    df_copy['Date'] = pd.to_datetime(df_copy['Date'])\n    \n    # Sort by date\n    df_copy = df_copy.sort_values('Date')\n    \n    # Convert dates to numerical values (days since first date)\n    first_date = df_copy['Date'].min()\n    df_copy['Days'] = (df_copy['Date'] - first_date).dt.total_seconds() / (24 * 60 * 60)\n    \n    # Prepare data for linear regression\n    X = df_copy['Days'].values.reshape(-1, 1)\n    y = df_copy['Close'].values\n    \n    # Train linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Generate future dates for prediction (next 7 days)\n    last_date = df_copy['Date'].max()\n    future_dates = []\n    future_days = []\n    \n    for i in range(1, 8):\n        future_date = last_date + pd.Timedelta(seconds=i * 24 * 60 * 60)\n        future_dates.append(future_date)\n        days_since_first = (future_date - first_date).total_seconds() / (24 * 60 * 60)\n        future_days.append(days_since_first)\n    \n    # Make predictions\n    future_days_array = np.array(future_days).reshape(-1, 1)\n    predictions = model.predict(future_days_array)\n    \n    # Create plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot historical data\n    ax.scatter(df_copy['Date'], df_copy['Close'], color='blue', label='Historical Data', alpha=0.6)\n    \n    # Plot regression line for historical data\n    historical_predictions = model.predict(X)\n    ax.plot(df_copy['Date'], historical_predictions, color='red', linestyle='--', \n            label='Linear Regression Fit', alpha=0.8)\n    \n    # Plot future predictions\n    ax.scatter(future_dates, predictions, color='green', marker='s', s=100, \n              label='Predicted Prices', alpha=0.8)\n    \n    # Connect last historical point to first prediction\n    ax.plot([df_copy['Date'].iloc[-1], future_dates[0]], \n            [df_copy['Close'].iloc[-1], predictions[0]], \n            color='gray', linestyle=':', alpha=0.5)\n    \n    # Extend regression line to future\n    all_dates = list(df_copy['Date']) + future_dates\n    all_days = np.concatenate([X.flatten(), future_days])\n    all_predictions = model.predict(all_days.reshape(-1, 1))\n    ax.plot(all_dates, all_predictions, color='red', linestyle='--', alpha=0.3)\n    \n    # Formatting\n    ax.set_xlabel('Date', fontsize=12)\n    ax.set_ylabel('Closing Price', fontsize=12)\n    ax.set_title('Stock Price Prediction - Next 7 Days', fontsize=14)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    # Convert predictions to list\n    predictions_list = predictions.tolist()\n    \n    return predictions_list, ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Date'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 110, in test_case_1\n  File \"__test__.py\", line 29, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Date'\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Date'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 118, in test_case_2\n  File \"__test__.py\", line 29, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Date'\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Date'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 126, in test_case_3\n  File \"__test__.py\", line 29, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Date'\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Date'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 134, in test_case_4\n  File \"__test__.py\", line 29, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Date'\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Date'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 142, in test_case_5\n  File \"__test__.py\", line 29, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Date'\n"
        }
      }
    ],
    "BigCodeBench/915": [
      {
        "task_id": "BigCodeBench/915",
        "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\ndef task_func(df, z_threshold=2):\n\n    pass\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\nimport pandas as pd\ndef task_func(df, z_threshold=2):\n    \"\"\"\n    Identifies and plots outliers in the 'closing_price' column of a given DataFrame using the Z-Score method.\n    \n    Parameters:\n    df (pandas.DataFrame): DataFrame containing a 'closing_price' column\n    z_threshold (float): Z-Score threshold for identifying outliers (default: 2)\n    \n    Returns:\n    tuple: A tuple containing the following elements:\n        pandas.DataFrame: A DataFrame containing the outliers in the 'closing_price' column.\n        matplotlib.axes._axes.Axes: The plot object displaying the outliers, with x-axis label 'Index', \n                                   y-axis label 'Closing Price', and title 'Outliers in Closing Prices'.\n    \"\"\"\n    # Calculate Z-scores for the closing_price column\n    z_scores = np.abs(zscore(df['closing_price']))\n    \n    # Identify outliers based on the threshold\n    outlier_mask = z_scores > z_threshold\n    outliers_df = df[outlier_mask].copy()\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot all closing prices\n    ax.scatter(df.index, df['closing_price'], label='Normal', alpha=0.6, color='blue')\n    \n    # Highlight outliers\n    if len(outliers_df) > 0:\n        ax.scatter(outliers_df.index, outliers_df['closing_price'], \n                  label='Outliers', color='red', s=100, marker='o', edgecolors='black')\n    \n    # Set labels and title\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    return outliers_df, ax",
        "status": "fail",
        "details": {
          "test_case_2": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Z_score'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 71, in test_case_2\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Z_score'\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Z_score'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 79, in test_case_3\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Z_score'\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Z_score'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 86, in test_case_4\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Z_score'\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Z_score'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 94, in test_case_5\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Z_score'\n"
        }
      }
    ],
    "BigCodeBench/928": [
      {
        "task_id": "BigCodeBench/928",
        "solution": "from collections import Counter\nimport itertools\nimport string\ndef task_func(word: str) -> dict:\n\n    pass\nfrom collections import Counter\nimport itertools\nimport string\ndef task_func(word: str) -> dict:\n    \"\"\"\n    Create a dictionary containing all possible two-letter combinations of the lowercase English alphabets.\n    The dictionary values represent the frequency of these two-letter combinations in the given word.\n    If a combination does not appear in the word, its value will be 0.\n    \n    Args:\n        word (str): The input word to analyze\n        \n    Returns:\n        dict: A dictionary with keys as two-letter alphabet combinations and values as their counts in the word.\n    \"\"\"\n    # Generate all possible two-letter combinations\n    all_combinations = [''.join(combo) for combo in itertools.product(string.ascii_lowercase, repeat=2)]\n    \n    # Initialize dictionary with all combinations set to 0\n    result = {combo: 0 for combo in all_combinations}\n    \n    # Count two-letter combinations in the word\n    word = word.lower()  # Convert to lowercase to match our alphabet\n    for i in range(len(word) - 1):\n        two_letter = word[i:i+2]\n        # Only count if both characters are lowercase letters\n        if len(two_letter) == 2 and all(c in string.ascii_lowercase for c in two_letter):\n            result[two_letter] += 1\n    \n    return result",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/916": [
      {
        "task_id": "BigCodeBench/916",
        "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df: pd.DataFrame) -> tuple:\n\n    pass\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualize the distribution of stock closing prices using both a box plot and a histogram \n    within a single figure. This function is designed to help understand the spread, central \n    tendency, and the distribution shape of stock closing prices.\n    \n    Parameters:\n    df (pd.DataFrame): DataFrame containing stock data with a 'Close' column\n    \n    Returns:\n    tuple: A tuple containing two matplotlib.axes._axes.Axes objects: the first for the boxplot\n    and the second for the histogram.\n    \"\"\"\n    # Create a figure with two subplots side by side\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Create box plot\n    ax1.boxplot(df['Close'].dropna())\n    ax1.set_title('Box Plot of Closing Prices')\n    ax1.set_ylabel('Closing Price')\n    ax1.grid(True, alpha=0.3)\n    \n    # Create histogram\n    ax2.hist(df['Close'].dropna(), bins=30, edgecolor='black', alpha=0.7)\n    ax2.set_title('Histogram of Closing Prices')\n    ax2.set_xlabel('Closing Price')\n    ax2.set_ylabel('Frequency')\n    ax2.grid(True, alpha=0.3)\n    \n    # Adjust layout to prevent overlap\n    plt.tight_layout()\n    \n    return (ax1, ax2)",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Close'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 54, in test_case_1\n  File \"__test__.py\", line 27, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Close'\n",
          "test_empty_df": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Close'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 67, in test_empty_df\n  File \"__test__.py\", line 27, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Close'\n",
          "test_large_values_df": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Close'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 88, in test_large_values_df\n  File \"__test__.py\", line 27, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Close'\n",
          "test_single_value_df": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Close'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 80, in test_single_value_df\n  File \"__test__.py\", line 27, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Close'\n"
        }
      }
    ],
    "BigCodeBench/952": [
      {
        "task_id": "BigCodeBench/952",
        "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n\n    pass\nimport pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    \"\"\"\n    Randomly assigns a specified number of tasks to employees with a due date of the current day.\n    \n    Args:\n        task_list: List of task names to choose from\n        n_tasks: Number of tasks to assign\n        employees: List of employee names (default provided)\n        seed: Random seed for reproducibility (optional)\n    \n    Returns:\n        pd.DataFrame: Contains columns 'Task Name', 'Assigned To', and 'Due Date'\n    \n    Raises:\n        ValueError: If n_tasks is negative\n    \"\"\"\n    # Check if n_tasks is negative\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n    \n    # Set random seed if provided\n    if seed is not None:\n        random.seed(seed)\n    \n    # Get current date\n    current_date = datetime.now().date()\n    \n    # Initialize lists to store data\n    task_names = []\n    assigned_to = []\n    due_dates = []\n    \n    # Randomly assign tasks\n    for _ in range(n_tasks):\n        # Randomly select a task from task_list\n        task = random.choice(task_list)\n        # Sanitize task name by replacing spaces with underscores\n        sanitized_task = task.replace(\" \", \"_\")\n        task_names.append(sanitized_task)\n        \n        # Randomly select an employee\n        employee = random.choice(employees)\n        assigned_to.append(employee)\n        \n        # Set due date to current date\n        due_dates.append(current_date)\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'Task Name': task_names,\n        'Assigned To': assigned_to,\n        'Due Date': due_dates\n    })\n    \n    return df",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 90, in test_case_1\nAssertionError: False is not true\n",
          "test_case_6": "Traceback (most recent call last):\n  File \"__test__.py\", line 119, in test_case_6\n  File \"__test__.py\", line 55, in task_func\n  File \"/usr/local/lib/python3.10/random.py\", line 378, in choice\n    return seq[self._randbelow(len(seq))]\nIndexError: list index out of range\n"
        }
      }
    ],
    "BigCodeBench/963": [
      {
        "task_id": "BigCodeBench/963",
        "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\ndef task_func(source_directory, target_directory, zip_name):\n\n    pass\nimport os\nimport glob\nfrom pathlib import Path\nimport zipfile\ndef task_func(source_directory, target_directory, zip_name):\n    \"\"\"\n    Zip files with certain extensions from a source directory and save it as a zip file \n    saved to a target directory.\n    \n    Args:\n        source_directory (str): Path to the source directory containing files to zip\n        target_directory (str): Path to the target directory where zip file will be saved\n        zip_name (str): Name of the zip file (without .zip extension)\n    \n    Returns:\n        str: The full path to the created zip file in the format \"/path/to/target_directory/zip_name.zip\"\n    \n    Raises:\n        OSError: If the source_directory does not exist\n    \"\"\"\n    # Check if source directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(f\"Source directory '{source_directory}' does not exist\")\n    \n    # Valid extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    \n    # Create target directory if it doesn't exist\n    os.makedirs(target_directory, exist_ok=True)\n    \n    # Ensure zip_name has .zip extension\n    if not zip_name.endswith('.zip'):\n        zip_name += '.zip'\n    \n    # Full path for the zip file\n    zip_path = os.path.join(target_directory, zip_name)\n    \n    # Collect all files with valid extensions\n    files_to_zip = []\n    for ext in valid_extensions:\n        # Use glob to find files with the extension\n        pattern = os.path.join(source_directory, f'*{ext}')\n        files_to_zip.extend(glob.glob(pattern))\n    \n    # Create the zip file\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for file_path in files_to_zip:\n            # Add file to zip with just the filename (not the full path)\n            arcname = os.path.basename(file_path)\n            zipf.write(file_path, arcname)\n    \n    # Return the full path to the created zip file\n    return os.path.abspath(zip_path)",
        "status": "fail",
        "details": {
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 145, in test_case_5\nAssertionError: 0 != 2\n",
          "test_case_7": "Traceback (most recent call last):\n  File \"__test__.py\", line 161, in test_case_7\nAssertionError: 0 != 4\n"
        }
      }
    ],
    "BigCodeBench/502": [
      {
        "task_id": "BigCodeBench/502",
        "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\ndef task_func(days_in_past=7, random_seed=0):\n\n    pass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    \"\"\"\n    Generates a graph of daily activity durations for a specified number of days in the past \n    using randomly generated data for activities.\n    \n    Parameters:\n    days_in_past (int): Number of days to generate data for\n    random_seed (int): Random seed for reproducibility\n    \n    Returns:\n    Tuple containing:\n        ax (matplotlib.pyplot.Axes): Axes object from the plot\n        df (pd.DataFrame): DataFrame used for plotting\n    \"\"\"\n    # Set random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Define activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    \n    # Generate dates\n    end_date = datetime.now().date()\n    dates = [end_date - timedelta(days=i) for i in range(days_in_past-1, -1, -1)]\n    \n    # Create data\n    data = []\n    for date in dates:\n        for activity in activities:\n            # Generate random duration between 0 and 120\n            duration = random.randint(0, 120)\n            data.append({\n                'Date': date,\n                'Activity': activity,\n                'Duration': duration\n            })\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    \n    # Create the plot\n    plt.figure(figsize=(12, 6))\n    ax = sns.lineplot(data=df, x='Date', y='Duration', hue='Activity', marker='o')\n    \n    # Customize the plot\n    ax.set_title(f'Daily Activity Durations (Past {days_in_past} Days)', fontsize=16)\n    ax.set_xlabel('Date', fontsize=12)\n    ax.set_ylabel('Duration (minutes)', fontsize=12)\n    \n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45, ha='right')\n    \n    # Adjust layout to prevent label cutoff\n    plt.tight_layout()\n    \n    # Move legend outside the plot area\n    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    \n    return ax, df",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/971": [
      {
        "task_id": "BigCodeBench/971",
        "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\ndef task_func(directory_path: str):\n\n    pass\nimport os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\ndef task_func(directory_path: str):\n    \"\"\"\n    Analyzes a given directory, listing each file it contains along with its size,\n    creation time, and last modification time without recursing into subdirectories.\n    \n    Args:\n        directory_path (str): Path to the directory to analyze\n        \n    Returns:\n        list of tuples: Each tuple contains (file name, file size in bytes,\n        creation time in ISO format, modification time in ISO format).\n        \n    Raises:\n        ValueError: If the provided directory does not exist.\n        \n    Notes:\n        The function assumes the directory exists and contains only files \n        (no subdirectories are processed). Times are reported in system time, UTC.\n        The creation and modification times are platform dependent; on some systems,\n        the creation time might not be available and might be replaced by the last\n        metadata change time.\n    \"\"\"\n    # Check if directory exists\n    if not os.path.exists(directory_path):\n        raise ValueError(f\"Directory '{directory_path}' does not exist\")\n    \n    # Check if it's actually a directory\n    if not os.path.isdir(directory_path):\n        raise ValueError(f\"'{directory_path}' is not a directory\")\n    \n    result = []\n    \n    # List all items in the directory\n    for item in os.listdir(directory_path):\n        item_path = os.path.join(directory_path, item)\n        \n        # Only process files, skip directories\n        if os.path.isfile(item_path):\n            # Get file stats\n            stats = os.stat(item_path)\n            \n            # File size in bytes\n            file_size = stats.st_size\n            \n            # Creation time (on Windows) or last metadata change time (on Unix)\n            # Convert to UTC datetime and then to ISO format\n            creation_time = datetime.fromtimestamp(stats.st_ctime, tz=timezone.utc).isoformat()\n            \n            # Modification time\n            # Convert to UTC datetime and then to ISO format\n            modification_time = datetime.fromtimestamp(stats.st_mtime, tz=timezone.utc).isoformat()\n            \n            # Add tuple to result list\n            result.append((item, file_size, creation_time, modification_time))\n    \n    return result",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/945": [
      {
        "task_id": "BigCodeBench/945",
        "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n\n    pass\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    \"\"\"\n    Generates a time series of sales data starting from a specified date, then uses linear regression \n    to forecast future sales based on the provided or generated sales data.\n    \n    Parameters:\n    - start_date: str, starting date for the time series (default: '2016-01-01')\n    - periods: int, number of periods to generate (default: 13)\n    - freq: str, frequency of the time series (default: 'WOM-2FRI' - WeekOfMonth-2nd Friday)\n    - sales_data: array-like or None, if provided, uses this data instead of generating random data\n    \n    Returns:\n    - numpy array containing the forecasted future sales for the same number of periods as the input data\n    \"\"\"\n    \n    # Generate date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate or use provided sales data\n    if sales_data is None:\n        # Generate random sales data with an upward trend and some noise\n        np.random.seed(42)  # For reproducibility\n        trend = np.linspace(100, 200, periods)\n        noise = np.random.normal(0, 20, periods)\n        sales_data = trend + noise\n    else:\n        sales_data = np.array(sales_data)\n        if len(sales_data) != periods:\n            raise ValueError(f\"sales_data length ({len(sales_data)}) must match periods ({periods})\")\n    \n    # Prepare data for linear regression\n    # X: time index (0, 1, 2, ..., periods-1)\n    # y: sales data\n    X = np.arange(periods).reshape(-1, 1)\n    y = sales_data\n    \n    # Train linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Generate future time indices for prediction\n    # Predict for the same number of periods into the future\n    future_X = np.arange(periods, 2 * periods).reshape(-1, 1)\n    \n    # Make predictions\n    future_sales = model.predict(future_X)\n    \n    return future_sales",
        "status": "fail",
        "details": {
          "test_forecasted_values_increasing": "Traceback (most recent call last):\n  File \"__test__.py\", line 83, in test_forecasted_values_increasing\nAssertionError: True is not false\n"
        }
      }
    ],
    "BigCodeBench/955": [
      {
        "task_id": "BigCodeBench/955",
        "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\ndef task_func(mystrings, text):\n\n    pass\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\ndef task_func(mystrings, text):\n    \"\"\"\n    Replace spaces in given words with underscores, then plots the frequency of each unique word.\n    \n    Parameters:\n    mystrings (list): List of strings where spaces should be replaced with underscores\n    text (str): Text to analyze for word frequencies\n    \n    Returns:\n    matplotlib.axes.Axes: The Axes object of the plot\n    \n    Raises:\n    ValueError: If the input text is empty\n    \"\"\"\n    # Check if text is empty\n    if not text or text.strip() == \"\":\n        raise ValueError(\"If the input text is empty.\")\n    \n    # Create a dictionary to store the modified strings\n    replacements = {}\n    for string in mystrings:\n        # Replace spaces with underscores (case-insensitive)\n        modified = string.replace(\" \", \"_\")\n        replacements[string.lower()] = modified.lower()\n    \n    # Convert text to lowercase for case-insensitive operations\n    text_lower = text.lower()\n    \n    # Replace the strings in the text\n    for original, modified in replacements.items():\n        # Use word boundaries to ensure we're replacing whole words/phrases\n        pattern = r'\\b' + re.escape(original) + r'\\b'\n        text_lower = re.sub(pattern, modified, text_lower)\n    \n    # Extract all words from the modified text\n    words = re.findall(r'\\b\\w+\\b', text_lower)\n    \n    # Count word frequencies while preserving order of first appearance\n    word_freq = Counter()\n    unique_words_ordered = []\n    \n    for word in words:\n        if word not in word_freq:\n            unique_words_ordered.append(word)\n        word_freq[word] += 1\n    \n    # Prepare data for plotting\n    x_labels = unique_words_ordered\n    frequencies = [word_freq[word] for word in unique_words_ordered]\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    x_positions = np.arange(len(x_labels))\n    \n    ax.bar(x_positions, frequencies)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Word Frequency Plot')\n    ax.set_xticks(x_positions)\n    ax.set_xticklabels(x_labels, rotation=45, ha='right')\n    \n    # Adjust layout to prevent label cutoff\n    plt.tight_layout()\n    \n    return ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 86, in test_case_1\nAssertionError: False is not true\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 92, in test_case_2\nAssertionError: 'hello_world' != 'hello_world!'\n- hello_world\n+ hello_world!\n?            +\n\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 99, in test_case_3\nAssertionError: False is not true\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 108, in test_case_4\nAssertionError: False is not true\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 113, in test_case_5\nAssertionError: 'hello_world!' not found in ['hello_world']\n",
          "test_case_6": "Traceback (most recent call last):\n  File \"__test__.py\", line 120, in test_case_6\nAssertionError: 'Hello_World!' not found in ['hello_world']\n",
          "test_case_9": "Traceback (most recent call last):\n  File \"__test__.py\", line 138, in test_case_9\nAssertionError: '#$%!' not found in ['test_123', 'is', 'fun']\n"
        }
      }
    ],
    "BigCodeBench/942": [
      {
        "task_id": "BigCodeBench/942",
        "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n\n    pass\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    \"\"\"\n    Create and visualize a sales report for different categories over a period of time.\n    \n    Returns:\n        DataFrame: A DataFrame containing the sales data with columns 'Date', 'Category', 'Sales'\n        Axes: The Matplotlib Axes object for the plot\n    \"\"\"\n    # Generate date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create empty lists to store data\n    all_dates = []\n    all_categories = []\n    all_sales = []\n    \n    # Generate sales data for each category\n    np.random.seed(42)  # For reproducibility\n    \n    for category in categories:\n        # Generate random sales data with some trend and seasonality\n        base_sales = np.random.randint(1000, 5000)\n        trend = np.linspace(0, 500, periods)\n        seasonality = 1000 * np.sin(np.linspace(0, 4*np.pi, periods))\n        noise = np.random.normal(0, 200, periods)\n        \n        sales = base_sales + trend + seasonality + noise\n        sales = np.maximum(sales, 0)  # Ensure no negative sales\n        \n        # Append data\n        all_dates.extend(dates)\n        all_categories.extend([category] * periods)\n        all_sales.extend(sales)\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'Date': all_dates,\n        'Category': all_categories,\n        'Sales': all_sales\n    })\n    \n    # Create visualization\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    # Plot each category\n    for category in categories:\n        category_data = df[df['Category'] == category]\n        ax.plot(category_data['Date'], category_data['Sales'], \n                marker='o', label=category, linewidth=2, markersize=6)\n    \n    # Customize the plot\n    ax.set_xlabel('Date', fontsize=12)\n    ax.set_ylabel('Sales ($)', fontsize=12)\n    ax.set_title('Sales Report by Category Over Time', fontsize=14, fontweight='bold')\n    ax.legend(loc='best')\n    ax.grid(True, alpha=0.3)\n    \n    # Format y-axis to show currency\n    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:,.0f}'))\n    \n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45, ha='right')\n    \n    # Adjust layout to prevent label cutoff\n    plt.tight_layout()\n    \n    return df, ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 96, in test_case_1\nAssertionError: 'Sales Report by Category Over Time' != 'Category-wise Sales Trends'\n- Sales Report by Category Over Time\n+ Category-wise Sales Trends\n\n"
        }
      }
    ],
    "BigCodeBench/969": [
      {
        "task_id": "BigCodeBench/969",
        "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n\n    pass\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Computes the MinMax-normalized cumulative sum for each numeric column in the given DataFrame.\n    \n    Args:\n        df: Input DataFrame\n        \n    Returns:\n        pd.DataFrame: A DataFrame where each column contains the normalized cumulative sum of the\n        respective column in the input DataFrame, retaining the original column names.\n        \n    Raises:\n        TypeError: If the DataFrame contains non-numeric data types.\n        ValueError: If the DataFrame is empty or contains NaN values.\n    \"\"\"\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    # Check for non-numeric data types\n    if not all(df.dtypes.apply(lambda x: np.issubdtype(x, np.number))):\n        raise TypeError(\"DataFrame contains non-numeric data types\")\n    \n    # Check for NaN values\n    if df.isnull().any().any():\n        raise ValueError(\"DataFrame contains NaN values\")\n    \n    # Create a copy to avoid modifying the original DataFrame\n    result_df = pd.DataFrame(index=df.index, columns=df.columns)\n    \n    # Process each column\n    for col in df.columns:\n        # Compute cumulative sum\n        cumsum = df[col].cumsum()\n        \n        # Apply MinMax normalization\n        # Reshape to 2D array as required by MinMaxScaler\n        cumsum_reshaped = cumsum.values.reshape(-1, 1)\n        \n        # Create and fit the scaler\n        scaler = MinMaxScaler()\n        normalized = scaler.fit_transform(cumsum_reshaped)\n        \n        # Store the result\n        result_df[col] = normalized.flatten()\n    \n    return result_df",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/964": [
      {
        "task_id": "BigCodeBench/964",
        "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\ndef task_func(source_directory: str, target_directory: str) -> int:\n\n    pass\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    Converts files with specific extensions (.txt, .docx, .xlsx, .csv) from a source directory \n    to CSV files and saves them in a target directory.\n    \n    Args:\n        source_directory: Path to the source directory containing files to convert\n        target_directory: Path to the target directory where CSV files will be saved\n        \n    Returns:\n        int: The number of files successfully converted to CSV\n        \n    Raises:\n        FileNotFoundError: If the source directory does not exist\n    \"\"\"\n    # Check if source directory exists\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"Source directory '{source_directory}' does not exist\")\n    \n    # Create target directory if it doesn't exist\n    os.makedirs(target_directory, exist_ok=True)\n    \n    # Define supported extensions\n    supported_extensions = {'.txt', '.docx', '.xlsx', '.csv'}\n    \n    # Counter for successfully converted files\n    converted_count = 0\n    \n    # Process each file in the source directory\n    for file_path in Path(source_directory).iterdir():\n        if file_path.is_file() and file_path.suffix.lower() in supported_extensions:\n            try:\n                # Extract text content based on file type\n                text_content = []\n                \n                if file_path.suffix.lower() == '.txt':\n                    # Read text file\n                    with open(file_path, 'r', encoding='utf-8') as f:\n                        text_content = [f.read()]\n                \n                elif file_path.suffix.lower() == '.docx':\n                    # Read Word document\n                    doc = docx.Document(file_path)\n                    full_text = []\n                    for paragraph in doc.paragraphs:\n                        full_text.append(paragraph.text)\n                    text_content = ['\\n'.join(full_text)]\n                \n                elif file_path.suffix.lower() == '.xlsx':\n                    # Read Excel file\n                    df = pd.read_excel(file_path)\n                    # Convert all cells to string and join them\n                    all_text = []\n                    for col in df.columns:\n                        all_text.extend(df[col].astype(str).tolist())\n                    text_content = [' '.join(all_text)]\n                \n                elif file_path.suffix.lower() == '.csv':\n                    # Read CSV file\n                    df = pd.read_csv(file_path)\n                    # Convert all cells to string and join them\n                    all_text = []\n                    for col in df.columns:\n                        all_text.extend(df[col].astype(str).tolist())\n                    text_content = [' '.join(all_text)]\n                \n                # Create DataFrame with single 'Text' column\n                output_df = pd.DataFrame({'Text': text_content})\n                \n                # Save to CSV in target directory\n                output_filename = file_path.stem + '.csv'\n                output_path = os.path.join(target_directory, output_filename)\n                output_df.to_csv(output_path, index=False)\n                \n                converted_count += 1\n                \n            except Exception as e:\n                # Skip files that cannot be processed\n                print(f\"Error processing {file_path}: {str(e)}\")\n                continue\n    \n    return converted_count",
        "status": "fail",
        "details": {
          "test_case_9": "Traceback (most recent call last):\n  File \"__test__.py\", line 182, in test_case_9\nAssertionError: 0 != 1\n"
        }
      }
    ],
    "BigCodeBench/657": [
      {
        "task_id": "BigCodeBench/657",
        "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(texts, stopwords=None):\n\n    pass\nimport re\nimport nltk\nfrom gensim.models import Word2Vec\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    Generate word vectors from a list of texts using the gensim Word2Vec model.\n    \n    Args:\n        texts: List of text strings to process\n        stopwords: Optional list of stopwords. If None, uses NLTK English stopwords\n        \n    Returns:\n        Word2Vec: A trained Word2Vec model\n    \"\"\"\n    # Download stopwords if not already present\n    try:\n        nltk.data.find('corpora/stopwords')\n    except LookupError:\n        nltk.download('stopwords')\n    \n    # Use NLTK stopwords if none provided\n    if stopwords is None:\n        from nltk.corpus import stopwords as nltk_stopwords\n        stopwords = set(nltk_stopwords.words('english'))\n    else:\n        stopwords = set(stopwords)\n    \n    # Process texts\n    processed_texts = []\n    \n    for text in texts:\n        # Remove non-alphanumeric characters except spaces\n        cleaned_text = ALPHANUMERIC.sub(' ', text)\n        \n        # Convert to lowercase\n        cleaned_text = cleaned_text.lower()\n        \n        # Split into words\n        words = cleaned_text.split()\n        \n        # Remove stopwords\n        words = [word for word in words if word not in stopwords]\n        \n        # Add to processed texts if not empty\n        if words:\n            processed_texts.append(words)\n    \n    # Train Word2Vec model\n    # Using default parameters, but you can adjust these as needed\n    model = Word2Vec(\n        sentences=processed_texts,\n        vector_size=100,  # Size of word vectors\n        window=5,         # Context window size\n        min_count=1,      # Minimum word frequency\n        workers=4,        # Number of worker threads\n        sg=0             # Use CBOW (0) or Skip-gram (1)\n    )\n    \n    return model",
        "status": "fail",
        "details": {
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 87, in test_case_3\n  File \"__test__.py\", line 59, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/gensim/models/word2vec.py\", line 430, in __init__\n    self.train(\n  File \"/usr/local/lib/python3.10/site-packages/gensim/models/word2vec.py\", line 1045, in train\n    self._check_training_sanity(epochs=epochs, total_examples=total_examples, total_words=total_words)\n  File \"/usr/local/lib/python3.10/site-packages/gensim/models/word2vec.py\", line 1554, in _check_training_sanity\n    raise RuntimeError(\"you must first build vocabulary before training the model\")\nRuntimeError: you must first build vocabulary before training the model\n"
        }
      }
    ],
    "BigCodeBench/985": [
      {
        "task_id": "BigCodeBench/985",
        "solution": "import pandas as pd\nimport json\nimport os\nimport math\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n\n    pass\nimport pandas as pd\nimport json\nimport os\nimport math\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    \"\"\"\n    Generates a population report DataFrame and CSV file based on provided JSON data.\n    \n    Args:\n        json_data: JSON string containing country-population data\n        output_dir: Directory to save the CSV file (default: current directory)\n        file_name: Name of the output CSV file (default: \"country_population_report.csv\")\n    \n    Returns:\n        str: The file path of the generated CSV report.\n        pd.DataFrame: The country-population data loaded from the input JSON, with columns:\n        \"Country\", \"Population\".\n    \n    Raises:\n        ValueError: If the JSON data is malformed, empty, contains non-string country names,\n                   non-numeric or negative populations.\n        IOError: If the file cannot be written to the specified directory.\n    \"\"\"\n    \n    # Parse JSON data\n    try:\n        data = json.loads(json_data)\n    except (json.JSONDecodeError, TypeError) as e:\n        raise ValueError(f\"Malformed JSON data: {e}\")\n    \n    # Check if data is empty\n    if not data:\n        raise ValueError(\"JSON data is empty\")\n    \n    # Validate data structure\n    if not isinstance(data, (list, dict)):\n        raise ValueError(\"JSON data must be a list or dictionary\")\n    \n    # Convert to list of dictionaries if needed\n    if isinstance(data, dict):\n        # Assume dict format is {\"country1\": population1, \"country2\": population2, ...}\n        data_list = [{\"Country\": k, \"Population\": v} for k, v in data.items()]\n    else:\n        data_list = data\n    \n    # Validate and process each entry\n    processed_data = []\n    for i, entry in enumerate(data_list):\n        if not isinstance(entry, dict):\n            raise ValueError(f\"Entry at index {i} is not a dictionary\")\n        \n        # Check for required fields\n        if \"Country\" not in entry or \"Population\" not in entry:\n            raise ValueError(f\"Entry at index {i} missing 'Country' or 'Population' field\")\n        \n        country = entry[\"Country\"]\n        population = entry[\"Population\"]\n        \n        # Validate country name\n        if not isinstance(country, str):\n            raise ValueError(f\"Country name must be a string, got {type(country).__name__} for entry {i}\")\n        \n        # Validate population\n        try:\n            pop_float = float(population)\n        except (TypeError, ValueError):\n            raise ValueError(f\"Population must be numeric, got {type(population).__name__} for country '{country}'\")\n        \n        if pop_float < 0:\n            raise ValueError(f\"Population cannot be negative for country '{country}'\")\n        \n        # Round down float populations\n        pop_int = int(math.floor(pop_float))\n        \n        processed_data.append({\n            \"Country\": country,\n            \"Population\": pop_int\n        })\n    \n    # Create DataFrame\n    df = pd.DataFrame(processed_data, columns=[\"Country\", \"Population\"])\n    \n    # Prepare file path\n    file_path = os.path.join(output_dir, file_name)\n    \n    # Write to CSV\n    try:\n        # Create directory if it doesn't exist\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Save DataFrame to CSV without index\n        df.to_csv(file_path, index=False)\n    except Exception as e:\n        raise IOError(f\"Cannot write file to {file_path}: {e}\")\n    \n    return file_path, df",
        "status": "fail",
        "details": {
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 140, in test_case_3\nAssertionError: ValueError not raised\n",
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 72, in task_func\nTypeError: float() argument must be a string or a real number, not 'dict'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 122, in test_case_1\n  File \"__test__.py\", line 74, in task_func\nValueError: Population must be numeric, got dict for country 'Countries'\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 72, in task_func\nTypeError: float() argument must be a string or a real number, not 'dict'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 151, in test_case_4\n  File \"__test__.py\", line 74, in task_func\nValueError: Population must be numeric, got dict for country 'Countries'\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 72, in task_func\nTypeError: float() argument must be a string or a real number, not 'dict'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 159, in test_case_5\n  File \"__test__.py\", line 74, in task_func\nValueError: Population must be numeric, got dict for country 'Countries'\n",
          "test_case_6": "Traceback (most recent call last):\n  File \"__test__.py\", line 72, in task_func\nTypeError: float() argument must be a string or a real number, not 'dict'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 167, in test_case_6\n  File \"__test__.py\", line 74, in task_func\nValueError: Population must be numeric, got dict for country 'Countries'\n",
          "test_case_8": "Traceback (most recent call last):\n  File \"__test__.py\", line 72, in task_func\nTypeError: float() argument must be a string or a real number, not 'dict'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 191, in test_case_8\n  File \"__test__.py\", line 74, in task_func\nValueError: Population must be numeric, got dict for country 'Countries'\n",
          "test_case_9": "Traceback (most recent call last):\n  File \"__test__.py\", line 72, in task_func\nTypeError: float() argument must be a string or a real number, not 'dict'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 201, in test_case_9\n  File \"__test__.py\", line 74, in task_func\nValueError: Population must be numeric, got dict for country 'Countries'\n"
        }
      }
    ],
    "BigCodeBench/990": [
      {
        "task_id": "BigCodeBench/990",
        "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\ndef task_func(hex_string):\n\n    pass\nimport binascii\nimport base64\nimport urllib.parse\nimport codecs\ndef task_func(hex_string):\n    \"\"\"\n    Convert a hexadecimal string to various encodings.\n    \n    This function takes a hexadecimal string as input and performs several encoding operations.\n    Initially, it decodes the hexadecimal string to bytes and then converts these bytes into a UTF-8 string.\n    This UTF-8 string is subsequently encoded into different formats: hexadecimal, base64, UTF-8, UTF-16,\n    UTF-32, ASCII (if possible), URL encoding, and ROT13.\n    \n    Args:\n        hex_string (str): A hexadecimal string to be converted\n        \n    Returns:\n        dict: A dictionary containing the input string encoded in various formats\n    \"\"\"\n    # Convert hex string to bytes\n    byte_data = binascii.unhexlify(hex_string)\n    \n    # Decode bytes to UTF-8 string\n    utf8_string = byte_data.decode('utf-8')\n    \n    # Initialize result dictionary\n    result = {}\n    \n    # Hex encoding (back to hex)\n    result['hex'] = binascii.hexlify(utf8_string.encode('utf-8')).decode('ascii')\n    \n    # Base64 encoding\n    result['base64'] = base64.b64encode(utf8_string.encode('utf-8')).decode('ascii')\n    \n    # UTF-8 (the string itself)\n    result['utf-8'] = utf8_string\n    \n    # UTF-16 (decode the encoded bytes back to string for display)\n    result['utf-16'] = utf8_string\n    \n    # UTF-32 (decode the encoded bytes back to string for display)\n    result['utf-32'] = utf8_string\n    \n    # ASCII encoding\n    try:\n        # Try to encode as ASCII\n        utf8_string.encode('ascii')\n        result['ASCII'] = utf8_string\n    except UnicodeEncodeError:\n        result['ASCII'] = 'Not representable in ASCII'\n    \n    # URL encoding\n    result['URL'] = urllib.parse.quote(utf8_string, safe='')\n    \n    # ROT13 encoding\n    result['ROT13'] = codecs.encode(utf8_string, 'rot_13')\n    \n    return result",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/988": [
      {
        "task_id": "BigCodeBench/988",
        "solution": "import os\nimport re\nfrom pathlib import Path\ndef task_func(dir_path: str, predicates: list) -> dict:\n\n    pass\nimport os\nimport re\nfrom pathlib import Path\ndef task_func(dir_path: str, predicates: list) -> dict:\n    \"\"\"\n    Evaluates each item (files and directories) in a given directory against specified conditions.\n    \n    Args:\n        dir_path: Path to the directory to evaluate\n        predicates: List of predicate functions to apply to each item\n        \n    Returns:\n        dict: A dictionary with directory items as keys and the results of condition checks as values.\n        \n    Raises:\n        ValueError: If no valid predicates are provided.\n        FileNotFoundError: If the specified directory does not exist or is not a directory.\n    \"\"\"\n    # Check if directory exists and is a directory\n    path = Path(dir_path)\n    if not path.exists():\n        raise FileNotFoundError(f\"The specified directory does not exist: {dir_path}\")\n    if not path.is_dir():\n        raise FileNotFoundError(f\"The specified path is not a directory: {dir_path}\")\n    \n    # Deduplicate predicates while preserving order\n    seen = set()\n    unique_predicates = []\n    for pred in predicates:\n        if pred not in seen and callable(pred):\n            seen.add(pred)\n            unique_predicates.append(pred)\n    \n    # Check if we have valid predicates\n    if not unique_predicates:\n        raise ValueError(\"No valid predicates are provided\")\n    \n    # Get all items in the directory (files and directories)\n    items = os.listdir(dir_path)\n    \n    # Evaluate each item against all predicates\n    results = {}\n    for item in items:\n        item_results = []\n        for predicate in unique_predicates:\n            try:\n                # Apply predicate to the item name (not full path)\n                result = predicate(item)\n                item_results.append(result)\n            except Exception:\n                # If predicate fails, consider it as False\n                item_results.append(False)\n        \n        results[item] = item_results\n    \n    return results",
        "status": "fail",
        "details": {
          "test_deduplicate_predicates": "Traceback (most recent call last):\n  File \"__test__.py\", line 170, in test_deduplicate_predicates\n  File \"__test__.py\", line 42, in task_func\nValueError: No valid predicates are provided\n",
          "test_dir_is_dir": "Traceback (most recent call last):\n  File \"__test__.py\", line 119, in test_dir_is_dir\n  File \"__test__.py\", line 42, in task_func\nValueError: No valid predicates are provided\n",
          "test_dir_is_not_file": "Traceback (most recent call last):\n  File \"__test__.py\", line 127, in test_dir_is_not_file\n  File \"__test__.py\", line 42, in task_func\nValueError: No valid predicates are provided\n",
          "test_file_is_file": "Traceback (most recent call last):\n  File \"__test__.py\", line 103, in test_file_is_file\n  File \"__test__.py\", line 42, in task_func\nValueError: No valid predicates are provided\n",
          "test_file_is_not_dir": "Traceback (most recent call last):\n  File \"__test__.py\", line 111, in test_file_is_not_dir\n  File \"__test__.py\", line 42, in task_func\nValueError: No valid predicates are provided\n",
          "test_has_no_special_char": "Traceback (most recent call last):\n  File \"__test__.py\", line 145, in test_has_no_special_char\n  File \"__test__.py\", line 42, in task_func\nValueError: No valid predicates are provided\n",
          "test_has_numbers": "Traceback (most recent call last):\n  File \"__test__.py\", line 154, in test_has_numbers\n  File \"__test__.py\", line 42, in task_func\nValueError: No valid predicates are provided\n",
          "test_has_special_char": "Traceback (most recent call last):\n  File \"__test__.py\", line 136, in test_has_special_char\n  File \"__test__.py\", line 42, in task_func\nValueError: No valid predicates are provided\n",
          "test_multiple_predicates": "Traceback (most recent call last):\n  File \"__test__.py\", line 161, in test_multiple_predicates\n  File \"__test__.py\", line 42, in task_func\nValueError: No valid predicates are provided\n"
        }
      }
    ],
    "BigCodeBench/998": [
      {
        "task_id": "BigCodeBench/998",
        "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\ndef task_func(url):\n\n    pass\nimport urllib.request\nimport os\nimport hashlib\nimport tarfile\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\ndef task_func(url):\n    \"\"\"\n    Downloads a tar.gz file from a specified URL, then validates its MD5 checksum against a predefined expected value.\n    If the checksum matches, it extracts the contents of the tar.gz file. Otherwise, it deletes the downloaded file.\n    \n    Args:\n        url (str): The URL of the tar.gz file to download\n        \n    Returns:\n        bool: Returns True if the file is successfully downloaded, its MD5 checksum matches the expected value, and\n        it is extracted. Returns False if the checksum does not match the expected value or if the download fails.\n    \"\"\"\n    try:\n        # Download the file\n        print(f\"Downloading file from {url}...\")\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n        print(f\"File downloaded successfully as {TARGET_TAR_FILE}\")\n        \n        # Calculate MD5 checksum of the downloaded file\n        md5_hash = hashlib.md5()\n        with open(TARGET_TAR_FILE, \"rb\") as f:\n            # Read the file in chunks to handle large files efficiently\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                md5_hash.update(chunk)\n        \n        calculated_checksum = md5_hash.hexdigest()\n        print(f\"Calculated MD5 checksum: {calculated_checksum}\")\n        print(f\"Expected MD5 checksum: {EXPECTED_MD5_CHECKSUM}\")\n        \n        # Validate checksum\n        if calculated_checksum == EXPECTED_MD5_CHECKSUM:\n            print(\"Checksum matches! Extracting the tar.gz file...\")\n            \n            # Extract the tar.gz file\n            with tarfile.open(TARGET_TAR_FILE, 'r:gz') as tar:\n                # Extract to current directory\n                tar.extractall()\n                print(\"File extracted successfully!\")\n            \n            return True\n        else:\n            print(\"Checksum does not match! Deleting the downloaded file...\")\n            os.remove(TARGET_TAR_FILE)\n            print(\"Downloaded file deleted.\")\n            return False\n            \n    except urllib.error.URLError as e:\n        print(f\"Failed to download file: {e}\")\n        # Clean up if file was partially downloaded\n        if os.path.exists(TARGET_TAR_FILE):\n            os.remove(TARGET_TAR_FILE)\n        return False\n    except tarfile.TarError as e:\n        print(f\"Failed to extract tar file: {e}\")\n        # Clean up the tar file if extraction failed\n        if os.path.exists(TARGET_TAR_FILE):\n            os.remove(TARGET_TAR_FILE)\n        return False\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        # Clean up if file exists\n        if os.path.exists(TARGET_TAR_FILE):\n            os.remove(TARGET_TAR_FILE)\n        return False",
        "status": "fail",
        "details": {
          "test_valid_file": "Traceback (most recent call last):\n  File \"__test__.py\", line 105, in test_valid_file\nAssertionError: True is not false\n"
        }
      }
    ],
    "BigCodeBench/999": [
      {
        "task_id": "BigCodeBench/999",
        "solution": "import urllib.request\nimport os\nimport csv\nimport collections\ndef task_func(url, column_name, csv_file_path):\n\n    pass\nimport urllib.request\nimport os\nimport csv\nimport collections\ndef task_func(url, column_name, csv_file_path):\n    \"\"\"\n    Download a CSV file from a given URL, save it to a specified path, and count \n    the occurrences of each value in a particular column.\n    \n    Args:\n        url (str): The URL of the CSV file to download\n        column_name (str): The name of the column to count values from\n        csv_file_path (str): The local path where the CSV file will be saved\n    \n    Returns:\n        dict: A dictionary mapping the values from the specified column to their\n        corresponding occurrence counts.\n    \n    Raises:\n        ValueError: If the specified column_name does not exist in the CSV file, \n        the function will delete the downloaded file and raise a ValueError with \n        a message stating \"The provided column_name '{column_name}' does not exist \n        in the CSV file.\"\n    \"\"\"\n    try:\n        # Download the CSV file\n        urllib.request.urlretrieve(url, csv_file_path)\n        \n        # Initialize a counter for the column values\n        value_counts = collections.Counter()\n        \n        # Read the CSV file\n        with open(csv_file_path, 'r', encoding='utf-8') as csvfile:\n            reader = csv.DictReader(csvfile)\n            \n            # Check if the column exists\n            if reader.fieldnames and column_name not in reader.fieldnames:\n                # Delete the file and raise ValueError\n                os.remove(csv_file_path)\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n            \n            # Count occurrences of each value in the specified column\n            for row in reader:\n                if column_name in row:\n                    value = row[column_name]\n                    value_counts[value] += 1\n        \n        # Delete the CSV file after processing\n        os.remove(csv_file_path)\n        \n        # Return the counts as a regular dictionary\n        return dict(value_counts)\n        \n    except Exception as e:\n        # If any error occurs during download or processing, ensure file is deleted\n        if os.path.exists(csv_file_path):\n            os.remove(csv_file_path)\n        \n        # Re-raise the exception\n        raise e",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/785": [
      {
        "task_id": "BigCodeBench/785",
        "solution": "import subprocess\nimport os\nimport glob\n# Constants\nARCHIVE_DIR = '/tmp/archive'\ndef task_func(pattern):\n\n    pass\nimport subprocess\nimport os\nimport glob\nARCHIVE_DIR = '/tmp/archive'\ndef task_func(pattern):\n    \"\"\"\n    Archive all files that match a particular pattern and then delete the original files.\n    \n    Args:\n        pattern (str): File pattern to match (e.g., '*.txt', '/path/to/*.log')\n    \n    Returns:\n        archive_file (str): The archive file path.\n    \"\"\"\n    # Find all files matching the pattern\n    files_to_archive = glob.glob(pattern)\n    \n    if not files_to_archive:\n        raise ValueError(f\"No files found matching pattern: {pattern}\")\n    \n    # Create archive directory if it doesn't exist\n    os.makedirs(ARCHIVE_DIR, exist_ok=True)\n    \n    # Generate archive filename based on pattern\n    # Clean the pattern to create a valid filename\n    pattern_clean = pattern.replace('/', '_').replace('*', 'all').replace('.', '_')\n    archive_name = f\"archive_{pattern_clean}_{os.getpid()}.tar.gz\"\n    archive_file = os.path.join(ARCHIVE_DIR, archive_name)\n    \n    # Create tar command\n    tar_command = ['tar', '-czf', archive_file] + files_to_archive\n    \n    try:\n        # Execute tar command to create archive\n        subprocess.run(tar_command, check=True, capture_output=True, text=True)\n        \n        # Delete original files after successful archiving\n        for file_path in files_to_archive:\n            try:\n                os.remove(file_path)\n            except OSError as e:\n                print(f\"Warning: Could not delete {file_path}: {e}\")\n        \n        return archive_file\n        \n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to create archive: {e.stderr}\")\n    except Exception as e:\n        raise RuntimeError(f\"An error occurred: {str(e)}\")",
        "status": "fail",
        "details": {
          "test_archiving_in_nonexistent_directory": "Traceback (most recent call last):\n  File \"__test__.py\", line 79, in tearDown\n  File \"/usr/local/lib/python3.10/shutil.py\", line 715, in rmtree\n    onerror(os.lstat, path, sys.exc_info())\n  File \"/usr/local/lib/python3.10/shutil.py\", line 713, in rmtree\n    orig_st = os.lstat(path)\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/archive'\n",
          "test_multiple_archiving_unique_naming": "Traceback (most recent call last):\n  File \"__test__.py\", line 101, in test_multiple_archiving_unique_naming\n  File \"__test__.py\", line 27, in task_func\nValueError: No files found matching pattern: ./test_files/*.txt\n",
          "test_no_matching_files": "Traceback (most recent call last):\n  File \"__test__.py\", line 79, in tearDown\n  File \"/usr/local/lib/python3.10/shutil.py\", line 715, in rmtree\n    onerror(os.lstat, path, sys.exc_info())\n  File \"/usr/local/lib/python3.10/shutil.py\", line 713, in rmtree\n    orig_st = os.lstat(path)\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/archive'\n"
        }
      }
    ],
    "BigCodeBench/1003": [
      {
        "task_id": "BigCodeBench/1003",
        "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\ndef task_func(url):\n\n    pass\nimport urllib.request\nfrom lxml import etree\nimport pandas as pd\ndef task_func(url):\n    \"\"\"\n    Fetches and parses an XML file from a specified URL, then converts it into a Pandas DataFrame.\n    \n    Parameters:\n    url (str): The URL of the XML file to fetch and parse.\n    \n    Returns:\n    pandas.DataFrame: A DataFrame constructed from the parsed XML data. Each row of the DataFrame \n                     corresponds to an 'item' element in the XML file, with child elements of 'item' \n                     becoming columns in the DataFrame.\n    \n    Raises:\n    ValueError: This error is raised in several scenarios:\n               1. If the URL is invalid or the XML file cannot be fetched from the URL.\n               2. If the XML file has invalid syntax.\n               3. If the XML structure does not conform to the expected format.\n    \"\"\"\n    try:\n        # Fetch the XML content from the URL\n        with urllib.request.urlopen(url) as response:\n            xml_content = response.read()\n    except Exception as e:\n        raise ValueError(f\"Failed to fetch XML from URL: {str(e)}\")\n    \n    try:\n        # Parse the XML content\n        root = etree.fromstring(xml_content)\n    except etree.XMLSyntaxError as e:\n        raise ValueError(f\"Invalid XML syntax: {str(e)}\")\n    except Exception as e:\n        raise ValueError(f\"Failed to parse XML: {str(e)}\")\n    \n    # Find all 'item' elements\n    items = root.findall('.//item')\n    \n    if not items:\n        raise ValueError(\"XML structure does not match expected format: no 'item' elements found\")\n    \n    # Extract data from items\n    data = []\n    columns = set()\n    \n    for item in items:\n        row_data = {}\n        for child in item:\n            row_data[child.tag] = child.text\n            columns.add(child.tag)\n        data.append(row_data)\n    \n    if not data:\n        raise ValueError(\"XML structure does not match expected format: no data extracted from items\")\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    \n    return df",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/1006": [
      {
        "task_id": "BigCodeBench/1006",
        "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n\n    pass\nimport os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    \"\"\"\n    Downloads and extracts a ZIP file from a specified URL to a given directory.\n    \n    Args:\n        url (str): The URL of the ZIP file to download\n        download_path (str): The directory path where the ZIP file will be extracted\n        \n    Returns:\n        str: Path to the directory containing the extracted contents. If an error occurs,\n        a descriptive message is returned. The message starts with \"Error: \".\n    \"\"\"\n    try:\n        # Create download directory if it doesn't exist\n        os.makedirs(download_path, exist_ok=True)\n        \n        # Download the file\n        try:\n            response = requests.get(url, timeout=30)\n            response.raise_for_status()  # Raise an exception for bad status codes\n        except (requests.RequestException, requests.ConnectionError, requests.Timeout) as e:\n            return \"Error: Unable to download the file from the provided URL.\"\n        \n        # Check if the content type is ZIP\n        content_type = response.headers.get('Content-Type', '')\n        if 'application/zip' not in content_type and not url.lower().endswith('.zip'):\n            # Additional check: try to see if it's actually a ZIP file by checking magic bytes\n            if len(response.content) < 4 or response.content[:2] != b'PK':\n                return \"Error: The URL does not point to a ZIP file.\"\n        \n        # Save the downloaded content to a temporary file\n        temp_zip_path = os.path.join(download_path, 'temp_download.zip')\n        try:\n            with open(temp_zip_path, 'wb') as f:\n                f.write(response.content)\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n        \n        # Try to extract the ZIP file\n        try:\n            with ZipFile(temp_zip_path, 'r') as zip_file:\n                # Extract all contents to the download path\n                zip_file.extractall(download_path)\n        except BadZipFile:\n            # Clean up the temporary file\n            if os.path.exists(temp_zip_path):\n                os.remove(temp_zip_path)\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n        except Exception as e:\n            # Clean up the temporary file\n            if os.path.exists(temp_zip_path):\n                os.remove(temp_zip_path)\n            return f\"Error: {str(e)}\"\n        \n        # Clean up the temporary ZIP file after successful extraction\n        if os.path.exists(temp_zip_path):\n            os.remove(temp_zip_path)\n        \n        # Return the path to the extracted contents\n        return os.path.abspath(download_path)\n        \n    except Exception as e:\n        # Catch any other unexpected exceptions\n        return f\"Error: {str(e)}\"",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/1008": [
      {
        "task_id": "BigCodeBench/1008",
        "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\ndef task_func(url, table_id):\n\n    pass\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\ndef task_func(url, table_id):\n    \"\"\"\n    Extracts and converts data from a specified HTML table based on the given 'table_id' \n    on a webpage into a Pandas DataFrame.\n    \n    Args:\n        url (str): The URL of the webpage containing the table\n        table_id (str): The ID of the HTML table to extract\n        \n    Returns:\n        df (pd.DataFrame): A DataFrame containing the data extracted from the specified HTML table.\n        If the table is found but has no rows (<tr> elements), an empty DataFrame is returned.\n        \n    Raises:\n        requests.exceptions.HTTPError: If the HTTP request fails\n        ValueError: If no table with the specified 'table_id' is found on the webpage\n    \"\"\"\n    # Make HTTP request\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raises HTTPError for bad status codes\n    except requests.exceptions.RequestException as e:\n        # Re-raise as HTTPError for consistency with requirements\n        if isinstance(e, requests.exceptions.HTTPError):\n            raise\n        else:\n            raise requests.exceptions.HTTPError(f\"HTTP request failed: {str(e)}\")\n    \n    # Parse HTML content\n    soup = BeautifulSoup(response.content, 'html.parser')\n    \n    # Find table with specified ID\n    table = soup.find('table', {'id': table_id})\n    \n    if table is None:\n        raise ValueError(\"Table with the specified ID not found.\")\n    \n    # Check if table has any rows\n    rows = table.find_all('tr')\n    \n    if not rows:\n        # Return empty DataFrame if no rows\n        return pd.DataFrame()\n    \n    # Extract headers if present\n    headers = []\n    header_row = table.find('thead')\n    if header_row:\n        header_cells = header_row.find_all(['th', 'td'])\n        headers = [cell.get_text(strip=True) for cell in header_cells]\n    else:\n        # Check if first row contains headers\n        first_row = rows[0]\n        if first_row.find('th'):\n            header_cells = first_row.find_all(['th', 'td'])\n            headers = [cell.get_text(strip=True) for cell in header_cells]\n            rows = rows[1:]  # Skip header row for data extraction\n    \n    # Extract data rows\n    data = []\n    for row in rows:\n        # Skip rows that are in thead\n        if row.parent.name == 'thead':\n            continue\n            \n        cells = row.find_all(['td', 'th'])\n        if cells:  # Only add non-empty rows\n            row_data = [cell.get_text(strip=True) for cell in cells]\n            data.append(row_data)\n    \n    # Create DataFrame\n    if not data:\n        # No data rows found\n        return pd.DataFrame()\n    \n    if headers:\n        df = pd.DataFrame(data, columns=headers)\n    else:\n        df = pd.DataFrame(data)\n    \n    return df",
        "status": "fail",
        "details": {
          "test_empty_table": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 162, in test_empty_table\n  File \"__test__.py\", line 41, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/bs4/__init__.py\", line 319, in __init__\n    for (self.markup, self.original_encoding, self.declared_html_encoding,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/builder/_htmlparser.py\", line 325, in prepare_markup\n    dammit = UnicodeDammit(markup, try_encodings, is_html=True,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 445, in __init__\n    for encoding in self.detector.encodings:\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 301, in encodings\n    self.declared_encoding = self.find_declared_encoding(\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 378, in find_declared_encoding\n    declared_encoding_match = xml_re.search(markup, endpos=xml_endpos)\nTypeError: expected string or bytes-like object\n",
          "test_network_error": "Traceback (most recent call last):\n  File \"__test__.py\", line 31, in task_func\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1114, in __call__\n    return self._mock_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1118, in _mock_call\n    return self._execute_mock_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1173, in _execute_mock_call\n    raise effect\nrequests.exceptions.ConnectionError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 137, in test_network_error\n  File \"__test__.py\", line 38, in task_func\nrequests.exceptions.HTTPError: HTTP request failed: \n",
          "test_successful_scrape": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 117, in test_successful_scrape\n  File \"__test__.py\", line 41, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/bs4/__init__.py\", line 319, in __init__\n    for (self.markup, self.original_encoding, self.declared_html_encoding,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/builder/_htmlparser.py\", line 325, in prepare_markup\n    dammit = UnicodeDammit(markup, try_encodings, is_html=True,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 445, in __init__\n    for encoding in self.detector.encodings:\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 301, in encodings\n    self.declared_encoding = self.find_declared_encoding(\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 378, in find_declared_encoding\n    declared_encoding_match = xml_re.search(markup, endpos=xml_endpos)\nTypeError: expected string or bytes-like object\n",
          "test_table_not_found": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 131, in test_table_not_found\n  File \"__test__.py\", line 41, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/bs4/__init__.py\", line 319, in __init__\n    for (self.markup, self.original_encoding, self.declared_html_encoding,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/builder/_htmlparser.py\", line 325, in prepare_markup\n    dammit = UnicodeDammit(markup, try_encodings, is_html=True,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 445, in __init__\n    for encoding in self.detector.encodings:\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 301, in encodings\n    self.declared_encoding = self.find_declared_encoding(\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 378, in find_declared_encoding\n    declared_encoding_match = xml_re.search(markup, endpos=xml_endpos)\nTypeError: expected string or bytes-like object\n"
        }
      }
    ],
    "BigCodeBench/1004": [
      {
        "task_id": "BigCodeBench/1004",
        "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(url):\n\n    pass\nimport urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(url):\n    \"\"\"\n    Downloads a text file from a specified URL, processes the text to count the frequency \n    of each word, and then plots a bar chart showing the ten most frequently occurring words.\n    \n    Args:\n        url (str): The URL of the text file to download and process\n        \n    Returns:\n        tuple: A tuple containing two elements:\n            Counter: A Counter object from the collections module, containing word frequencies in the text.\n            Axes: A matplotlib Axes object that represents the plotted bar chart of the ten most common words.\n    \"\"\"\n    # Download the text file from the URL\n    try:\n        with urllib.request.urlopen(url) as response:\n            # Read the content and decode it as text\n            text = response.read().decode('utf-8')\n    except Exception as e:\n        raise Exception(f\"Error downloading file from URL: {e}\")\n    \n    # Extract words using regular expression (case-sensitive)\n    # This pattern matches sequences of word characters (letters, digits, underscores)\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Count word frequencies\n    word_counter = Counter(words)\n    \n    # Get the 10 most common words\n    most_common_words = word_counter.most_common(10)\n    \n    # Prepare data for plotting\n    if most_common_words:\n        words_list = [word for word, count in most_common_words]\n        counts_list = [count for word, count in most_common_words]\n    else:\n        words_list = []\n        counts_list = []\n    \n    # Create the bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    if words_list:\n        bars = ax.bar(range(len(words_list)), counts_list)\n        \n        # Set x-axis labels\n        ax.set_xticks(range(len(words_list)))\n        ax.set_xticklabels(words_list, rotation=45, ha='right')\n        \n        # Set labels and title\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequency')\n        ax.set_title('Top 10 Most Frequent Words')\n        \n        # Add value labels on top of bars\n        for i, (bar, count) in enumerate(zip(bars, counts_list)):\n            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n                   str(count), ha='center', va='bottom')\n    else:\n        ax.text(0.5, 0.5, 'No words found', ha='center', va='center', \n                transform=ax.transAxes)\n        ax.set_title('No Words Found in Text')\n    \n    # Adjust layout to prevent label cutoff\n    plt.tight_layout()\n    \n    # Display the plot\n    plt.show()\n    \n    return word_counter, ax",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/1020": [
      {
        "task_id": "BigCodeBench/1020",
        "solution": "import json\nimport requests\nimport chardet\n# Constants\nAPI_URL = \"http://api.example.com/data\"\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n\n    pass\nimport json\nimport requests\nimport chardet\nAPI_URL = \"http://api.example.com/data\"\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    \"\"\"\n    Fetches data from a specified REST API URL and processes it for JSON parsing.\n    The process involves decoding and re-encoding the data, handling different encoding scenarios.\n    \n    Args:\n        url (str): The URL to fetch data from. Defaults to API_URL.\n        from_encoding (str, optional): The encoding to decode from. If None, auto-detects encoding.\n        to_encoding (str): The encoding to encode to. Defaults to \"utf8\".\n    \n    Returns:\n        dict: The JSON-parsed data after re-encoding. Returns an empty dictionary if the content is empty.\n    \n    Raises:\n        ValueError: If unable to detect encoding for non-empty content.\n    \"\"\"\n    try:\n        # Make HTTP GET request with 5-second timeout\n        response = requests.get(url, timeout=5)\n        \n        # Get raw bytes content\n        content = response.content\n        \n        # Handle empty content\n        if not content:\n            return {}\n        \n        # Determine encoding\n        if from_encoding is None:\n            # Auto-detect encoding using chardet\n            detection = chardet.detect(content)\n            detected_encoding = detection.get('encoding')\n            \n            # If encoding cannot be detected for non-empty content, raise ValueError\n            if detected_encoding is None:\n                raise ValueError(\"Unable to detect encoding for non-empty content\")\n            \n            from_encoding = detected_encoding\n        \n        # Decode content from detected/specified encoding\n        decoded_text = content.decode(from_encoding)\n        \n        # Re-encode to target encoding\n        re_encoded_bytes = decoded_text.encode(to_encoding)\n        \n        # Parse JSON from re-encoded bytes\n        json_data = json.loads(re_encoded_bytes.decode(to_encoding))\n        \n        return json_data\n        \n    except requests.exceptions.Timeout:\n        raise\n    except requests.exceptions.RequestException:\n        raise\n    except json.JSONDecodeError:\n        raise\n    except UnicodeDecodeError:\n        raise\n    except ValueError as e:\n        if \"Unable to detect encoding\" in str(e):\n            raise\n        else:\n            raise",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/1019": [
      {
        "task_id": "BigCodeBench/1019",
        "solution": "from PIL import Image\nimport codecs\nimport pytesseract\nIMAGE_PATH = \"image.png\"\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n\n    pass\nfrom PIL import Image\nimport codecs\nimport pytesseract\nIMAGE_PATH = \"image.png\"\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    \"\"\"\n    Opens an image file, extracts text using OCR, and converts the text encoding,\n    with a fallback to image comment processing.\n    \n    Args:\n        filename (str): Path to the image file\n        from_encoding (str): Source encoding for text conversion\n        to_encoding (str): Target encoding for text conversion\n    \n    Returns:\n        comment (str): The text extracted from the image or the image comment,\n                      converted to the target encoding. If OCR extraction and\n                      comment processing both fail, returns an empty string.\n    \n    Raises:\n        ValueError: UnicodeDecodeError or LookupError occurs during conversion\n        ValueError: If incorrect encodings are provided for the text or comment conversion\n    \"\"\"\n    try:\n        # Validate encodings first\n        try:\n            codecs.lookup(from_encoding)\n            codecs.lookup(to_encoding)\n        except LookupError as e:\n            raise ValueError(f\"Invalid encoding provided: {e}\")\n        \n        # Open the image\n        image = Image.open(filename)\n        \n        # Try OCR extraction first\n        try:\n            # Extract text using OCR\n            extracted_text = pytesseract.image_to_string(image)\n            \n            if extracted_text.strip():  # If OCR found text\n                try:\n                    # Convert encoding\n                    # First encode to bytes using the target encoding to ensure it's valid\n                    # Then decode from source encoding and encode to target encoding\n                    converted_text = extracted_text.encode(from_encoding).decode(from_encoding).encode(to_encoding).decode(to_encoding)\n                    return converted_text\n                except (UnicodeDecodeError, UnicodeEncodeError) as e:\n                    # If the text can't be converted with the specified encodings,\n                    # try direct conversion assuming the text is already in a standard format\n                    try:\n                        # Assume the OCR text is in UTF-8 and convert to target encoding\n                        converted_text = extracted_text.encode('utf-8').decode('utf-8').encode(to_encoding).decode(to_encoding)\n                        return converted_text\n                    except (UnicodeDecodeError, UnicodeEncodeError):\n                        raise ValueError(f\"Unicode conversion error during OCR text processing: {e}\")\n        \n        except Exception as ocr_error:\n            # OCR failed, continue to comment processing\n            pass\n        \n        # Fallback to image comment processing\n        try:\n            # Get image info/comments\n            info = image.info\n            comment = \"\"\n            \n            # Check various possible comment fields\n            comment_fields = ['comment', 'Comment', 'description', 'Description', \n                            'UserComment', 'ImageDescription']\n            \n            for field in comment_fields:\n                if field in info:\n                    comment = str(info[field])\n                    break\n            \n            # Also check EXIF data if available\n            if not comment and hasattr(image, '_getexif'):\n                exif = image._getexif()\n                if exif:\n                    # EXIF UserComment tag\n                    if 0x9286 in exif:\n                        comment = str(exif[0x9286])\n                    # EXIF ImageDescription tag\n                    elif 0x010e in exif:\n                        comment = str(exif[0x010e])\n            \n            if comment:\n                try:\n                    # Convert comment encoding\n                    # Handle bytes or string input\n                    if isinstance(comment, bytes):\n                        converted_comment = comment.decode(from_encoding).encode(to_encoding).decode(to_encoding)\n                    else:\n                        converted_comment = comment.encode(from_encoding).decode(from_encoding).encode(to_encoding).decode(to_encoding)\n                    return converted_comment\n                except (UnicodeDecodeError, UnicodeEncodeError) as e:\n                    raise ValueError(f\"Unicode conversion error during comment processing: {e}\")\n            \n        except Exception as comment_error:\n            # Comment processing failed\n            pass\n        \n        # Both OCR and comment processing failed\n        return \"\"\n        \n    except ValueError:\n        # Re-raise ValueError as specified\n        raise\n    except Exception as e:\n        # For any other exception, return empty string\n        return \"\"",
        "status": "fail",
        "details": {
          "test_ocr_fails_comment_extraction_succeeds": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 147, in test_ocr_fails_comment_extraction_succeeds\nAssertionError: '' != 'Mocked Comment in cp1251'\n+ Mocked Comment in cp1251\n"
        }
      }
    ],
    "BigCodeBench/1022": [
      {
        "task_id": "BigCodeBench/1022",
        "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n\n    pass\nimport pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"\n    Reads a CSV file and processes its date-related data. The function performs several key tasks:\n    - Checks for the file's existence\n    - Validates the presence of a specified date column\n    - Converts date values to datetime objects\n    - Filters rows based on the current date\n    - Sorts the resulting data\n    \n    Parameters:\n    csv_file_path (str): Path to the CSV file\n    column_name (str): Name of the date column to process\n    date_format (str): Format of the date strings in the CSV file (default: \"%Y-%m-%d\")\n    \n    Returns:\n    pandas.DataFrame: Processed DataFrame with rows filtered and sorted by date\n    \n    Raises:\n    FileNotFoundError: If the specified CSV file is not found at the given path\n    ValueError: If the specified column is not present in the CSV file\n    \"\"\"\n    \n    # Check if file exists\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} was not found.\")\n    \n    try:\n        # Read the CSV file\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        # Return empty DataFrame if CSV is empty\n        return pd.DataFrame()\n    \n    # Check if the DataFrame is empty after reading\n    if df.empty:\n        return pd.DataFrame()\n    \n    # Check if the specified column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"Column '{column_name}' not found in the CSV file.\")\n    \n    # Convert the date column to datetime objects\n    try:\n        df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n    except Exception as e:\n        # If conversion fails, try without format specification\n        try:\n            df[column_name] = pd.to_datetime(df[column_name])\n        except:\n            raise ValueError(f\"Unable to convert column '{column_name}' to datetime format.\")\n    \n    # Get current date (without time component for fair comparison)\n    current_date = datetime.now().date()\n    \n    # Filter rows where the date is less than or equal to current date\n    # Convert datetime to date for comparison\n    df_filtered = df[df[column_name].dt.date <= current_date]\n    \n    # Sort the DataFrame by the date column in ascending order\n    df_sorted = df_filtered.sort_values(by=column_name, ascending=True)\n    \n    # Reset index for clean output\n    df_sorted = df_sorted.reset_index(drop=True)\n    \n    return df_sorted",
        "status": "fail",
        "details": {
          "test_no_future_dates": "Traceback (most recent call last):\n  File \"__test__.py\", line 138, in test_no_future_dates\nAssertionError: False is not true\n",
          "test_valid_input": "Traceback (most recent call last):\n  File \"__test__.py\", line 103, in test_valid_input\nAssertionError: True is not false\n"
        }
      }
    ],
    "BigCodeBench/1013": [
      {
        "task_id": "BigCodeBench/1013",
        "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n\n    pass\nimport requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    This function scrapes a webpage for all hyperlinks and saves them as absolute URLs to a CSV file.\n    \n    Args:\n        url: The URL of the webpage to scrape\n        base_url: The base URL to use for converting relative URLs to absolute URLs\n        csv_file: The name of the CSV file to save the links to\n        \n    Returns:\n        int: The number of unique absolute links scraped from the webpage.\n    \"\"\"\n    try:\n        # Send GET request to the URL\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()  # Raise an exception for bad status codes\n        \n        # Parse the HTML content\n        soup = BeautifulSoup(response.content, 'html.parser')\n        \n        # Find all anchor tags with href attribute\n        links = soup.find_all('a', href=True)\n        \n        # Set to store unique absolute URLs\n        unique_absolute_urls = set()\n        \n        # Process each link\n        for link in links:\n            href = link['href']\n            \n            # Convert relative URLs to absolute URLs\n            absolute_url = urljoin(base_url, href)\n            \n            # Add to set (automatically handles duplicates)\n            unique_absolute_urls.add(absolute_url)\n        \n        # Write unique URLs to CSV file\n        with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n            writer = csv.writer(file)\n            # Write header\n            writer.writerow(['URL'])\n            # Write each unique URL\n            for url in sorted(unique_absolute_urls):\n                writer.writerow([url])\n        \n        # Return the count of unique URLs\n        return len(unique_absolute_urls)\n        \n    except requests.RequestException as e:\n        print(f\"Error fetching the webpage: {e}\")\n        return 0\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return 0",
        "status": "fail",
        "details": {
          "test_duplicate_links": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 117, in test_duplicate_links\nAssertionError: 0 != 1\n",
          "test_external_links": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 127, in test_external_links\nAssertionError: 0 != 1\n",
          "test_multiple_links": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 107, in test_multiple_links\nAssertionError: 0 != 2\n",
          "test_single_link": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 97, in test_single_link\nAssertionError: 0 != 1\n"
        }
      }
    ],
    "BigCodeBench/1015": [
      {
        "task_id": "BigCodeBench/1015",
        "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n\n    pass\nimport requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    \"\"\"\n    This function parses HTML table data from a specified URL or local file and stores it into an SQLite database.\n    \n    Args:\n        webpage_url (str): The URL or local file path containing HTML table data\n        database_name (str): The name of the SQLite database file (default: \"my_database.db\")\n    \n    Returns:\n        int: The number of rows in the parsed HTML table.\n    \n    Raises:\n        requests.RequestException: If there is a network issue in accessing the URL\n        sqlite3.DatabaseError: If there are issues with the SQLite database operations\n    \"\"\"\n    \n    # Fetch HTML content\n    if webpage_url.startswith(('http://', 'https://')):\n        # It's a URL\n        try:\n            response = requests.get(webpage_url)\n            response.raise_for_status()  # Raise an exception for bad status codes\n            html_content = response.text\n        except requests.RequestException:\n            # Re-raise the exception as required\n            raise\n    else:\n        # It's a local file\n        try:\n            with open(webpage_url, 'r', encoding='utf-8') as file:\n                html_content = file.read()\n        except IOError as e:\n            # Convert file reading errors to requests.RequestException for consistency\n            raise requests.RequestException(f\"Error reading local file: {e}\")\n    \n    # Parse HTML content\n    tree = html.fromstring(html_content)\n    \n    # Find all tables in the HTML\n    tables = tree.xpath('//table')\n    \n    if not tables:\n        # No tables found\n        return 0\n    \n    # Extract data from the first table\n    table = tables[0]\n    \n    # Extract headers (if any)\n    headers = []\n    header_rows = table.xpath('.//thead//tr')\n    if header_rows:\n        headers = [th.text_content().strip() for th in header_rows[0].xpath('.//th')]\n    \n    # If no headers in thead, check first row of tbody\n    if not headers:\n        first_row = table.xpath('.//tr[1]')\n        if first_row:\n            # Check if first row contains th elements\n            th_elements = first_row[0].xpath('.//th')\n            if th_elements:\n                headers = [th.text_content().strip() for th in th_elements]\n            else:\n                # Use td elements from first row as headers\n                td_elements = first_row[0].xpath('.//td')\n                if td_elements:\n                    headers = [f\"Column_{i}\" for i in range(len(td_elements))]\n    \n    # Extract data rows\n    data_rows = []\n    all_rows = table.xpath('.//tr')\n    \n    for row in all_rows:\n        # Skip rows that only contain headers (th elements)\n        if row.xpath('.//th') and not row.xpath('.//td'):\n            continue\n        \n        # Extract td elements\n        cells = row.xpath('.//td')\n        if cells:\n            row_data = [cell.text_content().strip() for cell in cells]\n            data_rows.append(row_data)\n    \n    if not data_rows:\n        # Table is empty\n        return 0\n    \n    # Create DataFrame\n    if headers and len(headers) == len(data_rows[0]):\n        df = pd.DataFrame(data_rows, columns=headers)\n    else:\n        # Generate column names if headers don't match\n        num_cols = len(data_rows[0])\n        columns = [f\"Column_{i}\" for i in range(num_cols)]\n        df = pd.DataFrame(data_rows, columns=columns)\n    \n    # Store in SQLite database\n    try:\n        conn = sqlite3.connect(database_name)\n        \n        # Replace the table with new data\n        df.to_sql('my_table', conn, if_exists='replace', index=False)\n        \n        # Get the number of rows inserted\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT COUNT(*) FROM my_table\")\n        row_count = cursor.fetchone()[0]\n        \n        conn.close()\n        \n        return row_count\n        \n    except sqlite3.Error as e:\n        # Convert to DatabaseError and re-raise\n        raise sqlite3.DatabaseError(f\"Database error: {e}\")",
        "status": "fail",
        "details": {
          "test_database_error": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 194, in test_database_error\n  File \"__test__.py\", line 48, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/lxml/html/__init__.py\", line 872, in fromstring\n    is_full_html = _looks_like_full_html_unicode(html)\nTypeError: expected string or bytes-like object\n",
          "test_empty_table": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 175, in test_empty_table\n  File \"__test__.py\", line 48, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/lxml/html/__init__.py\", line 872, in fromstring\n    is_full_html = _looks_like_full_html_unicode(html)\nTypeError: expected string or bytes-like object\n",
          "test_valid_webpage_url": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 145, in test_valid_webpage_url\n  File \"__test__.py\", line 48, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/lxml/html/__init__.py\", line 872, in fromstring\n    is_full_html = _looks_like_full_html_unicode(html)\nTypeError: expected string or bytes-like object\n"
        }
      }
    ],
    "BigCodeBench/1039": [
      {
        "task_id": "BigCodeBench/1039",
        "solution": "import ssl\nimport os\nimport hashlib\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n\n    pass\nimport ssl\nimport os\nimport hashlib\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    \"\"\"\n    This function secures a client socket using SSL/TLS and sends back the SHA256 hash \n    of a file requested by the client.\n    \n    Args:\n        client_socket: The client socket to secure with SSL/TLS\n        cert_file: Path to the SSL certificate file\n        key_file: Path to the SSL key file\n        buffer_size: Size of buffer for reading data (default: 1024)\n    \n    Returns:\n        str: The SHA256 hash of the requested file. If the requested file does not exist, \n             returns 'File not found'. In case of an exception during processing, \n             an error message is returned.\n    \"\"\"\n    try:\n        # Create SSL context\n        context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n        \n        # Wrap the client socket with SSL\n        ssl_socket = context.wrap_socket(client_socket, server_side=True)\n        \n        # Receive the file path from the client\n        file_path = ssl_socket.recv(buffer_size).decode('utf-8').strip()\n        \n        # Check if the file exists\n        if not os.path.exists(file_path):\n            response = 'File not found'\n            ssl_socket.send(response.encode('utf-8'))\n            return response\n        \n        # Check if it's actually a file (not a directory)\n        if not os.path.isfile(file_path):\n            response = 'File not found'\n            ssl_socket.send(response.encode('utf-8'))\n            return response\n        \n        # Calculate SHA256 hash of the file\n        sha256_hash = hashlib.sha256()\n        \n        with open(file_path, 'rb') as f:\n            # Read file in chunks to handle large files\n            while True:\n                data = f.read(buffer_size)\n                if not data:\n                    break\n                sha256_hash.update(data)\n        \n        # Get the hexadecimal representation of the hash\n        file_hash = sha256_hash.hexdigest()\n        \n        # Send the hash back to the client\n        ssl_socket.send(file_hash.encode('utf-8'))\n        \n        return file_hash\n        \n    except Exception as e:\n        error_message = f\"Error processing request: {str(e)}\"\n        try:\n            # Try to send error message to client if possible\n            if 'ssl_socket' in locals():\n                ssl_socket.send(error_message.encode('utf-8'))\n        except:\n            pass\n        return error_message",
        "status": "fail",
        "details": {
          "test_exception_handling": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 162, in test_exception_handling\nAssertionError: False is not true\n",
          "test_file_found": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 108, in test_file_found\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 920, in assert_called_with\n    raise AssertionError(error_message)\nAssertionError: expected call not found.\nExpected: open('path/to/requested_file.txt', 'rb')\nActual: not called.\n",
          "test_file_not_found": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 144, in test_file_not_found\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 898, in assert_called\n    raise AssertionError(msg)\nAssertionError: Expected 'close' to have been called.\n",
          "test_task_func_empty_file": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 189, in test_task_func_empty_file\nAssertionError: 'File not found' != 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'\n- File not found\n+ e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n\n",
          "test_task_func_large_file": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 216, in test_task_func_large_file\nAssertionError: 'File not found' != 'cdc76e5c9914fb9281a1c7e284d73e67f1809a48a497200e046d39ccc7112cd0'\n- File not found\n+ cdc76e5c9914fb9281a1c7e284d73e67f1809a48a497200e046d39ccc7112cd0\n\n"
        }
      }
    ],
    "BigCodeBench/1042": [
      {
        "task_id": "BigCodeBench/1042",
        "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\nSERVER_ADDRESS = \"localhost\"\nSERVER_PORT = 25\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\ndef task_func(client_socket):\n\n    pass\nimport smtplib\nfrom email.message import EmailMessage\nimport getpass\nimport socket\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\ndef task_func(client_socket):\n    \"\"\"\n    Receive a message from a client socket and send it as an email via an SMTP server.\n    \n    Args:\n        client_socket: A connected socket object from which to receive the message\n        \n    Returns:\n        None\n    \"\"\"\n    try:\n        # Receive message from client socket\n        message_data = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n        \n        if not message_data:\n            print(\"No message received from client\")\n            return None\n            \n        print(f\"Received message: {message_data}\")\n        \n        # Get email credentials and details\n        sender_email = input(\"Enter sender's email address: \")\n        sender_password = getpass.getpass(\"Enter sender's email password: \")\n        recipient_email = input(\"Enter recipient's email address: \")\n        email_subject = input(\"Enter email subject: \")\n        \n        # Create email message\n        msg = EmailMessage()\n        msg['Subject'] = email_subject\n        msg['From'] = sender_email\n        msg['To'] = recipient_email\n        msg.set_content(message_data)\n        \n        # Send email via SMTP\n        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as smtp:\n            smtp.starttls()  # Enable encryption\n            smtp.login(sender_email, sender_password)\n            smtp.send_message(msg)\n            \n        print(\"Email sent successfully!\")\n        \n    except socket.timeout:\n        print(\"Socket timeout - no data received\")\n    except socket.error as e:\n        print(f\"Socket error: {e}\")\n    except smtplib.SMTPAuthenticationError:\n        print(\"SMTP authentication failed. Check your email and password.\")\n    except smtplib.SMTPException as e:\n        print(f\"SMTP error occurred: {e}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    \n    return None",
        "status": "fail",
        "details": {
          "test_smtp_server_connection_error": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 137, in test_smtp_server_connection_error\nAssertionError: SMTPConnectError not raised\n",
          "test_socket_closes_after_operation": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 156, in test_socket_closes_after_operation\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 908, in assert_called_once\n    raise AssertionError(msg)\nAssertionError: Expected 'close' to have been called once. Called 0 times.\n",
          "test_successful_email_dispatch": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 175, in test_successful_email_dispatch\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 920, in assert_called_with\n    raise AssertionError(error_message)\nAssertionError: expected call not found.\nExpected: SMTP('smtp.gmail.com', 587)\nActual: not called.\n",
          "test_successful_email_send": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 96, in test_successful_email_send\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 920, in assert_called_with\n    raise AssertionError(error_message)\nAssertionError: expected call not found.\nExpected: SMTP('smtp.gmail.com', 587)\nActual: not called.\n"
        }
      }
    ],
    "BigCodeBench/1129": [
      {
        "task_id": "BigCodeBench/1129",
        "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\ndef task_func(json_data, unknown_key, save_dir=None):\n\n    pass\nimport json\nimport requests\nimport os\nfrom datetime import datetime\ndef task_func(json_data, unknown_key, save_dir=None):\n    \"\"\"\n    Parses a JSON string to find a URL associated with a specified key, downloads the file from the URL,\n    and saves it with a timestamped filename. The filename format is '{unknown_key}_{timestamp}.txt',\n    where 'timestamp' is formatted as '%Y%m%d%H%M%S%f' to include the date and time down to microseconds.\n    The file is saved in the specified directory or in the current working directory by default.\n    \n    Args:\n        json_data (str): JSON string containing the data\n        unknown_key (str): The key to look for in the JSON data\n        save_dir (str, optional): Directory to save the file. Defaults to current working directory.\n    \n    Returns:\n        str: The absolute path of the downloaded file, reflecting where it has been saved.\n    \"\"\"\n    # Parse the JSON data\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Invalid JSON data: {e}\")\n    \n    # Find the URL associated with the specified key\n    if unknown_key not in data:\n        raise KeyError(f\"Key '{unknown_key}' not found in JSON data\")\n    \n    url = data[unknown_key]\n    \n    # Validate that the value is a string (URL)\n    if not isinstance(url, str):\n        raise ValueError(f\"Value for key '{unknown_key}' is not a string/URL\")\n    \n    # Download the file from the URL\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for bad status codes\n    except requests.RequestException as e:\n        raise RuntimeError(f\"Failed to download file from URL: {e}\")\n    \n    # Generate timestamp\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    \n    # Create filename\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    \n    # Determine save directory\n    if save_dir is None:\n        save_dir = os.getcwd()\n    else:\n        # Create directory if it doesn't exist\n        os.makedirs(save_dir, exist_ok=True)\n    \n    # Create full file path\n    file_path = os.path.join(save_dir, filename)\n    \n    # Save the downloaded content to file\n    try:\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n    except IOError as e:\n        raise RuntimeError(f\"Failed to save file: {e}\")\n    \n    # Return the absolute path\n    return os.path.abspath(file_path)",
        "status": "fail",
        "details": {
          "test_download_to_default_directory": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 121, in test_download_to_default_directory\n  File \"__test__.py\", line 46, in task_func\nAttributeError: 'MockResponse' object has no attribute 'raise_for_status'\n",
          "test_download_with_direct_key": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 96, in test_download_with_direct_key\n  File \"__test__.py\", line 46, in task_func\nAttributeError: 'MockResponse' object has no attribute 'raise_for_status'\n",
          "test_download_with_specified_directory": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 112, in test_download_with_specified_directory\n  File \"__test__.py\", line 46, in task_func\nAttributeError: 'MockResponse' object has no attribute 'raise_for_status'\n",
          "test_multiple_downloads": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 132, in test_multiple_downloads\n  File \"__test__.py\", line 46, in task_func\nAttributeError: 'MockResponse' object has no attribute 'raise_for_status'\n"
        }
      }
    ],
    "BigCodeBench/1057": [
      {
        "task_id": "BigCodeBench/1057",
        "solution": "import pandas as pd\nimport itertools\nimport numpy as np\ndef task_func(animals=None, foods=None):\n\n    pass\nimport pandas as pd\nimport itertools\nimport numpy as np\ndef task_func(animals=None, foods=None):\n    \"\"\"\n    Create a DataFrame with combinations of animals and foods in a 'animal:food' format.\n    \n    Parameters:\n    animals (list): List of animal names. If None or empty, uses predefined list.\n    foods (list): List of food names. If None or empty, uses predefined list.\n    \n    Returns:\n    df (pandas.DataFrame): A DataFrame where each row represents a unique animal from the 'animals'\n    list and each column represents a food item from the 'foods' list. Each cell contains a string \n    in the format 'animal:food'.\n    \"\"\"\n    # Predefined lists\n    default_animals = ['dog', 'cat', 'bird', 'fish']\n    default_foods = ['meat', 'fish', 'vegetables', 'fruits']\n    \n    # Handle special cases\n    if (animals is None or len(animals) == 0) and (foods is None or len(foods) == 0):\n        # Both are empty/None - return empty DataFrame\n        return pd.DataFrame()\n    \n    # Use predefined lists if parameters are None or empty\n    if animals is None or len(animals) == 0:\n        animals = default_animals\n    if foods is None or len(foods) == 0:\n        foods = default_foods\n    \n    # Generate all combinations\n    combinations = list(itertools.product(animals, foods))\n    \n    # Create formatted strings\n    formatted_combinations = [f\"{animal}:{food}\" for animal, food in combinations]\n    \n    # Shuffle the combinations\n    np.random.shuffle(formatted_combinations)\n    \n    # Create DataFrame with animals as rows and foods as columns\n    # First, we need to reshape the shuffled data back into a matrix\n    n_animals = len(animals)\n    n_foods = len(foods)\n    \n    # Create an empty DataFrame with the right structure\n    df = pd.DataFrame(index=animals, columns=foods)\n    \n    # Fill the DataFrame with the shuffled combinations\n    # We need to ensure each cell gets the right animal:food combination\n    for i, (animal, food) in enumerate(itertools.product(animals, foods)):\n        row_idx = animals.index(animal)\n        col_idx = foods.index(food)\n        df.loc[animal, food] = formatted_combinations[i]\n    \n    return df",
        "status": "fail",
        "details": {
          "test_default_input": "Traceback (most recent call last):\n  File \"__test__.py\", line 73, in test_default_input\nAssertionError: Tuples differ: (0, 0) != (10, 7)\n\nFirst differing element 0:\n0\n10\n\n- (0, 0)\n?     ^\n\n+ (10, 7)\n?  +   ^\n : The shape of the DataFrame with default inputs is not as expected.\n",
          "test_partial_default": "Traceback (most recent call last):\n  File \"__test__.py\", line 130, in test_partial_default\nAssertionError: Tuples differ: (3, 4) != (3, 7)\n\nFirst differing element 1:\n4\n7\n\n- (3, 4)\n?     ^\n\n+ (3, 7)\n?     ^\n : The shape of the DataFrame with partial default inputs is not as expected.\n"
        }
      }
    ],
    "BigCodeBench/1077": [
      {
        "task_id": "BigCodeBench/1077",
        "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\ndef task_func(time_strings, timezone):\n\n    pass\nfrom datetime import datetime\nimport pytz\nimport numpy as np\ndef task_func(time_strings, timezone):\n    \"\"\"\n    Calculates the average time difference in seconds between each consecutive pair of timestamps \n    in a given list, after converting them to a specified timezone.\n    \n    Args:\n        time_strings: List of timestamp strings\n        timezone: String representing the target timezone (e.g., 'UTC', 'US/Eastern')\n    \n    Returns:\n        float: The mean (average) time difference in seconds between each consecutive pair of timestamps.\n        If there are less than two timestamps in the list, the function returns 0.0.\n    \"\"\"\n    # Return 0.0 if less than 2 timestamps\n    if len(time_strings) < 2:\n        return 0.0\n    \n    # Get the timezone object\n    tz = pytz.timezone(timezone)\n    \n    # Convert time strings to datetime objects in the specified timezone\n    timestamps = []\n    for time_string in time_strings:\n        # Parse the datetime string - assuming ISO format or similar\n        # You may need to adjust the parsing based on your input format\n        try:\n            # Try parsing with timezone info\n            dt = datetime.fromisoformat(time_string.replace('Z', '+00:00'))\n        except:\n            try:\n                # Try parsing without timezone info\n                dt = datetime.strptime(time_string, '%Y-%m-%d %H:%M:%S')\n                # Assume UTC if no timezone info\n                dt = pytz.UTC.localize(dt)\n            except:\n                # Try other common formats\n                try:\n                    dt = datetime.strptime(time_string, '%Y-%m-%dT%H:%M:%S')\n                    dt = pytz.UTC.localize(dt)\n                except:\n                    # Last resort - try to parse with dateutil if available\n                    from dateutil import parser\n                    dt = parser.parse(time_string)\n                    if dt.tzinfo is None:\n                        dt = pytz.UTC.localize(dt)\n        \n        # Convert to target timezone\n        dt_tz = dt.astimezone(tz)\n        timestamps.append(dt_tz)\n    \n    # Calculate time differences between consecutive pairs\n    time_differences = []\n    for i in range(1, len(timestamps)):\n        diff = abs((timestamps[i] - timestamps[i-1]).total_seconds())\n        time_differences.append(diff)\n    \n    # Return 0.0 if no time differences\n    if len(time_differences) == 0:\n        return 0.0\n    \n    # Calculate and return the mean time difference\n    return np.mean(time_differences)",
        "status": "fail",
        "details": {
          "test_span_across_days": "Traceback (most recent call last):\n  File \"__test__.py\", line 107, in test_span_across_days\nAssertionError: 7516680.0 != 120.0 within 7 places (7516560.0 difference)\n"
        }
      }
    ],
    "BigCodeBench/1137": [
      {
        "task_id": "BigCodeBench/1137",
        "solution": "import bs4\nimport requests\nimport re\nimport json\ndef task_func(url: str, output_path: str) -> list:\n\n    pass\nimport bs4\nimport requests\nimport re\nimport json\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    Extracts phone numbers from a given URL or local file and saves them to a specified file in JSON format.\n    \n    Args:\n        url (str): The URL or local file path to extract phone numbers from\n        output_path (str): The file path where the extracted phone numbers will be saved in JSON format\n    \n    Returns:\n        list: A list of strings, each representing a found phone number\n    \"\"\"\n    # Phone number patterns to match various formats\n    phone_patterns = [\n        r'\\+?1?\\s*\\(?[0-9]{3}\\)?[\\s.-]?[0-9]{3}[\\s.-]?[0-9]{4}',  # US format\n        r'\\+?[0-9]{1,3}[\\s.-]?[0-9]{1,4}[\\s.-]?[0-9]{1,4}[\\s.-]?[0-9]{1,4}[\\s.-]?[0-9]{1,4}',  # International\n        r'\\(?[0-9]{3}\\)?[\\s.-]?[0-9]{3}[\\s.-]?[0-9]{4}',  # Simple US format\n        r'[0-9]{3}[\\s.-][0-9]{3}[\\s.-][0-9]{4}',  # xxx-xxx-xxxx or xxx.xxx.xxxx\n        r'\\([0-9]{3}\\)\\s*[0-9]{3}[\\s.-]?[0-9]{4}',  # (xxx) xxx-xxxx\n    ]\n    \n    # Combine all patterns into one\n    combined_pattern = '|'.join(phone_patterns)\n    \n    try:\n        # Check if it's a URL or local file\n        if url.startswith(('http://', 'https://')):\n            # Fetch content from URL\n            response = requests.get(url, timeout=10)\n            response.raise_for_status()\n            content = response.text\n        else:\n            # Read from local file\n            with open(url, 'r', encoding='utf-8') as file:\n                content = file.read()\n        \n        # Parse HTML content\n        soup = bs4.BeautifulSoup(content, 'html.parser')\n        \n        # Get all text from the page\n        text = soup.get_text()\n        \n        # Find all phone numbers\n        phone_numbers = re.findall(combined_pattern, text)\n        \n        # Clean and normalize phone numbers\n        cleaned_numbers = []\n        seen = set()\n        \n        for number in phone_numbers:\n            # Remove extra whitespace and normalize\n            cleaned = re.sub(r'\\s+', ' ', number.strip())\n            \n            # Remove duplicate numbers\n            if cleaned not in seen and len(cleaned) >= 10:  # Minimum length check\n                seen.add(cleaned)\n                cleaned_numbers.append(cleaned)\n        \n        # Save to JSON file\n        with open(output_path, 'w', encoding='utf-8') as json_file:\n            json.dump(cleaned_numbers, json_file, indent=2)\n        \n        return cleaned_numbers\n        \n    except requests.RequestException as e:\n        print(f\"Error fetching URL: {e}\")\n        return []\n    except FileNotFoundError:\n        print(f\"File not found: {url}\")\n        return []\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return []",
        "status": "fail",
        "details": {
          "test_phone_numbers_page_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 111, in test_phone_numbers_page_1\n  File \"__test__.py\", line 137, in run_test_case\nAssertionError: Lists differ: [] != ['+1234567890']\n\nSecond list contains 1 additional elements.\nFirst extra element 0:\n'+1234567890'\n\n- []\n+ ['+1234567890']\n",
          "test_phone_numbers_page_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 116, in test_phone_numbers_page_2\n  File \"__test__.py\", line 137, in run_test_case\nAssertionError: Lists differ: [] != ['+9876543210', '+1122334455']\n\nSecond list contains 2 additional elements.\nFirst extra element 0:\n'+9876543210'\n\n- []\n+ ['+9876543210', '+1122334455']\n",
          "test_phone_numbers_page_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 126, in test_phone_numbers_page_4\n  File \"__test__.py\", line 137, in run_test_case\nAssertionError: Lists differ: [] != ['+919876543210']\n\nSecond list contains 1 additional elements.\nFirst extra element 0:\n'+919876543210'\n\n- []\n+ ['+919876543210']\n",
          "test_phone_numbers_page_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 131, in test_phone_numbers_page_5\n  File \"__test__.py\", line 137, in run_test_case\nAssertionError: Lists differ: [] != ['+14151234567', '+13171234567']\n\nSecond list contains 2 additional elements.\nFirst extra element 0:\n'+14151234567'\n\n- []\n+ ['+14151234567', '+13171234567']\n",
          "test_phone_numbers_page_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 121, in test_phone_numbers_page_3\n  File \"__test__.py\", line 138, in run_test_case\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmp0somhd2l/output_test_page_3.html'\n"
        }
      }
    ],
    "BigCodeBench/1124": [
      {
        "task_id": "BigCodeBench/1124",
        "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\ndef task_func(myString):\n\n    pass\nimport re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\ndef task_func(myString):\n    \"\"\"\n    Extracts a URL from a given string and retrieves the title of the web page from that URL.\n    \n    Args:\n        myString (str): The input string containing a URL\n        \n    Returns:\n        str: The title of the webpage at the extracted URL if successful, otherwise one of the following error messages:\n        \"No valid URL found in the provided string.\"\n        \"Unable to fetch the content of the URL: {url}\"\n        \"No title tag found in the webpage.\"\n    \"\"\"\n    \n    # Regular expression pattern to match URLs\n    url_pattern = r'https?://(?:www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9()@:%_\\+.~#?&\\/=]*)'\n    \n    # Find all URLs in the string\n    urls = re.findall(url_pattern, myString)\n    \n    # If no URL found, return error message\n    if not urls:\n        return \"No valid URL found in the provided string.\"\n    \n    # Use the first URL found\n    url = urls[0]\n    \n    try:\n        # Set a timeout and headers to avoid hanging requests\n        headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n        }\n        \n        # Fetch the webpage content\n        response = requests.get(url, headers=headers, timeout=10)\n        response.raise_for_status()  # Raise an exception for bad status codes\n        \n        # Parse the HTML content\n        soup = BeautifulSoup(response.content, 'html.parser')\n        \n        # Find the title tag\n        title_tag = soup.find('title')\n        \n        if title_tag and title_tag.string:\n            # Clean up the title by stripping whitespace\n            title = title_tag.string.strip()\n            return title\n        else:\n            return \"No title tag found in the webpage.\"\n            \n    except (requests.RequestException, requests.ConnectionError, requests.Timeout) as e:\n        return f\"Unable to fetch the content of the URL: {url}\"\n    except Exception as e:\n        return f\"Unable to fetch the content of the URL: {url}\"",
        "status": "fail",
        "details": {
          "test_multiple_urls_in_string": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 96, in test_multiple_urls_in_string\nAssertionError: 'Unable to fetch the content of the URL: https://www.google.com' != 'Google'\n- Unable to fetch the content of the URL: https://www.google.com\n+ Google\n\n",
          "test_url_with_no_title_tag": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 102, in test_url_with_no_title_tag\nAssertionError: 'Unable to fetch the content of the URL: https://www.notitle.com' != 'No title tag found in the webpage.'\n- Unable to fetch the content of the URL: https://www.notitle.com\n+ No title tag found in the webpage.\n\n",
          "test_valid_url_with_title": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 82, in test_valid_url_with_title\nAssertionError: 'Unable to fetch the content of the URL: https://www.google.com' != 'Google'\n- Unable to fetch the content of the URL: https://www.google.com\n+ Google\n\n"
        }
      }
    ],
    "BigCodeBench/1053": [
      {
        "task_id": "BigCodeBench/1053",
        "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n\n    pass\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    \"\"\"\n    Processes a CSV file containing text data and generates a histogram of the ten most common words.\n    \n    Args:\n        file_path (str): Path to the CSV file containing text data\n        save_path (str, optional): Path to save the histogram image. If None, displays the plot.\n    \n    Returns:\n        matplotlib.axes.Axes: The Axes object of the plot if save_path is not provided.\n        None: If save_path is provided, the plot is saved to the specified path.\n    \n    Raises:\n        FileNotFoundError: If the specified file_path does not exist.\n        Exception: For any other errors that occur during the function execution.\n    \"\"\"\n    try:\n        # Check if file exists\n        import os\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(f\"File not found: {file_path}\")\n        \n        # Read the CSV file\n        try:\n            # Try reading with header\n            df = pd.read_csv(file_path)\n            if 'Text' in df.columns:\n                text_data = df['Text']\n            else:\n                # If no 'Text' column, use the first column\n                text_data = df.iloc[:, 0]\n        except:\n            # If reading with header fails, read without header and use first column\n            df = pd.read_csv(file_path, header=None)\n            text_data = df.iloc[:, 0]\n        \n        # Convert all text to string and join\n        all_text = ' '.join(text_data.astype(str).tolist())\n        \n        # Create CountVectorizer with custom stop words\n        vectorizer = CountVectorizer(\n            stop_words=STOP_WORDS,\n            lowercase=True,\n            token_pattern=r'\\b[a-zA-Z]+\\b'  # Only alphabetic words\n        )\n        \n        # Fit and transform the text\n        word_count_matrix = vectorizer.fit_transform([all_text])\n        \n        # Get word frequencies\n        word_freq = word_count_matrix.toarray()[0]\n        words = vectorizer.get_feature_names_out()\n        \n        # Create a dictionary of word frequencies\n        word_freq_dict = dict(zip(words, word_freq))\n        \n        # Sort by frequency and get top 10\n        sorted_words = sorted(word_freq_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n        \n        # Prepare data for plotting\n        top_words = [word for word, freq in sorted_words]\n        top_frequencies = [freq for word, freq in sorted_words]\n        \n        # Create the plot\n        fig, ax = plt.subplots(figsize=(10, 6))\n        bars = ax.bar(range(len(top_words)), top_frequencies)\n        \n        # Customize the plot\n        ax.set_xticks(range(len(top_words)))\n        ax.set_xticklabels(top_words, rotation=45, ha='right')\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequency')\n        ax.set_title('Top 10 Most Common Words')\n        \n        # Add value labels on bars\n        for i, (bar, freq) in enumerate(zip(bars, top_frequencies)):\n            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n                   str(freq), ha='center', va='bottom')\n        \n        # Adjust layout to prevent label cutoff\n        plt.tight_layout()\n        \n        # Save or return the plot\n        if save_path:\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n            plt.close()\n            return None\n        else:\n            return ax\n            \n    except FileNotFoundError:\n        raise\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n        return None",
        "status": "fail",
        "details": {
          "test_display_plot": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 129, in test_display_plot\n  File \"__test__.py\", line 33, in task_func\nFileNotFoundError: File not found: dummy_path.csv\n",
          "test_empty_file": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 153, in test_empty_file\n  File \"__test__.py\", line 33, in task_func\nFileNotFoundError: File not found: dummy_path.csv\n",
          "test_large_data_set": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 174, in test_large_data_set\n  File \"__test__.py\", line 33, in task_func\nFileNotFoundError: File not found: dummy_path.csv\n",
          "test_save_plot": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 142, in test_save_plot\n  File \"__test__.py\", line 33, in task_func\nFileNotFoundError: File not found: dummy_path.csv\n"
        }
      }
    ],
    "BigCodeBench/1085": [
      {
        "task_id": "BigCodeBench/1085",
        "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text):\n\n    pass\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    \"\"\"\n    Analyzes the frequency of words in a given text after lowercasing, \n    removing punctuation, splitting into words, and plots the top 10 most common words.\n    \n    Parameters:\n    text (str): The input text to analyze\n    \n    Returns:\n    list: A list of tuples containing the 10 most common words and their counts.\n    Axes: The matplotlib Axes object of the bar chart.\n    \"\"\"\n    # Convert text to lowercase\n    text = text.lower()\n    \n    # Remove punctuation and split into words\n    # This regex keeps only alphanumeric characters and spaces\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Count word frequencies\n    word_counts = Counter(words)\n    \n    # Get the 10 most common words\n    top_10_words = word_counts.most_common(10)\n    \n    # Create the bar chart\n    if top_10_words:\n        words, counts = zip(*top_10_words)\n    else:\n        words, counts = [], []\n    \n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 Most Common Words')\n    ax.tick_params(axis='x', rotation=45)\n    \n    # Adjust layout to prevent label cutoff\n    plt.tight_layout()\n    \n    return top_10_words, ax",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/1012": [
      {
        "task_id": "BigCodeBench/1012",
        "solution": "import requests\nfrom pathlib import Path\nimport zipfile\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n\n    pass\nimport requests\nfrom pathlib import Path\nimport zipfile\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n    \"\"\"\n    Downloads and extracts a zip file from a specified URL.\n    \n    Args:\n        url (str): The URL of the zip file to download\n        filename (str): The name to save the downloaded file as\n    \n    Returns:\n        tuple: A tuple containing a status message and a list of filenames \n               in the unzipped directory, or an empty list if extraction fails.\n    \"\"\"\n    # Create directories if they don't exist\n    DOWNLOAD_DIR.mkdir(exist_ok=True)\n    ZIP_DIR.mkdir(exist_ok=True)\n    \n    # Full path for the downloaded file\n    zip_path = DOWNLOAD_DIR / filename\n    \n    try:\n        # Download the file\n        response = requests.get(url, stream=True, timeout=30)\n        response.raise_for_status()  # Raise an exception for bad status codes\n        \n        # Write the content to file\n        with open(zip_path, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                if chunk:\n                    f.write(chunk)\n        \n        # Extract the zip file\n        extracted_files = []\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n            # Extract all files to the ZIP_DIR\n            zip_ref.extractall(ZIP_DIR)\n            \n            # Get list of all extracted files\n            for info in zip_ref.filelist:\n                if not info.is_dir():\n                    extracted_files.append(info.filename)\n        \n        return (f\"Successfully downloaded and extracted {filename}\", extracted_files)\n        \n    except requests.exceptions.RequestException as e:\n        # Network-related exceptions\n        return (f\"Error: Network error occurred - {str(e)}\", [])\n    \n    except zipfile.BadZipFile:\n        # Invalid zip file\n        return (f\"Error: {filename} is not a valid zip file\", [])\n    \n    except FileNotFoundError:\n        # File not found\n        return (f\"Error: File not found - {filename}\", [])\n    \n    except PermissionError:\n        # Permission denied\n        return (f\"Error: Permission denied when accessing {filename}\", [])\n    \n    except IOError as e:\n        # Other file-related exceptions\n        return (f\"Error: File I/O error - {str(e)}\", [])\n    \n    except Exception as e:\n        # Any other unexpected exceptions\n        return (f\"Error: Unexpected error occurred - {str(e)}\", [])",
        "status": "fail",
        "details": {
          "test_invalid_url": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 100, in test_invalid_url\nAssertionError: 'Download failed' not found in 'Error: test.zip is not a valid zip file'\n",
          "test_non_200_http_response": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 107, in test_non_200_http_response\nAssertionError: 'Download failed' not found in 'Error: test.zip is not a valid zip file'\n",
          "test_successful_download_and_extraction": "Traceback (most recent call last):\n  File \"__test__.py\", line 93, in test_successful_download_and_extraction\nAssertionError: 'Download and extraction successful' not found in 'Successfully downloaded and extracted test.zip'\n"
        }
      }
    ],
    "BigCodeBench/418": [
      {
        "task_id": "BigCodeBench/418",
        "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ndef task_func(X, Y):\n\n    pass\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ndef task_func(X, Y):\n    \"\"\"\n    Divide the input data into training and test sets (70% training, 30% test),\n    create a Keras Sequential model with one hidden layer using a sigmoid activation function,\n    compile the model with binary cross-entropy loss and an SGD optimizer,\n    fit the model to the training data, and plot the ROC curve for the model on the test set.\n    \n    Parameters:\n    X: Input features\n    Y: Target labels (binary)\n    \n    Returns:\n    keras.models.Sequential: The trained Keras model.\n    matplotlib.axes._axes.Axes: The matplotlib Axes object for the ROC curve plot.\n    \"\"\"\n    \n    # Split the data into training and test sets (70% training, 30% test)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n    \n    # Create a Keras Sequential model with one hidden layer\n    model = keras.Sequential([\n        keras.layers.Dense(10, activation='sigmoid', input_shape=(X.shape[1],)),\n        keras.layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n    ])\n    \n    # Compile the model with binary cross-entropy loss and SGD optimizer\n    optimizer = keras.optimizers.SGD(learning_rate=0.01)\n    model.compile(optimizer=optimizer, \n                  loss='binary_crossentropy', \n                  metrics=['accuracy'])\n    \n    # Fit the model to the training data in non-verbose mode\n    model.fit(X_train, Y_train, epochs=100, batch_size=32, verbose=0)\n    \n    # Get predictions on the test set\n    Y_pred_proba = model.predict(X_test, verbose=0)\n    \n    # Calculate ROC curve and AUC\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred_proba)\n    roc_auc = auc(fpr, tpr)\n    \n    # Plot the ROC curve\n    fig, ax = plt.subplots(figsize=(8, 6))\n    ax.plot(fpr, tpr, color='darkorange', lw=2, \n            label=f'ROC curve (AUC = {roc_auc:.2f})')\n    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    ax.set_xlim([0.0, 1.0])\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlabel('False positive rate')\n    ax.set_ylabel('True positive rate')\n    ax.set_title('ROC curve')\n    ax.legend(loc=\"lower right\")\n    ax.grid(True, alpha=0.3)\n    \n    return model, ax",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/587": [
      {
        "task_id": "BigCodeBench/587",
        "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\ndef task_func(file_path):\n\n    pass\nimport rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\ndef task_func(file_path):\n    \"\"\"\n    Generates RSA public and private keys, encrypts a file using AES encryption, \n    and then encrypts the AES key with the public RSA key. The encrypted file \n    and the encrypted AES key are saved in separate new files.\n    \n    Args:\n        file_path (str): Path to the file to be encrypted\n        \n    Returns:\n        rsa.PublicKey: The RSA public key.\n        str: The filename of the encrypted file.\n        str: The filename of the file containing the encrypted AES key.\n    \"\"\"\n    # Generate RSA key pair\n    (public_key, private_key) = rsa.newkeys(2048)\n    \n    # Generate random AES key (256 bits) and IV (128 bits)\n    aes_key = os.urandom(32)  # 256 bits\n    iv = os.urandom(16)  # 128 bits\n    \n    # Read the file content\n    with open(file_path, 'rb') as f:\n        file_content = f.read()\n    \n    # Pad the file content to be a multiple of 16 bytes (AES block size)\n    padder = padding.PKCS7(128).padder()\n    padded_data = padder.update(file_content) + padder.finalize()\n    \n    # Create AES cipher and encrypt the file content\n    cipher = Cipher(\n        algorithms.AES(aes_key),\n        modes.CBC(iv),\n        backend=default_backend()\n    )\n    encryptor = cipher.encryptor()\n    encrypted_content = encryptor.update(padded_data) + encryptor.finalize()\n    \n    # Encrypt the AES key with RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, public_key)\n    \n    # Create output filenames\n    base_name = os.path.splitext(file_path)[0]\n    encrypted_file_name = base_name + \"_encrypted.bin\"\n    encrypted_key_file_name = base_name + \"_key.bin\"\n    \n    # Save the encrypted file content (prepend IV for decryption)\n    with open(encrypted_file_name, 'wb') as f:\n        f.write(iv + encrypted_content)\n    \n    # Save the encrypted AES key\n    with open(encrypted_key_file_name, 'wb') as f:\n        f.write(encrypted_aes_key)\n    \n    return public_key, encrypted_file_name, encrypted_key_file_name",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/99": [
      {
        "task_id": "BigCodeBench/99",
        "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n\n    pass\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n    \"\"\"\n    Draws a seaborn pair plot of the iris dataset using Arial font.\n    \n    This function sets the global font to Arial for better readability and visual appeal.\n    It then generates a pair plot from the iris dataset, where each subplot represents\n    the relationship between two features, colored by species. The plot includes the\n    title 'Iris Dataset Pair Plot' and labels for each feature on the axes.\n    \n    Returns:\n        plt.Figure: A matplotlib Figure object containing the seaborn pair plot of the\n        iris dataset. The plot has 'Iris Dataset Pair Plot' as its title. Each subplot's\n        axes are labeled with the corresponding feature names, such as 'sepal length (cm)',\n        'sepal width (cm)', 'petal length (cm)', and 'petal width (cm)'.\n    \"\"\"\n    # Set the global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n    plt.rcParams['font.sans-serif'] = ['Arial']\n    \n    # Load the iris dataset\n    iris = load_iris()\n    \n    # Create a DataFrame from the iris dataset\n    iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    iris_df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n    \n    # Create the pair plot\n    pair_plot = sns.pairplot(iris_df, hue='species', diag_kind='kde')\n    \n    # Set the title\n    pair_plot.fig.suptitle('Iris Dataset Pair Plot', y=1.02, fontsize=16)\n    \n    # Adjust the layout to prevent overlapping\n    plt.tight_layout()\n    \n    # Return the figure\n    return pair_plot.fig",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/461": [
      {
        "task_id": "BigCodeBench/461",
        "solution": "import subprocess\nimport psutil\nimport time\nimport os\ndef task_func(script_path: str, timeout=10) -> dict:\n\n    pass\nimport subprocess\nimport psutil\nimport time\nimport os\ndef task_func(script_path: str, timeout=10) -> dict:\n    \"\"\"\n    Executes a given bash script and returns the CPU and memory usage of the script's process.\n    \n    Args:\n        script_path (str): Path to the bash script to execute\n        timeout (int): Maximum execution time in seconds (default: 10)\n    \n    Returns:\n        dict: A dictionary containing:\n            'CPU Usage': The accumulated CPU usage in percentage.\n            'Memory Usage': The accumulated memory usage in bytes.\n    \"\"\"\n    # Check if script exists\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"Script not found: {script_path}\")\n    \n    # Initialize metrics\n    cpu_usage = 0.0\n    memory_usage = 0\n    last_memory = 0\n    \n    # Start the subprocess\n    try:\n        process = subprocess.Popen(['bash', script_path])\n        pid = process.pid\n        \n        # Get psutil process object\n        try:\n            ps_process = psutil.Process(pid)\n        except psutil.NoSuchProcess:\n            return {'CPU Usage': cpu_usage, 'Memory Usage': memory_usage}\n        \n        # Monitor the process\n        start_time = time.time()\n        \n        while True:\n            # Check timeout\n            if time.time() - start_time > timeout:\n                # Terminate the process\n                try:\n                    process.terminate()\n                    process.wait(timeout=1)\n                except subprocess.TimeoutExpired:\n                    process.kill()\n                    process.wait()\n                break\n            \n            # Check if process has completed\n            if process.poll() is not None:\n                break\n            \n            try:\n                # Check if process exists and is not a zombie\n                if ps_process.is_running() and ps_process.status() != psutil.STATUS_ZOMBIE:\n                    # Get CPU usage (cumulative)\n                    cpu_percent = ps_process.cpu_percent(interval=0.1)\n                    cpu_usage += cpu_percent\n                    \n                    # Get memory usage (RSS)\n                    memory_info = ps_process.memory_info()\n                    current_memory = memory_info.rss\n                    \n                    # Accumulate memory increments\n                    if current_memory > last_memory:\n                        memory_usage += (current_memory - last_memory)\n                    last_memory = current_memory\n                else:\n                    # Process is zombie or terminated\n                    break\n                    \n            except psutil.NoSuchProcess:\n                # Process no longer exists\n                break\n            except psutil.AccessDenied:\n                # Can't access process info\n                break\n            \n            # Small sleep to prevent busy waiting\n            time.sleep(0.01)\n        \n        # Ensure process is cleaned up\n        try:\n            process.wait(timeout=1)\n        except subprocess.TimeoutExpired:\n            process.kill()\n            process.wait()\n            \n    except Exception as e:\n        # Handle any unexpected errors\n        return {'CPU Usage': cpu_usage, 'Memory Usage': memory_usage}\n    \n    return {'CPU Usage': cpu_usage, 'Memory Usage': memory_usage}",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/417": [
      {
        "task_id": "BigCodeBench/417",
        "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y):\n\n    pass\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y):\n    \"\"\"\n    Trains a simple neural network on given input data and target labels.\n    \n    Parameters:\n    X: Input data with dimension 2\n    Y: Target labels\n    \n    Returns:\n    Sequential: The trained Keras Sequential model.\n    matplotlib.axes.Axes: The Axes object of the plot.\n    \"\"\"\n    # Split the data into training and test sets (75% train, 25% test)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n    \n    # Construct a Sequential model\n    model = Sequential()\n    # Add one dense hidden layer with sigmoid activation\n    # Input dimension is 2 as specified\n    model.add(Dense(units=10, activation='sigmoid', input_dim=2))\n    # Add output layer for binary classification\n    model.add(Dense(units=1, activation='sigmoid'))\n    \n    # Compile the model with binary cross-entropy loss and SGD optimizer\n    model.compile(loss='binary_crossentropy', \n                  optimizer=SGD(learning_rate=0.01),\n                  metrics=['accuracy'])\n    \n    # Fit the model to training data with validation on test set\n    history = model.fit(X_train, Y_train, \n                       epochs=100,\n                       batch_size=32,\n                       validation_data=(X_test, Y_test),\n                       verbose=0)  # No verbose output\n    \n    # Plot training and validation loss\n    fig, ax = plt.subplots()\n    ax.plot(history.history['loss'], label='Train')\n    ax.plot(history.history['val_loss'], label='Test')\n    ax.set_title('Model loss')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Loss')\n    ax.legend()\n    \n    return model, ax",
        "status": "fail",
        "details": {
          "test_model_weights": "Traceback (most recent call last):\n  File \"__test__.py\", line 97, in test_model_weights\nAssertionError: 4 != 2\n"
        }
      }
    ],
    "BigCodeBench/583": [
      {
        "task_id": "BigCodeBench/583",
        "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\ndef task_func():\n\n    pass\nimport rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\ndef task_func():\n    # Generate RSA key pair\n    (public_key, private_key) = rsa.newkeys(2048)\n    \n    # Generate random password for AES encryption\n    password = get_random_bytes(32)  # 256-bit key for AES\n    \n    # Generate random nonce for AES-GCM\n    nonce = get_random_bytes(16)\n    \n    # Create AES cipher in GCM mode\n    cipher = AES.new(password, AES.MODE_GCM, nonce=nonce)\n    \n    # Convert private key to PEM format\n    private_key_pem = private_key.save_pkcs1('PEM')\n    \n    # Encrypt the private key\n    ciphertext, tag = cipher.encrypt_and_digest(private_key_pem)\n    \n    # Generate random 8 bytes for filename\n    random_bytes = get_random_bytes(8)\n    hex_string = random_bytes.hex()\n    filename = f\"private_key_{hex_string}.txt\"\n    \n    # Prepare data to save (nonce + tag + ciphertext)\n    encrypted_data = nonce + tag + ciphertext\n    \n    # Save encrypted private key to file\n    with open(filename, 'wb') as f:\n        # Encode to base64 for text file storage\n        f.write(b64encode(encrypted_data))\n    \n    return public_key, filename, password, nonce",
        "status": "fail",
        "details": {
          "test_key_size": "Traceback (most recent call last):\n  File \"__test__.py\", line 72, in test_key_size\nAssertionError: 2048 != 512\n",
          "test_encryption_decryption": "Traceback (most recent call last):\n  File \"__test__.py\", line 87, in test_encryption_decryption\n  File \"/usr/local/lib/python3.10/site-packages/rsa/key.py\", line 125, in load_pkcs1\n    return method(keyfile)\n  File \"/usr/local/lib/python3.10/site-packages/rsa/key.py\", line 612, in _load_pkcs1_pem\n    der = rsa.pem.load_pem(keyfile, b\"RSA PRIVATE KEY\")\n  File \"/usr/local/lib/python3.10/site-packages/rsa/pem.py\", line 103, in load_pem\n    pem_lines = [line for line in _pem_lines(contents, pem_start, pem_end)]\n  File \"/usr/local/lib/python3.10/site-packages/rsa/pem.py\", line 103, in <listcomp>\n    pem_lines = [line for line in _pem_lines(contents, pem_start, pem_end)]\n  File \"/usr/local/lib/python3.10/site-packages/rsa/pem.py\", line 77, in _pem_lines\n    raise ValueError('No PEM start marker \"%r\" found' % pem_start)\nValueError: No PEM start marker \"b'-----BEGIN RSA PRIVATE KEY-----'\" found\n"
        }
      }
    ],
    "BigCodeBench/1040": [
      {
        "task_id": "BigCodeBench/1040",
        "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n\n    pass\nimport socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    \"\"\"\n    Run a non-blocking echo server that appends the server's current time to received data \n    and sends it back to the client, while handling exceptional conditions for each socket.\n    \n    Returns:\n        str: A status message indicating the server's operation and run duration.\n    \"\"\"\n    # Create server socket\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.setblocking(False)\n    server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    \n    try:\n        server.bind((server_address, server_port))\n        server.listen(5)\n    except Exception as e:\n        return f\"Failed to start server: {str(e)}\"\n    \n    # Lists of sockets to monitor\n    inputs = [server]\n    outputs = []\n    message_queues = {}\n    \n    # Track start time and client count\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=run_duration)\n    clients_served = 0\n    \n    try:\n        while datetime.now() < end_time:\n            # Calculate timeout for select\n            remaining_time = (end_time - datetime.now()).total_seconds()\n            timeout = max(0.1, min(1.0, remaining_time))\n            \n            # Wait for at least one socket to be ready for processing\n            readable, writable, exceptional = select.select(\n                inputs, outputs, inputs, timeout\n            )\n            \n            # Handle inputs\n            for s in readable:\n                if s is server:\n                    # Accept new connection\n                    try:\n                        connection, client_address = s.accept()\n                        connection.setblocking(False)\n                        inputs.append(connection)\n                        message_queues[connection] = queue.Queue()\n                        clients_served += 1\n                    except Exception:\n                        pass\n                else:\n                    # Handle data from client\n                    try:\n                        data = s.recv(buffer_size)\n                        if data:\n                            # Echo data with timestamp\n                            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n                            response = f\"{data.decode('utf-8', errors='ignore')} [Server time: {timestamp}]\"\n                            message_queues[s].put(response.encode('utf-8'))\n                            \n                            # Add to outputs if not already there\n                            if s not in outputs:\n                                outputs.append(s)\n                        else:\n                            # Client disconnected\n                            if s in outputs:\n                                outputs.remove(s)\n                            inputs.remove(s)\n                            s.close()\n                            del message_queues[s]\n                    except Exception:\n                        # Error reading from socket\n                        if s in outputs:\n                            outputs.remove(s)\n                        inputs.remove(s)\n                        s.close()\n                        if s in message_queues:\n                            del message_queues[s]\n            \n            # Handle outputs\n            for s in writable:\n                try:\n                    if s in message_queues and not message_queues[s].empty():\n                        next_msg = message_queues[s].get_nowait()\n                        s.send(next_msg)\n                    else:\n                        # No more messages for this client\n                        outputs.remove(s)\n                except Exception:\n                    # Error sending to socket\n                    if s in outputs:\n                        outputs.remove(s)\n                    if s in inputs:\n                        inputs.remove(s)\n                    s.close()\n                    if s in message_queues:\n                        del message_queues[s]\n            \n            # Handle exceptional conditions\n            for s in exceptional:\n                if s in inputs:\n                    inputs.remove(s)\n                if s in outputs:\n                    outputs.remove(s)\n                s.close()\n                if s in message_queues:\n                    del message_queues[s]\n    \n    finally:\n        # Clean up\n        for s in inputs:\n            s.close()\n        \n    actual_duration = (datetime.now() - start_time).total_seconds()\n    return f\"Server ran for {actual_duration:.2f} seconds and served {clients_served} clients\"",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/101": [
      {
        "task_id": "BigCodeBench/101",
        "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42):\n\n    pass\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_path=None):\n    \"\"\"\n    Draw the correlation heatmap of the Boston Housing dataset using Seaborn.\n    \n    Parameters:\n    -----------\n    data_url : str, optional\n        URL to fetch the Boston Housing dataset from. Default is the CMU StatLib URL.\n    seed : int, optional\n        Random seed for reproducibility. Default is 42.\n    save_path : str, optional\n        File path to save the heatmap. If None, the plot is not saved.\n    \n    Returns:\n    --------\n    matplotlib.axes.Axes: The Axes object containing the heatmap plot.\n    \n    Raises:\n    -------\n    ValueError: If an error occurs in generating or saving the plot.\n    \"\"\"\n    try:\n        # Set random seed for reproducibility\n        np.random.seed(seed)\n        \n        # Load the Boston Housing dataset\n        # The dataset from CMU requires special parsing\n        raw_data = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n        \n        # The data is in a specific format where we need to reshape it\n        data = np.hstack([raw_data.values[::2, :], raw_data.values[1::2, :3]])\n        target = raw_data.values[1::2, 3]\n        \n        # Create column names\n        columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS',\n                   'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n        \n        # Create DataFrame\n        df = pd.DataFrame(data, columns=columns)\n        df['MEDV'] = target\n        \n        # Calculate correlation matrix\n        correlation_matrix = df.corr()\n        \n        # Create figure and axis\n        plt.figure(figsize=(12, 10))\n        \n        # Create heatmap\n        ax = sns.heatmap(correlation_matrix, \n                        annot=True, \n                        cmap='coolwarm', \n                        center=0,\n                        square=True,\n                        linewidths=0.5,\n                        cbar_kws={\"shrink\": 0.8},\n                        fmt='.2f')\n        \n        # Set title\n        ax.set_title('Boston Housing Dataset Correlation Heatmap', fontsize=16, pad=20)\n        \n        # Rotate labels for better readability\n        plt.xticks(rotation=45, ha='right')\n        plt.yticks(rotation=0)\n        \n        # Adjust layout to prevent label cutoff\n        plt.tight_layout()\n        \n        # Save the plot if save_path is provided\n        if save_path:\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        \n        return ax\n        \n    except Exception as e:\n        raise ValueError(f\"Error in generating or saving the plot: {str(e)}\")",
        "status": "timeout",
        "details": {}
      }
    ]
  }
}